<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 8 - Model-Free Control –
Off-Policy</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">

</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>8 - Model-Free Control –
Off-Policy</h2>
                  </div>



                  <div class="author"> Prof. Dr. Malte Schilling </div>

                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>


         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="overview-lecture" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout row columns">
<div class="area left">
<div id="model-free-control" class="left box block">
<h2 class="left">Model-free control</h2>
<p><strong>Optimise policy</strong> using GPI</p>
<ul>
<li>Monte-Carlo (full trajectories)</li>
<li>Temporal Difference Learning – using TD estimates for bootstrapping</li>
</ul>
</div>
</div><div class="area right">
<div id="section" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/arulkumaran_drl.svg" style="height:320px;width:auto;" alt="../data/05/arulkumaran_drl.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-1" class="bottom box block">
<h2 class="bottom"></h2>
<p>Distinguish</p>
<ul>
<li>On-Policy Approach: Improving directly the policy</li>
<li>Off-Policy Approach: Use a behavior policy for exploration</li>
</ul>
</div>
<div id="section-2" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-arulkumaran2017brief" role="doc-biblioref">Arulkumaran u. a. 2017</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-model-free-policy-evaluation-approaches" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Model-Free Policy Evaluation Approaches</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Iterative approximation of value function for given policy <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[ v_{n+1} (S_t) = v_n(S_t) + \alpha \Big(G_t - v_n(S_t) \Big)\]</span></p>
</div>
<div id="different-methods" class="box block">
<h2>Different Methods:</h2>
<table>
<thead>
<tr class="header">
<th>Approach</th>
<th>Target computation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Monte-Carlo</td>
<td><span class="math inline">\(G_t^{\text{MC}} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots\)</span></td>
</tr>
<tr class="even">
<td>TD(0)</td>
<td><span class="math inline">\(G_t^{(1)} = R_{t+1} + \gamma v_t(S_{t+1})\)</span></td>
</tr>
<tr class="odd">
<td>n-step TD</td>
<td><span class="math inline">\(G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n}+ \gamma^n v_t(S_{t+n})\)</span></td>
</tr>
<tr class="even">
<td>TD(<span class="math inline">\(\lambda\)</span>)</td>
<td><span class="math inline">\(G_t^{(\lambda)} = R_{t+1} + \lambda \Big( (1-\gamma)v_t(S_{t+1}) + \lambda G_{t+1}^\lambda \Big)\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-policy-iteration-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Policy Iteration (Control)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1200px;">
<img src="../decker/code/code-2c8f8dd4.tex.svg" style="height:auto;width:100%;" alt="code-2c8f8dd4.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation:</strong><span class="math inline">\(\overset{\textrm{E}}{\longrightarrow}\)</span></p>
<p><strong>Policy improvement</strong> <span class="math inline">\(\overset{\textrm{I}}{\longrightarrow}\)</span></p>
<p>For deterministic policies: each policy is guaranteed to be strictly better until we reach the optimal policy.</p>
<p>For finite MDP: <span class="math inline">\(\exists\)</span> only a finite number of deterministic policies; therefore this converges to an optimal policy and an optimal value function in a finite number of iterations.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-monte-carlo-policy-improvement" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Monte-Carlo Policy Improvement</h1>
<div class="layout row columns">
<div class="area left">
<div id="monte-carlo-policy-iteration" class="left box block">
<h2 class="left">Monte-Carlo Policy Iteration</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:600px;">
<img src="../decker/code/code-04d07d4f.tex.svg" style="height:auto;width:100%;" alt="code-04d07d4f.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation</strong>: MC policy evaluation, <span class="math inline">\(q = q_\pi\)</span></p>
<p><strong>Policy improvement</strong>: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</p>
</div>
</div><div class="area right">
<div id="monte-carlo-control" class="right box block fragment">
<h2 class="right">Monte-Carlo Control</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:600px;">
<img src="../decker/code/code-a3071c23.tex.svg" style="height:auto;width:100%;" alt="code-a3071c23.tex.svg" />
</figure>
</div>
<p><strong>Every episode:</strong></p>
<p>MC policy evaluation, <span class="math inline">\(q ≈ q_\pi\)</span></p>
<p><span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="varepsilon-greedy-exploration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><span class="math inline">\(\varepsilon\)</span>-Greedy Exploration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Simplest idea for ensuring continual exploration: Continue to sample randomly (for a small fraction).</p>
<ul>
<li>All <span class="math inline">\(m\)</span> actions are tried with non-zero probability</li>
<li>With probability <span class="math inline">\(1 − \varepsilon\)</span> choose the greedy action</li>
<li>With probability <span class="math inline">\(\varepsilon\)</span> choose an action at random</li>
</ul>
<p><span class="math display">\[
\pi(a|s) =
\begin{cases}
      \frac{\varepsilon}{m}+1-\varepsilon &amp; \text{if}\ a= \arg\max_{a&#39; \in \mathcal{A}} q(s,a&#39;) \\
      \frac{\varepsilon}{m} &amp; \text{otherwise}
\end{cases}
\]</span></p>
</div>
<div id="section-3" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="on-policy-characteristics" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>On-Policy Characteristics</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The policy …</p>
<ul>
<li>is generally <em>soft</em>: <span class="math inline">\(\pi(a|s) &gt; 0 ,\forall s \in \mathcal{S}\)</span> and <span class="math inline">\(a \in \mathcal{A}\)</span>,</li>
<li>gradually shifts closer and closer to a deterministic optimal policy.</li>
</ul>
<p>We can use an <span class="math inline">\(\varepsilon\)</span>-greedy policy.</p>
</div>
<div id="varepsilon-soft-policy" class="definition box block fragment">
<h2 class="definition"><span class="math inline">\(\varepsilon\)</span>-soft policy</h2>
<p>A policy, for which</p>
<p><span class="math display">\[\pi(a|s) \geq \frac{\varepsilon}{|\mathcal A(s)|}, \forall \text{ states and actions for some } \varepsilon &gt; 0\]</span></p>
<p>Among <span class="math inline">\(\varepsilon\)</span>-soft policies: <span class="math inline">\(\varepsilon\)</span>-greedy policies are closest to greedy.</p>
</div>
<div id="section-4" class="box block fragment">
<h2></h2>
<p>Overall: Idea of on-policy Monte Carlo methods is General Policy Improvement.</p>
<!--
# Generalised Policy Iteration Action-Value Function {.columns}

## {.left}

**Policy evaluation**: Monte-Carlo policy evaluation, $q ≈ q_\pi$

**Policy improvement**: Greedy policy improvement -- but this can stop exploration!
(We won't sample all $s, a$ when learning by interacting)

## {.right .fragment}

**Policy evaluation**: Monte-Carlo policy evaluation $q ≈ q_\pi$

**Policy improvement**: $\varepsilon$-greedy policy improvement -->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-glie" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence: GLIE</h1>
<div class="layout">
<div class="area">
<div id="greedy-in-the-limit-with-infinite-exploration-glie" class="definition box block">
<h2 class="definition">Greedy in the Limit with Infinite Exploration (GLIE)</h2>
<p>All state-action pairs are explored infinitely many times,</p>
<p><span class="math display">\[\lim_{t \rightarrow \infty}  N_t(s,a)= \infty, \forall a, s\]</span></p>
<p>The policy converges on a greedy policy, <span class="math display">\[\lim_{t \rightarrow \infty}  \pi_t(a | s)= \mathbb{1} \Big(\arg\max_{a&#39; \in \mathcal{A}} Q_t(s,a&#39;) \Big)\]</span></p>
</div>
<div id="section-5" class="box block">
<h2></h2>
<p>For example, <span class="math inline">\(\varepsilon\)</span>-greedy is GLIE if <span class="math inline">\(\varepsilon\)</span> reduces to zero at <span class="math inline">\(\varepsilon_t = \frac{1}{t}\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="glie-monte-carlo-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>GLIE Monte-Carlo Control</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Sample <span class="math inline">\(k-th\)</span> episode using <span class="math inline">\(\pi\)</span>: <span class="math display">\[{S_1, A_1, R_2, \dots, S_T } ∼ \pi\]</span></p>
</div>
<div id="section-6" class="box block fragment">
<h2></h2>
<p>For each state <span class="math inline">\(S_t\)</span> and action <span class="math inline">\(A_t\)</span> in the episode: <span class="math display">\[
\begin{eqnarray*}
N(S_t , A_t ) &amp;\rightarrow&amp; N(S_t , A_t ) + 1 \\
Q(S_t,A_t) &amp;\rightarrow&amp; Q(S_t,A_t)+ \frac{1}{N(S_t, A_t)} \big(G_t − Q(S_t,A_t)\big)\\
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-7" class="box block fragment">
<h2></h2>
<p>Improve policy based on new action-value function <span class="math display">\[
\varepsilon \rightarrow \frac{1}{t}, \pi \rightarrow \varepsilon\text{-greedy}(q)
\]</span></p>
</div>
<div id="section-8" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-glie-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence GLIE</h1>
<div class="layout">
<div class="area">
<div id="section-9" class="theorem box block">
<h2 class="theorem"></h2>
<p>GLIE Model-free control converges to the optimal action-value function, <span class="math inline">\(q_t(s,a) \rightarrow q_∗(s,a)\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="model-free-control-temporal-difference-learning" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Model-free Control – Temporal Difference Learning</h1>
</div>
</div>
</section>
<section id="advantages-and-disadvantages-of-mc-vs.-td" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Advantages and Disadvantages of MC vs. TD</h1>
<div class="layout row columns">
<div class="area left">
<div id="monte-carlo" class="left box block">
<h2 class="left">Monte-Carlo</h2>
<p>Update value <span class="math inline">\(v(S_t)\)</span> towards <strong>actual</strong> return <span class="math inline">\(\color{red}G_t\)</span>:</p>
<p><span class="math display">\[v(S_t) \leftarrow v(S_t) + \alpha(\color{red}G_t\color{black} − v(S_t))\]</span></p>
</div>
</div><div class="area right">
<div id="td-learning" class="right box block">
<h2 class="right">TD Learning</h2>
<p>Update value <span class="math inline">\(v(S_t)\)</span> towards <strong>estimated</strong> return <span class="math inline">\(\color{blue}R_{t+1} + \gamma v(S_{t+1})\)</span>:</p>
<p><span class="math display">\[v(S_t) \leftarrow v(S_t) + \alpha (\color{blue}R_{t+1} + \gamma v(S_t+1) \color{black}- v(S_t))\]</span></p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-10" class="bottom box block">
<h2 class="bottom"></h2>
<ul>
<li>MC only works for episodic environments and needs full episodes</li>
<li>TD is independent of the temporal span of the prediction and can learn from single transitions</li>
<li>TD needs reasonable value estimates</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="mc-vs.-td-control" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>MC vs. TD Control</h1>
<div class="layout row columns">
<div class="area left">
<div id="td-learning-has-advantages-over-monte-carlo-in-prediction" class="left box block">
<h2 class="left">TD-learning has advantages over Monte-Carlo in prediction</h2>
<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Can use incomplete sequences</li>
</ul>
</div>
</div><div class="area right">
<div id="natural-idea" class="right box block fragment">
<h2 class="right">Natural Idea</h2>
<p>Use TD instead of MC in our control loop</p>
<ul>
<li>Apply TD to <span class="math inline">\(q(S, A)\)</span></li>
<li>Use <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</li>
<li>Update every time-step</li>
</ul>
</div>
<div id="section-11" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sarsa---for-update-of-the-action-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>SARSA - for update of the Action-Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-14" class="left box block">
<h2 class="left"></h2>
<p>In every time-step:</p>
<ul>
<li>Policy evaluation: SARSA, <span class="math inline">\(q(s,a) \approx q_\pi(s,a)\)</span></li>
<li>Policy improvement: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement step</li>
</ul>
</div>
</div><div class="area right">
<div id="section-12" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-dfdbac68.tex.svg" style="height:360px;width:auto;" alt="code-dfdbac68.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-13" class="bottom box block">
<h2 class="bottom"></h2>
<p><span class="math display">\[
q&#39;(s,a) \leftarrow q(s,a) + \alpha \Big(r + \gamma q(S&#39;,A&#39;) - q(S,A)\Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sarsa-on-policy-td-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>SARSA – On-Policy TD Control</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-a000a053.tex.svg" style="height:460px;width:auto;" alt="code-a000a053.tex.svg" />
</figure>
</div>
<p><span class="math inline">\(\color{blue}\text{Agent part of the algorithm; } \color{red}\text{ Environment interaction}\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-for-td-learning-windy-grid-world" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example for TD-Learning: Windy Grid World</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-7156b2c2.tex.svg" style="height:480px;width:auto;" alt="code-7156b2c2.tex.svg" />
</figure>
</div>
</div>
<div id="section-15" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example---explanation-windy-grid-world" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example - Explanation: Windy Grid World</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><strong>Goal:</strong> An agent should find a route to travel from the start point to the goal point.</p>
<p><strong>Environment:</strong></p>
<ul>
<li><span class="math inline">\(10 \times 7\)</span> grid environment</li>
<li>actions are movements in the four main directions</li>
<li>crosswind (upwards) <span class="math inline">\(\rightarrow\)</span> shifts agent one (or two) grid further to the top</li>
</ul>
<p><strong>Reward:</strong> Aiming for reaching goal as fast as possible, <span class="math inline">\(-1\)</span> for each timestep in environment.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-results-for-windy-grid-world-via-sarsa" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example Results for Windy Grid World via SARSA</h1>
<div class="layout row columns">
<div class="area left">
<div id="learning-optimal-path" class="left box block">
<h2 class="left">Learning optimal path</h2>
<div class="media">
<figure class="image">
<img src="../data/07/windy_gridworld_res.png" alt="../data/07/windy_gridworld_res.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="learned-policy" class="right box block">
<h2 class="right">Learned Policy</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-cdbc2998.tex.svg" style="height:300px;width:auto;" alt="code-cdbc2998.tex.svg" />
</figure>
</div>
<p><span class="math inline">\(\varepsilon = 0.1, \alpha=0.5\)</span>, </br>initially <span class="math inline">\(q(S,A) = 0, \forall S,A\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-improvement-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Improvement Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col40">
<div class="media">
<figure class="image" style="height:auto;width:240px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>You can step <strong>policy evaluation</strong> and <strong>policy improvement</strong> in this interactive grid environment.</p>
<p>Here shown for a form of TD Learning.</p>
<h2 class="footer" id="section-16"></h2>
<p><span class="citation">(<a href="#ref-karpathy_mdp" role="doc-biblioref">Karpathy 2015</a>)</span></p>
</div>
<div class="col60">
<div class="media">
<figure class="iframe" style="width:100%;height:auto;">
<iframe style="width:100%;height:700px;" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="summary-sarsa" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Summary SARSA</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Updating Action-Value Function with SARSA</p>
<p><span class="math display">\[
q&#39;(S_t,A_t) \leftarrow q (S_t,A_t) + \alpha \Big(R_{t+1} + \gamma q(S_{t+1},A_{t+1}) - q(S_t,A_t)\Big)
\]</span></p>
</div>
<div id="convergence" class="theorem box block">
<h2 class="theorem">Convergence</h2>
<p>Tabular SARSA converges to the optimal action-value function, <span class="math inline">\(q(s, a) \rightarrow q_∗(s, a)\)</span>, if the policy is GLIE (Greedy in the Limit with Infinite Exploration)</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="model-free-control-off-policy-approaches" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Model-free Control – Off-Policy Approaches</h1>
</div>
</div>
</section>
<section id="on-and-off-policy-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>On and Off-Policy Learning</h1>
<div class="layout">
<div class="area">
<div id="on-policy-learning" class="box block">
<h2>On-policy learning</h2>
<ul>
<li>Learn about behaviour policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li>
</ul>
</div>
<div id="off-policy-learning" class="box block fragment">
<h2>Off-policy learning</h2>
<ul>
<li>Learn about target policy <span class="math inline">\(\pi\)</span> from experience sampled from behavioral policy <span class="math inline">\(b\)</span></li>
<li>Learn ‘counterfactually’ about other things you could do: “what if…?”, e.g.:
<ul>
<li>“What if I would turn left?” <span class="math inline">\(\Rightarrow\)</span> new observations, rewards?</li>
<li>“What if I would play more defensively?” <span class="math inline">\(\Rightarrow\)</span> different win probability?</li>
<li>“What if I would continue to go forward?” <span class="math inline">\(\Rightarrow\)</span> how long until I bump into a wall?</li>
</ul></li>
</ul>
</div>
<div id="section-17" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="off-policy-learning-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Off-Policy Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(q_\pi(s,a)\)</span> while following behaviour policy <span class="math inline">\(b(a|s)\)</span>:</p>
<p><span class="math display">\[{S_1,A_1,R_2,\dots,S_T} ∼ b\]</span></p>
<p>Why is this important?</p>
<ul>
<li>Learn from observing humans or other agents</li>
<li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, \dots, \pi_{t−1}\)</span></li>
<li>Learn about optimal policy while following exploratory policy</li>
<li>Learn about multiple policies while following a single behavioral policy</li>
</ul>
</div>
<div id="section-18" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-learning-control-approach" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-Learning Control Approach</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Q-Learning estimtaes the value of the greedy policy</p>
<p><span class="math display">\[
q&#39;(S_t,A_t) \leftarrow q(S_t,A_t) + \alpha_t \Big(R_{t+1} + \gamma \max_{a&#39;}q(S_{t+1},a&#39;) - q(S_t,A_t)\Big)
\]</span></p>
</div>
<div id="section-19" class="box block fragment">
<h2></h2>
<p>When always acting greedy, the policy might not explore sufficiently.</p>
</div>
<div id="convergence-q-learning" class="theorem box block fragment">
<h2 class="theorem">Convergence Q-Learning</h2>
<p>Q-learning control converges to the optimal action-value function, <span class="math inline">\(q \rightarrow q_*\)</span>, as long as each action in each state is infinitely often selected (by a behavioral policy).</p>
<p>This works for any behavioral policy that selects all actions sufficiently often (requires appropriately decaying step sizes, e.g., <span class="math inline">\(\alpha = \frac{1}{t^\omega}\)</span> with <span class="math inline">\(\omega \in [0.5,1]\)</span>, and in general it must hold: <span class="math inline">\(\sum_t \alpha_t = \infty, \sum_t \alpha_t^2 &lt; \infty\)</span>)</p>
<!--Caveats:

* Needs exhaustive exploration to guarantee convergence for suboptimal exploration.
* eventually learning rate must be quite small, but can not be reduced too quickly
-->
<!--# Q-Learning -- Off-Policy TD control

:::col20

![](../data/08/sutton_q_backup.svg){height=360px}

:::

:::col70

1. At time step $t$, we start from state $S_t$ and pick action according to $Q$ values, $A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$; $\varepsilon$-greedy is commonly applied.
2. With action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$.
3. Update the action-value function: $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))$
4. $t = t+1$ and repeat from step 1.

:::

Difference to SARSA: Q-learning does not follow the current policy to pick the second action, but rather estimate $q_∗$ out of the best $q$ values independently.

## {.footer}

[@weng2018rl;@sutton2018] -->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-learning-for-off-policy-td-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-Learning for Off-Policy TD Control</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-bf0245aa.tex.svg" style="height:460px;width:auto;" alt="code-bf0245aa.tex.svg" />
</figure>
</div>
<p><span class="math inline">\(\color{blue}\text{Agent part as in SARSA; }; \color{Green}\text{Q-Learning specific part of the algorithm; } \color{red}\text{ Environment interaction}\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="difference-bw.-sarsa-on-policy-and-q-learning-off-policy" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Difference bw. SARSA (on-policy) </br> and Q-Learning (off-policy)</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-20" class="left box block">
<h2 class="left"></h2>
<p>Q-learning is using a different policy for choosing next action <span class="math inline">\(A&#39;\)</span> and updating <span class="math inline">\(q\)</span>.</p>
<p>It is evaluating <span class="math inline">\(\pi\)</span> as proposed by a different greedy policy <span class="math inline">\(b\)</span> which makes it an off-policy algorithm (for selection which action to use for estimate still uses <span class="math inline">\(q_\pi\)</span>).</p>
<p>SARSA uses <span class="math inline">\(\pi\)</span> all the time and is an on-policy algorithm.</p>
</div>
</div><div class="area right">
<div id="section-21" class="right box block">
<h2 class="right"></h2>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">SARSA</th>
<th align="center">Q-learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Choosing <span class="math inline">\(A\)</span></td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td align="center">Choosing <span class="math inline">\(A&#39;\)</span></td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center">greedy, <span class="math inline">\(\pi\)</span></td>
</tr>
<tr class="odd">
<td align="center">Updating <span class="math inline">\(Q\)</span></td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center">greedy, <span class="math inline">\(\pi\)</span></td>
</tr>
</tbody>
</table>
<p>In SARSA: <span class="math inline">\(\pi\)</span> is an <span class="math inline">\(\varepsilon\)</span>-greedy policy (<span class="math inline">\(\varepsilon\)</span> &gt; 0 with exploration).</p>
<p>In Q-Learning: <span class="math inline">\(\pi\)</span> is the greedy target policy (NO exploration), but <span class="math inline">\(b\)</span> is, e.g., <span class="math inline">\(\varepsilon\)</span>-greedy.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-learning" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-Learning</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-22" class="left box block">
<h2 class="left"></h2>
<p>In every time-step:</p>
<ul>
<li>Policy evaluation: <span class="math display">\[q(s,a) \approx q_\pi(s,a)\]</span></li>
<li>Policy improvement: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement step</li>
</ul>
</div>
</div><div class="area right">
<div id="section-23" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-07ac4b4c.tex.svg" style="height:360px;width:auto;" alt="code-07ac4b4c.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-24" class="bottom box block">
<h2 class="bottom"></h2>
<p><span class="math display">\[
q&#39;(S_t,A_t) \leftarrow q(S_t,A_t) + \alpha_t \Big(R_{t+1} + \gamma \max_{a&#39;}q(S_{t+1},a&#39;) - q(S_t,A_t)\Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="cliff-walking-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Cliff Walking Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>A standard undiscounted episodic task:</p>
<ul>
<li><strong>Action Space:</strong> four actions (up, down, left, right)</li>
<li><strong>Reward:</strong> <span class="math inline">\(-1\)</span> <span class="math inline">\(\forall\)</span> transitions, except into region cliff, there <span class="math inline">\(r=-100\)</span> and reset</li>
</ul>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-646796e3.tex.svg" style="height:auto;width:100%;" alt="code-646796e3.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section-25" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="cliff-walking-optimal-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Cliff Walking – Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/08/CliffWalking_optimalV.JPG" style="height:auto;width:100%;" alt="../data/08/CliffWalking_optimalV.JPG" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="cliff-walking-results" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Cliff Walking Results</h1>
<div class="layout row columns">
<div class="area left">
<div id="comparison-algorithm" class="left box block">
<h2 class="left">Comparison Algorithm</h2>
<h3 id="sarsa">SARSA</h3>
<ul>
<li>Safer path, earns higher sum of rewards</li>
</ul>
<h3 id="q-learning-varepsilon-greedy-varepsilon0.1">Q-Learning (<span class="math inline">\(\varepsilon\)</span>-greedy, <span class="math inline">\(\varepsilon=0.1\)</span>)</h3>
<ul>
<li>Optimal path, but exploration can cause falling from cliff</li>
</ul>
<!--
Q-learning is able to recover the optimal path however its exploration strategy forces it to sometimes veer off into the cliff causing it to earn less return over the long run when compared with Sarsa.
Sarsa, on the other hand, still uses an exploration algorithm but it takes into account the effect of $\epsilon$-greedy action-selection when determining the best policy, and so that's why it learns the safest path: it protects itself from random actions that could cause it to veer toward the cliff.
It's noteworthy that if $\epsilon$ were gradually reduced, then both methods would asymptotically converge to the optimal policy. -->
</div>
</div><div class="area right">
<div id="section-26" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/08/cliff_rewards.png" style="height:auto;width:100%;" alt="../data/08/cliff_rewards.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="summary-q-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Summary Q-Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Q-Learning is an off-policy approach for learning of action-values <span class="math inline">\(q(s,a)\)</span>:</p>
<ul>
<li>Next action is chosen using behaviour policy <span class="math inline">\(A_{t+1} \sim b(·|S_t)\)</span></li>
<li>But we consider alternative successor action <span class="math inline">\(a&#39; \sim \pi(·|S_t)\)</span></li>
<li>And update <span class="math inline">\(q(S_t, A_t)\)</span> towards value of alternative action</li>
</ul>
<p><span class="math display">\[
q&#39;(S_t,A_t) \leftarrow q(S_t,A_t) + \alpha_t \Big(R_{t+1} + \gamma \max_{a&#39;}q(S_{t+1},a&#39;) - q(S_t,A_t)\Big)
\]</span></p>
</div>
<div id="section-27" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bias-in-q-learning-example-roulette" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Bias in Q-Learning – Example: Roulette</h1>
<div class="layout row columns">
<div class="area left">
<div id="roulette---gambling-game" class="left box block">
<h2 class="left">Roulette - Gambling Game</h2>
<ul>
<li>Actions: 171 actions: bet $1 on one of 170 options and continue playing, or ‘stop’ which ends the episode</li>
<li>Reward: is high variance, with negative expected value; for ‘stop’ <span class="math inline">\(r=0\)</span></li>
</ul>
</div>
</div><div class="area right">
<div id="section-29" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/08/roulette.jpg" style="height:300px;width:auto;" alt="../data/08/roulette.jpg" />
</figure>
</div>
</div>
<div id="section-30" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-hasselt2010double" role="doc-biblioref">Hado Hasselt 2010</a>)</span></p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-28" class="bottom box block">
<h2 class="bottom"></h2>
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:180px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<ul>
<li>Estimate the action-value function from experience.</li>
<li>What would you expect after testing each action once?</li>
<li>Do you see a problem with variance or bias?</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-roulette-overestimation-in-q-learning" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Roulette – Overestimation in Q-Learning</h1>
<div class="layout row columns">
<div class="area left">
<div id="roulette---gambling-game-1" class="left box block">
<h2 class="left">Roulette - Gambling Game</h2>
<p>All actions have high variance reward, with negative expected value.</p>
<p>Introduces initially large value estimate when one action randomly hits.</p>
</div>
</div><div class="area right">
<div id="section-31" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/08/hasselt_roulette.png" style="height:auto;width:100%;" alt="../data/08/hasselt_roulette.png" />
</figure>
</div>
</div>
<div id="section-32" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-hasselt2010double" role="doc-biblioref">Hado Hasselt 2010</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-learning-overestimation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-learning overestimation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Q-learning overestimates because it uses the same values to select and to evaluate</p>
<p><span class="math display">\[ \max_a {q_t}(S_{t+1},a) = q_t(S_{t+1},\arg\max_a q_t(S_{t+1},a))\]</span></p>
<p>These values are only approximate!</p>
<ul>
<li>more likely to select overestimated values</li>
<li>less likely to select underestimated values</li>
<li>This causes upward bias</li>
</ul>
</div>
<div id="solution" class="box block fragment">
<h2>Solution</h2>
<p>Decouple <strong>selection</strong> from <strong>evaluation</strong>.</p>
</div>
<div id="section-33" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="double-q-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Double Q-Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Store two action-value functions: <span class="math inline">\(q\)</span> and <span class="math inline">\(q&#39;\)</span>.</p>
<p><span class="math display">\[
\begin{eqnarray}
R_{t+1} + \gamma \color{blue}q&#39;_t\color{black}(S_{t+1},\arg\max_a q_t(S_{t+1},a))\\
R_{t+1} + \gamma q_t (S_{t+1}, \arg\max_a \color{blue}q&#39;_t\color{black}(S_{t+1}, a))
\end{eqnarray}
\]</span></p>
<ul>
<li>Each timestep, pick <span class="math inline">\(q\)</span> or <span class="math inline">\(\color{blue}q&#39;\)</span> (e.g., randomly) and update using (1) for <span class="math inline">\(q\)</span> or (2) for <span class="math inline">\(\color{blue}q&#39;\)</span>.</li>
<li>Both can be used to act (e.g., use policy based on <span class="math inline">\(\frac{(q + \color{blue}q&#39;\color{black})}{2}\)</span>).</li>
</ul>
<p>Double Q-learning also converges to the optimal policy under the same conditions as Q-learning.</p>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="double-q-learning-value-estimates" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Double Q-Learning – Value Estimates</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:800px;">
<img src="../data/08/hasselt_double_actionV.png" style="height:auto;width:100%;" alt="../data/08/hasselt_double_actionV.png" />
</figure>
</div>
<ul>
<li>Orange: Bias for a single Q-learning update: <span class="math inline">\(Q(s,a) = V_∗(s) + \varepsilon\)</span></li>
<li>Blue: used <span class="math inline">\(Q&#39;\)</span> identically, independently for estimate of value</li>
<li>errors are independent standard normal random variables</li>
</ul>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-hasselt2016double" role="doc-biblioref">H. van Hasselt, Guez, und Silver 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-roulette-results-double-q-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example Roulette – Results Double Q-Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1200px;">
<img src="../data/08/hasselt_roulette_double.png" style="height:auto;width:100%;" alt="../data/08/hasselt_roulette_double.png" />
</figure>
</div>
</div>
<div id="section-36" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-mdp-double-q-learning" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example MDP: Double Q-Learning</h1>
<div class="layout row columns">
<div class="area left">
<div id="small-markov-decision-process" class="left box block">
<h2 class="left">Small Markov Decision Process</h2>
<p>The reward given from ‘B’ is stochastic: for many different actions a reward is drawn from a normal distribution (this is similar to the roulette example).</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-53d55610.tex.svg" style="height:200px;width:auto;" alt="code-53d55610.tex.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-37" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/08/sb_ch06_fig_6_7.png" style="height:450px;width:auto;" alt="../data/08/sb_ch06_fig_6_7.png" />
</figure>
</div>
</div>
<div id="section-38" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="possible-problem-for-off-policy-learning" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Possible Problem for Off-Policy Learning</h1>
<div class="layout row columns">
<div class="area left">
<div id="recap---simple-maze" class="left box block">
<h2 class="left">Recap - Simple Maze</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:480px;">
<img src="../decker/code/code-0337dbf4.tex.svg" style="height:auto;width:100%;" alt="code-0337dbf4.tex.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="off-policy-mc" class="right box block">
<h2 class="right">Off-Policy MC</h2>
<p>We generat trajectories using a random policy.</p>
<p>Afterwards we update <span class="math inline">\(q\)</span> and want to converge.</p>
<p><span class="math display">\[q(S_t, A_t) \leftarrow q(S_t,A_t) + \alpha(\color{red}G_t\color{black} − q(S_t,A_t))\]</span></p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-39" class="bottom box block fragment">
<h2 class="bottom"></h2>
<p><strong>Observation</strong>: After a couple iterations, action-value-function for moving right – <span class="math inline">\(q(C,right)\)</span> – might become negative and the improved policy will pick ‘left’ from ‘C’.</p>
<p><strong>Reason</strong>: Even though <span class="math inline">\(q(&#39;R&#39;, \text{&#39;right&#39;}) \approx 0.01\)</span> might be very small (for stochastic <span class="math inline">\(\pi\)</span>), the behavioral policy is still selecting it <span class="math inline">\(50 \%\)</span> of the time.</p>
<div class="fragment">
<p>Action-values are wrt. a given policy. We have to <strong>adjust for different probabilities</strong>!</p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="importance-sampling" class="section slide level1" data-background-color="#1f77b4">
<div class="decker">
<div class="alignment">
<h1>Importance Sampling</h1>
</div>
</div>
</section>
<section id="general-problem-estimating-an-unknown-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>General Problem: Estimating an unknown function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Goal: Calculate an expectation of a function <span class="math inline">\(f(x)\)</span>, where <span class="math inline">\(x \sim p(x)\)</span> is subject to some distribution.</p>
<p>The estimate is given as</p>
<p><span class="math display">\[
\mathbb{E}[f(x)] = \int f(x) p(x) dx \approx \frac{1}{n}\sum_{i=1}^n f(x_i), \text{for MC}
\]</span></p>
</div>
<div id="section-40" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-zhang_is" role="doc-biblioref">Zhang 2019</a>)</span></p>
</div>
<div id="monte-carlo-approach" class="box block fragment">
<h2>Monte-Carlo approach</h2>
<p>We simply sample <span class="math inline">\(x\)</span> from the distribution <span class="math inline">\(p(x)\)</span> and take the average of all samples to get an estimation of the expectation.</p>
<p><strong>Problem:</strong> It might be hard to directly use <span class="math inline">\(p(x)\)</span> for sampling.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="approach-sample-from-a-different-distribution" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Approach: Sample from a different distribution</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Instead, we can use a different distribution for sampling.</p>
<p><strong>But</strong>, we have to correct for this which can be directly derived for the estimate:</p>
<p><span class="math display">\[
\mathbb{E}[f(x)] = \int f(x) p(x) dx \fragment{= \int f(x) \frac{q(x)}{q(x)}p(x) dx }\fragment{\approx \frac{1}{n}\sum_i f(x_i) \frac{p(x_i)}{q(x_i)}}
\]</span></p>
<p>We estimate the expectation as we sample from another distribution <span class="math inline">\(q(x)\)</span>.</p>
<p><span class="math inline">\(\frac{p(x)}{q(x)}\)</span> is the sampling ratio that acts as a correction weight to offset the probability sampling from a different distribution.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-finding-an-estimate-for-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Finding an Estimate for Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="function-fx" class="left box block">
<h2 class="left">Function <span class="math inline">\(f(x)\)</span></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/07/is_example_f.png" style="height:220px;width:auto;" alt="../data/07/is_example_f.png" />
</figure>
</div>
<p>Given is a simple exponential function.</p>
</div>
</div><div class="area right">
<div id="two-distributions-for-x" class="right box block">
<h2 class="right">Two distributions for <span class="math inline">\(x\)</span></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/07/is_example_distrA.png" style="height:220px;width:auto;" alt="../data/07/is_example_distrA.png" />
</figure>
</div>
<p>We are using two normal distributions (mean of <span class="math inline">\(3\)</span> and <span class="math inline">\(3.5\)</span>) as simple examples.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="estimates" class="bottom box block">
<h2 class="bottom">Estimates</h2>
<p>The real estimate (ground truth) <span class="math inline">\(\mathbb{E}_\pi \approx 0.954\)</span>, using <span class="math inline">\(1000\)</span> samples. The estimate using <span class="math inline">\(q(x)\)</span> and the sampling ratio as correction produes <span class="math inline">\(\mathbb{E} \approx 0.949\)</span>, , variance of <span class="math inline">\(0.30\)</span></p>
</div>
<div id="section-41" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-zhang_is" role="doc-biblioref">Zhang 2019</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-2-finding-an-estimate-for-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example 2: Finding an Estimate for Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="function-fx-1" class="left box block">
<h2 class="left">Function <span class="math inline">\(f(x)\)</span></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/07/is_example_f.png" style="height:220px;width:auto;" alt="../data/07/is_example_f.png" />
</figure>
</div>
<p>Given is a simple exponential function.</p>
</div>
</div><div class="area right">
<div id="two-dissimilar-distributions-for-x" class="right box block">
<h2 class="right">Two dissimilar distributions for <span class="math inline">\(x\)</span></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/07/is_example_distrB.png" style="height:220px;width:auto;" alt="../data/07/is_example_distrB.png" />
</figure>
</div>
<p>We are using two normal distributions (mean of <span class="math inline">\(3\)</span> and <span class="math inline">\(3.5\)</span>) as simple examples.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="estimates-1" class="bottom box block">
<h2 class="bottom">Estimates</h2>
<p>The real estimate (ground truth) <span class="math inline">\(\mathbb{E}_\pi \approx 0.954\)</span> (using <span class="math inline">\(5000\)</span> samples). The estimate using <span class="math inline">\(q(x)\)</span> and the sampling ratio as correction produes <span class="math inline">\(\mathbb{E} \approx 0.995\)</span>, variance of <span class="math inline">\(83.36\)</span>.</p>
</div>
<div id="section-42" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-zhang_is" role="doc-biblioref">Zhang 2019</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="variance-when-using-importance-sampling" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Variance when using Importance Sampling</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\mathbb{E}[f(x)] \approx \frac{1}{n}\sum_i f(x_i) \frac{p(x_i)}{q(x_i)}
\]</span></p>
<p>When the importance sampling ratio is high, this will introduce large variance:</p>
<p><span class="math display">\[
Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2, \text{ with } X = f(x)\frac{p(x)}{q(x)}
\]</span></p>
<p>Therefore, we should aim for selecting <span class="math inline">\(q(x)\)</span> appropriately, i.e. in a way where <span class="math inline">\(f(x) p(x)\)</span> is already large.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="importance-sampling-in-rl" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Importance Sampling in RL</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>From a starting state <span class="math inline">\(S_t\)</span>, the probability of the subsequent state–action trajectory occurring under a policy <span class="math inline">\(\pi\)</span> is</p>
<p><span class="math display">\[
\begin{eqnarray*}
p(&amp;A_t, &amp;S_{t+1}, A_{t+1}, \dots, S_T | S_t, A_{t:T-1} \sim \pi) \\
&amp;=&amp; \fragment{\pi(A_t, S_t) p(S_{t+1} | S_t, A_t) \pi(A_{t+1}, S_{t+1}) \cdots p(S_{T} | S_{T-1}, A_{T-1})}\\
&amp;=&amp; \fragment{\prod_{k=t}^{T-1} \pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}
\end{eqnarray*}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="importance-sampling-in-rl-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Importance Sampling in RL</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In off-policy RL: we are optimizing policy <span class="math inline">\(\pi\)</span>, but follow behavioral policy <span class="math inline">\(b\)</span>.</p>
</div>
<div id="importance-sampling-ratio" class="definition box block">
<h2 class="definition">Importance Sampling Ratio</h2>
<p>The relative probability of the trajectory under the target and behavior policies is given as</p>
<p><span class="math display">\[
\rho_{t:T-1} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k | S_k) p(S_{k+1} | S_k, A_k)}
{\prod_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)} \fragment{= \prod_{k=t}^{T-1} \frac{\pi(A_k | S_k)}
{b(A_k | S_k)}}
\]</span></p>
</div>
<div id="section-43" class="box block fragment">
<h2></h2>
<p><strong>Note:</strong> The ratio only depends on the probabilities of selecting an action wrt. to the two differing policies! Does not require knowledge on transition probabilities.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="importance-sampling-as-a-correction" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Importance Sampling as a Correction</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Intuition:</p>
<ul>
<li>scale down rewards that are rare under <span class="math inline">\(\pi\)</span>, but common under <span class="math inline">\(b\)</span></li>
<li>scale up rewards that are common under <span class="math inline">\(\pi\)</span>, but rare under <span class="math inline">\(b\)</span></li>
</ul>
<p>Importance sampling can dramatically <em>increase variance</em>.</p>
</div>
<div id="section-44" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hado van Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="ordinary-importance-sampling" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>(Ordinary) Importance Sampling</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><strong>Goal</strong>: estimate the expected returns for the target policy <span class="math inline">\(\pi\)</span> <span class="math display">\[\mathbb{E} [G_t | S_t = s ]\]</span></p>
<p><em>Available</em>: only returns <span class="math inline">\(G_t\)</span> due to the behavior policy which can give us <span class="math display">\[\mathbb{E} [G_t | S_t = s ] = v_b(s)\]</span></p>
</div>
<div id="section-45" class="box block fragment">
<h2></h2>
<p>The ratio <span class="math inline">\(\rho_{t:T−1}\)</span> transforms the collected returns to have the right expected value towards the target policy:</p>
<p><span class="math display">\[\mathbb{E} [\rho_{t:T−1} G_t | S_t = s ] = v_\pi(s)\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-arulkumaran2017brief" class="csl-entry">
Arulkumaran, Kai, Marc P. Deisenroth, Miles Brundage, und Anil A. Bharath. 2017. <span>„Deep Reinforcement Learning: A Brief Survey“</span>. <em>IEEE Signal Processing Magazine</em> 34 (6).
</div>
<div id="ref-hasselt2010double" class="csl-entry">
Hasselt, Hado. 2010. <span>„Double Q-learning“</span>. In <em>Advances in Neural Information Processing Systems</em>, herausgegeben von J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, und A. Culotta. Bd. 23. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf</a>.
</div>
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-hasselt2016double" class="csl-entry">
Hasselt, Hado van, Arthur Guez, und David Silver. 2016. <span>„Deep Reinforcement Learning with Double Q-Learning“</span>. In <em>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</em>, 2094–2100. AAAI’16. Phoenix, Arizona: AAAI Press.
</div>
<div id="ref-karpathy_mdp" class="csl-entry">
Karpathy, Andrej. 2015. <span>„REINFORCEjs“</span>. <a href="https://github.com/karpathy/reinforcejs">https://github.com/karpathy/reinforcejs</a>.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-zhang_is" class="csl-entry">
Zhang, Jeremy. 2019. <span>„Importance Sampling Introduction“</span>. <a href="https://towardsdatascience.com/importance-sampling-introduction-e76b2c32e744">https://towardsdatascience.com/importance-sampling-introduction-e76b2c32e744</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("bfb0403a5.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
