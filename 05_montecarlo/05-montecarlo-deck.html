<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 5 - Model-Free Prediction –
Monte-Carlo Methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">
  
</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>5 - Model-Free Prediction –
Monte-Carlo Methods</h2>
                  </div>

         
         
                  <div class="author"> Prof. Dr. Malte Schilling </div>
         
                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>
         
         
         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="recap-bellman-optimality-equation-for-v_" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Bellman Optimality Equation for <span class="math inline">\(V_{*}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-bbdd3e74.tex.svg" style="height:auto;width:100%;" alt="code-bbdd3e74.tex.svg" />
</figure>
</div>
</div>
<div id="how-to-calculate-optimal-value-function" class="box block">
<h2>1) How to calculate optimal value function?</h2>
<p><span class="math display">\[
v_{*}(s) = \fragment{\max_{a \in \mathcal{A}} \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;) \Big)}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-dynamic-programming-for-mdps" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Dynamic Programming for MDPs</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In general, dynamic programming relies on two parts:</p>
<ul>
<li>Optimal substructure: The problem can be broken down into subproblems.</li>
<li>Overlapping sub-problems: Sub-problems occur many times.</li>
</ul>
<p>Dynamic programming algorithms allow to deal with planning problems: This means, the <strong>complete model and environment is known (the MDP)</strong>.</p>
</div>
<div id="markov-decision-processes-satisfy-both-properties" class="box block">
<h2>Markov Decision Processes satisfy both properties</h2>
<ul>
<li>The Bellman Equation gives recursive decomposition: we have the optimal behaviour of the next step and then the estimated optimal value of the remaining steps.</li>
<li>The Value function stores solutions for reuses: It caches the optimal achievable reward from a state.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-policy-iteration-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Policy Iteration (Control)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1200px;">
<img src="../.decker/code/code-2c8f8dd4.tex.svg" style="height:auto;width:100%;" alt="code-2c8f8dd4.tex.svg" />
</figure>
</div>
</div>
<div id="policy-evaluation" class="box block">
<h2>Policy evaluation:</h2>
<p><span class="math inline">\(\overset{\textrm{E}}{\longrightarrow}\)</span> How do we proceed?</p>
</div>
<div id="policy-improvement" class="box block fragment">
<h2>Policy improvement:</h2>
<p><span class="math inline">\(\overset{\textrm{I}}{\longrightarrow}\)</span> How do we adjust the policy?</p>
</div>
<div id="section" class="box block fragment">
<h2></h2>
<p>For deterministic policies: each policy is guaranteed to be strictly better until we reach the optimal policy.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div id="last-week-dynamic-programming" class="box block">
<h2>Last week: Dynamic Programming</h2>
<ul>
<li>for a known MDP</li>
<li>solve using planning</li>
</ul>
<p>General Policy Improvement Approach:</p>
<ul>
<li>Policy Evaluation</li>
<li>Policy Improvement</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="next-model-free-approaches" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Next: Model-Free Approaches</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Prediction (this week): Estimate the value function of an unknown MDP
<ul>
<li>Monte-Carlo Method</li>
<li>Temporal Difference Learning</li>
</ul></li>
<li>Model-free control (next week):
<ul>
<li>Optimise the policy</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dynamic-programming-ctd." class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming (ctd.)</h1>
</div>
</div>
</section>
<section id="general-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>General Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>General idea of running</p>
<ul>
<li>policy evaluation and</li>
<li>policy improvement</li>
</ul>
<p>in interaction. Importantly, this could be as well done asynchronously.</p>
<p>Most RL learning methods can be described as GPI: they consist of a policy and value function.</p>
<p>When evaluation and improvement process each converge, then value function and policy are optimal – the policy is greedy wrt. the stable value function. This implies: the Bellman optimality equation holds.</p>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-c885d170.tex.svg" style="height:auto;width:100%;" alt="code-c885d170.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-policy-improvement-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Policy Improvement Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col40">
<p>When stepping the environment: What do we observe?</p>
<p>In policy evaluation the value function converges quite quickly and a stable pattern emerges even sooner.</p>
<h2 id="value-iteration">Value Iteration</h2>
<p>Make only a single policy evaluation step before immediatle updating policy.</p>
<h2 class="footer" id="section-1"></h2>
<p><span class="citation">(<a href="#ref-karpathy_mdp" role="doc-biblioref">Karpathy 2015</a>)</span></p>
</div>
<div class="col60">
<div class="media">
<figure class="iframe" style="width:100%;height:auto;">
<iframe style="width:100%;height:700px;" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-iteration-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Iteration Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-f4440bcf.tex.svg" style="height:540px;width:auto;" alt="code-f4440bcf.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 60fr 40fr;">
<ul>
<li>State Space: For two locations, each maximum of <span class="math inline">\(20\)</span> cars</li>
<li>Actions: Move up to 5 cars overnight (-2 $)</li>
<li>Reward: $10 for each rented car</li>
<li>Transitions of Environment: Returns and Requests are probabilistic
<ul>
<li>following Poisson distribution, <span class="math inline">\(n\)</span> returns (same for requests) with probability <span class="math inline">\(\frac{\lambda^n}{n!}e^{-\lambda}\)</span></li>
<li>location A: average requests is 3, returns also 3</li>
<li>location B: average request is 4, returns 2</li>
</ul></li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/Europcar_jhb.JPG" style="height:540px;width:auto;" alt="../data/04/Europcar_jhb.JPG" />
</figure>
</div>
</div>
</div>
<div id="section-2" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-0" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(0\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1280px;">
<img src="../data/04/sb_Figure_4.2_0_car_rental.png" style="height:auto;width:100%;" alt="../data/04/sb_Figure_4.2_0_car_rental.png" />
</figure>
</div>
<p>x-axis: number cars at second location, y-axis: number cars at first location</p>
</div>
<div id="section-3" class="footer box block">
<h2 class="footer"></h2>
<p>Visualization from <span class="citation">(<a href="#ref-pignatelli_sb_python_2020" role="doc-biblioref">Pignatelli 2022</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-1-and-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_1.png" aria-label="Policy after first iteration" title="Policy after first iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_1.png" />
<figcaption>
Policy after first iteration
</figcaption>
</figure>
</div>
<div class="media">
<figure class="fragment image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_2.png" aria-label="Policy after second iteration" title="Policy after second iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_2.png" />
<figcaption>
Policy after second iteration
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-3-and-4" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(3\)</span> and <span class="math inline">\(4\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_3.png" aria-label="Policy after third iteration" title="Policy after third iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_3.png" />
<figcaption>
Policy after third iteration
</figcaption>
</figure>
</div>
<div class="media">
<figure class="fragment image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_4.png" aria-label="Policy after fourth iteration" title="Policy after fourth iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_4.png" />
<figcaption>
Policy after fourth iteration
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-4" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(4\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1280px;">
<img src="../data/04/sb_Figure_4.2_4.png" style="height:auto;width:100%;" alt="../data/04/sb_Figure_4.2_4.png" />
</figure>
</div>
<p>x-axis: number cars at second location, y-axis: number cars at first location</p>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p>Visualization from <span class="citation">(<a href="#ref-pignatelli_sb_python_2020" role="doc-biblioref">Pignatelli 2022</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-iteration-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:640px;">
<img src="../.decker/code/code-d8fe6e36.tex.svg" style="height:auto;width:100%;" alt="code-d8fe6e36.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation</strong> = Estimate <span class="math inline">\(v_{\pi}\)</span></p>
<p><strong>Policy improvement</strong> = Generate new <span class="math inline">\(\pi&#39; \geq \pi\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>The Bellman equation for <span class="math inline">\(s \in \mathcal{S}\)</span> is given as</p>
<p><span class="math display">\[ v_{\pi}(s)= R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;)
\]</span></p>
<p>We can span a vector space <span class="math inline">\(\mathcal{V}\)</span> over all possible value functions of dimensionality <span class="math inline">\(|\mathcal{S}|\)</span> in which each point represents a specific value function.</p>
<p>The Bellman backup alters value functions in this space and we can show that it actually brings value functions between iterations closer together (<span class="math inline">\(\gamma &lt; 1\)</span>).</p>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-70e237e8.tex.svg" style="height:auto;width:100%;" alt="code-70e237e8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can measure the distance between two state-value functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> using the <span class="math inline">\(\infty\)</span>-norm (the largest difference between state values):</p>
<p><span class="math display">\[
\norm{u - v}_\infty = \max_{s \in \mathcal{S}} |u(s) - v(s)|
\]</span></p>
</div>
<div id="bellman-operator" class="definition box block">
<h2 class="definition">Bellman operator</h2>
<p>The Bellman operator underlying <span class="math inline">\(\pi\)</span> is defined as a mapping (between value functions) <span class="math inline">\(T^\pi : \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{S}\)</span>:</p>
<p><span class="math display">\[
(T^\pi v)(s)  = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v(s&#39;)
\]</span></p>
</div>
<div id="section-5" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup-3" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup (3)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>If <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span> then <span class="math inline">\(T^\pi\)</span> is a maximum-norm contraction with a unique fixed point as a solution to <span class="math inline">\(T^\pi v = v\)</span>.</p>
<p>The Bellman operator is a <span class="math inline">\(\gamma\)</span>-contraction – it brings value functions closer together by at least factor <span class="math inline">\(\gamma\)</span>.</p>
<h2 id="section-6"></h2>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-70e237e8.tex.svg" style="height:auto;width:100%;" alt="code-70e237e8.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p>For more details on convergence and derivation using contraction mapping theorem, see <span class="citation">(<a href="#ref-2010Szepesvari" role="doc-biblioref">Szepesvári 2010</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-iter.-policy-evaluation-and-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of Iter. Policy Evaluation and Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>The Bellman expectation operator <span class="math inline">\(T^\pi\)</span> has a unique fixed point</li>
<li>which is the value function <span class="math inline">\(v_\pi\)</span> to which it converges (through updates using the Bellman expectation equation).</li>
</ul>
<p>As a consequence</p>
<ul>
<li>Iterative policy evaluation converges on <span class="math inline">\(v_\pi\)</span></li>
<li>and over time policy iteration converges on <span class="math inline">\(v_∗\)</span></li>
</ul>
</div>
<div id="section-8" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
<!--
# Considering Convergence of Bellman Backup (4)

We can think of the Bellman expectation backup as this operator $T^\pi$, 

$$
T^\pi (v) = R^\pi + \gamma P^\pi v
$$

This operator is a $\gamma$-contraction: Subsequent value functions are closer by at least $\gamma$:

$$
\begin{eqnarray*}
  \norm{T^\pi(u) - T^\pi(v)}_\infty &=& \norm{ (R_s^a + \gamma p(s' | a,s) u) - (R^a_s + \gamma p(s' | a,s) v)}_\infty \\
  &=& \fragment{\norm{\gamma P^\pi (u-v)}_\infty }\\
  & \leq & \fragment{\norm{\gamma P^\pi \norm{u-v}_\infty}_\infty }\\
  & \leq & \fragment{\gamma \norm{u-v}_\infty}
\end{eqnarray*}
$$

## {.footer}

[@silver2015]-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="big-picture-overview-approaches" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Big Picture: Overview Approaches</h1>
</div>
</div>
</section>
<section id="big-picture-solving-an-mdp" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Big picture: Solving an MDP</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>Goal: Maximize return</p>
<p><span class="math display">\[G_t = R_{t+1} + R_{t+2} + ... \]</span></p>
<p>When MDP is fully known, joint probability <span class="math inline">\(p(s&#39;, r | s,a)\)</span>:</p>
<ul>
<li>How environment develops and</li>
<li>how reward is given.</li>
</ul>
<p>Plus we fixed a policy how to act.</p>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-af647fbb.tex.svg" style="height:auto;width:100%;" alt="code-af647fbb.tex.svg" />
</figure>
</div>
<h2 class="fragment" id="section"></h2>
<p><span class="math display">\[
v_{\pi}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi (a|s) \sum_{r} \sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) \Big(r + \dots \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="big-picture-solving-an-mdp-dynamic-programming" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Big picture: Solving an MDP, Dynamic Programming</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>Goal: Maximize return</p>
<p><span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + ... \]</span></p>
<ul>
<li>Use Bellman Equation for (recursive) backup.</li>
<li>Model-Based Approach.</li>
<li>Value function based Approach.</li>
</ul>
<p>Called <strong>bootstrapping</strong>.</p>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-80ff354a.tex.svg" style="height:auto;width:100%;" alt="code-80ff354a.tex.svg" />
</figure>
</div>
<h2 class="fragment" id="section"></h2>
<p><span class="math display">\[
v_{\pi}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi (a|s) \sum_{r} \sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) \Big(r + \gamma v_{\pi}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="real-world-tasks-the-underlying-mdp-structure-is-not-known" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Real World Tasks – the underlying MDP structure is not known</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="stream" style="width:auto;height:auto;">
<iframe style="width:auto;height:720px;aspect-ratio:16/9;" allow="fullscreen" data-src="https://www.youtube.com/embed/iX6OgG67-ZQ?cc_load_policy=0&amp;controls=1&amp;iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;showinfo=0">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-robot-playing-soccer-hierarchical-rl" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Robot Playing Soccer – Hierarchical RL</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><a href="https://www.youtube.com/watch?v=iX6OgG67-ZQ">See Video of robots playing soccer</a></p>
</div>
<div id="section-9" class="footer box block">
<h2 class="footer"></h2>
<p>Details see <span class="citation">(<a href="#ref-huang2022soccer" role="doc-biblioref">Huang u. a. 2022</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="big-picture-how-to-proceed-when-mdp-is-not-known" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Big picture: How to proceed when MDP is not known?</h1>
<div class="layout row columns">
<div class="area left">
<div id="recap-multiarmed-bandit" class="left box block">
<h2 class="left">Recap – Multiarmed Bandit</h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/02/An-illustration-of-the-multi-armed-bandit-problem.png" style="height:auto;width:100%;" alt="../data/02/An-illustration-of-the-multi-armed-bandit-problem.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="estimate-action-values" class="right box block">
<h2 class="right">Estimate Action Values</h2>
<p>The action value for action <span class="math inline">\(a\)</span> is the expected reward</p>
<p><span class="math display">\[q(a) = \mathbb{E}\Big[ R_t \mid A_{t}=a \Big]\]</span></p>
<p>A simple estimate is the average of the sampled rewards:</p>
<p><span class="math display">\[\begin{eqnarray*}
Q_t(a) &amp;=&amp; \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} \\
&amp;=&amp; \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
\end{eqnarray*}\]</span></p>
</div>
<div id="section-10" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-gao2021bandit" role="doc-biblioref">Gao u. a. 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bandits-with-states-contextual-bandits" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bandits with States – Contextual bandits</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col40">
<p>How can we extend the multiarmed bandit approach to multiple states? Consider bandits with different states</p>
<ul>
<li>but episodes are still one step</li>
<li>actions do not affect state transitions</li>
<li>no long-term consequences</li>
</ul>
<p>Then, we want to estimate: <span class="math display">\[
q(s,a) = \mathbb{E} (R_{t+1} | S_t = s,A_t=a)
\]</span></p>
</div>
<div class="col60">
<p><div style="display:block; clear:both; height:100px;"></div></p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:720px;">
<img src="../.decker/code/code-0abf4e62.tex.svg" style="height:auto;width:100%;" alt="code-0abf4e62.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-sampling" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Monte Carlo Sampling</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>Goal: Maximize return</p>
<p><span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + ... \]</span></p>
<ul>
<li>Model-free Approach.</li>
<li>Sample from experience.</li>
<li>Exploit sequential data.</li>
</ul>
<p>Sample along path (Monte Carlo)</p>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-9828e4fe.tex.svg" style="height:auto;width:100%;" alt="code-9828e4fe.tex.svg" />
</figure>
</div>
<h2 class="fragment" id="section"></h2>
<p><span class="math display">\[
v_\pi(s) \leftarrow \mathbb{E}_\pi (G_t | S_t=s )
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-solving-the-bellman-optimality-equation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Solving the Bellman Optimality Equation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li><p>The Bellman optimality equation is non-linear</p></li>
<li><p>Multiple iterative solution methods:</p>
<ul>
<li>Using models – dynamic programming
<ul>
<li>Value iteration</li>
<li>Policy iteration</li>
</ul></li>
<li>Using samples
<ul>
<li>Monte Carlo Approaches</li>
<li>Q-learning</li>
<li>SARSA</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="section-11" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-sampling-methods" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Monte-Carlo Sampling Methods</h1>
</div>
</div>
</section>
<section id="using-monte-carlo-method-for-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Using Monte-Carlo Method for Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Monte-Carlo:</p>
<ul>
<li>is a model-free approach that does not use knowledge of MDP (neither transition probabilities nor reward distribution),</li>
<li>learns directly from full episodes of experience and the obtained return in these</li>
<li>as it uses the straightforward idea to estimate the mean return for a state from these episodes.</li>
</ul>
<p>One drawback: requires episodic MDPs as the episodes must terminate.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-policy-evaluation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Monte-Carlo Policy Evaluation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Reminder:</p>
<ul>
<li>Return: <span class="math inline">\(G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_{T}\)</span></li>
<li>Value function describes expected return <span class="math inline">\(v_\pi(s) = \mathbb{E}_\pi (G_t | S_t=s )\)</span></li>
</ul>
<p>Goal in Policy Evaluation: Learn <span class="math inline">\(v_\pi\)</span> from episodes of experience when following policy <span class="math inline">\(\pi\)</span>.</p>
<p>Monte-Carlo policy evaluation:</p>
<p>Simply observe the <strong>empirical mean return</strong> obtained from a state as an approximation for the expected return</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="simple-implementation-every-visit-mc-policy-evaluation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Simple implementation: Every-Visit MC Policy Evaluation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>To evaluate a state <span class="math inline">\(s\)</span>, keep track of visits <span class="math inline">\(N(s)\)</span> and sum of returns <span class="math inline">\(S(s)\)</span>:</p>
<ul>
<li>Every time <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in an episode: Increment counter <span class="math inline">\(N(s) \leftarrow N(s) +1\)</span></li>
<li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li>
<li>Value is estimated by mean return <span class="math inline">\(v(s) = S(s)/N(s)\)</span>.</li>
</ul>
</div>
<div id="convergence" class="box block fragment">
<h2>Convergence</h2>
<p>Problem: Individual datapoints are not independent.</p>
<p>Still, this converges towards the value function of the policy: <span class="math inline">\(v(s) \rightarrow v_\pi(s)\)</span> for <span class="math inline">\(N(s) \rightarrow \infty\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="simple-implementation-first-visit-mc-policy-evaluation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Simple implementation: First-Visit MC Policy Evaluation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>To evaluate a state <span class="math inline">\(s\)</span>, keep track of visits <span class="math inline">\(N(s)\)</span> and sum of returns <span class="math inline">\(S(s)\)</span>:</p>
<ul>
<li>The first time-step <span class="math inline">\(t\)</span> that state <span class="math inline">\(s\)</span> is visited in an episode: Increment counter <span class="math inline">\(N(s) \leftarrow N(s) +1\)</span></li>
<li>Increment total return <span class="math inline">\(S(s) \leftarrow S(s) + G_t\)</span></li>
<li>Value is estimated by mean return <span class="math inline">\(v(s) = S(s)/N(s)\)</span>.</li>
</ul>
<p>This converges towards the value function of the policy as each state would be visited infinite number of times: <span class="math inline">\(v(s) \rightarrow v_\pi(s)\)</span> for <span class="math inline">\(N(s) \rightarrow \infty\)</span></p>
<p>Monte Carlo methods imply estimates for each state are independent.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computing-the-return-along-a-trajectory" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Computing the return along a trajectory</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-12" class="left box block">
<h2 class="left"></h2>
<p>For efficient computation of the return along an episode:</p>
<ul>
<li>Each return is included in the equation for writing the previous time step’s return.</li>
<li>We avoid duplicating computations by starting at the terminal state and working our way backwards.</li>
</ul>
</div>
</div><div class="area center">
<div id="section-13" class="center box block">
<h2 class="center"></h2>
<p><div style="display:block; clear:both; height:100px;"></div></p>
<p><span class="math display">\[
\begin{eqnarray*}
  G_0 &amp;=&amp; R_1 + \gamma G_1 \\
  G_1 &amp;=&amp; R_2 + \gamma G_2 \\
  G_2 &amp;=&amp; R_3 + \gamma G_3 \\
  G_3 &amp;=&amp; R_4 + \gamma G_4 \\
  G_4 &amp;=&amp; 0
\end{eqnarray*}
\]</span></p>
</div>
</div><div class="area right">
<div id="section-14" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-9d2b54bf.tex.svg" style="height:600px;width:auto;" alt="code-9d2b54bf.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="first-visit-mc-prediction-for-estimating-v-approx-v_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>First-visit MC prediction, for estimating <span class="math inline">\(v \approx v_\pi\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-2925173d.tex.svg" style="height:480px;width:auto;" alt="code-2925173d.tex.svg" />
</figure>
</div>
<!--# Monte-Carlo Methods

... learns from episodes of raw experience without modeling the environmental dynamics.

MC methods computes the observed mean return as an approximation of the expected return.

Computation of the empirical return $G_t$ requires complete episodes $S_1, A_1, R_2, ... , S_T$:

$$
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}, Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
$$

An optimal policy can be learned through an iteration of evaluation and improvement (similar to GPI).-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-blackjack" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Blackjack</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-15" class="left box block">
<h2 class="left"></h2>
<p>Game: Play only against a dealer.</p>
<p>Goal: sum of cards is as great as possible without exceeding 21.</p>
<p>Counting:</p>
<ul>
<li>Number cards equal their number,</li>
<li>all face cards count as 10,</li>
<li>an ace can count as either 1 or 11.</li>
</ul>
</div>
</div><div class="area right">
<div id="section-16" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/blackjack_cards.jpg" style="height:460px;width:auto;" alt="../data/05/blackjack_cards.jpg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-gameplay-blackjack" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Gameplay Blackjack</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Initially:</p>
<ul>
<li>Player gets two cards.</li>
<li>Dealer gets two cards, one is visible.</li>
</ul>
<p>Player starts</p>
<ul>
<li>hit = take another card (and possibly another) until</li>
<li>stick = don’t take another card (or goes bust).</li>
</ul>
<p>Dealer has a fixed strategy: stick on any sum 17 or greater, and hit otherwise.</p>
<p>This problem can naturally be formulated as an episodic MDP, where each game is an episode.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-blackjack-as-a-reinforcement-learning-problem" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Blackjack as a Reinforcement Learning problem</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:300px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>Our goal: Find a value function (that we later can exploit for good decision making).</p>
<p>How can we describe Blackjack as a MDP problem?</p>
<ul>
<li>What is the state space?</li>
<li>What are the actions?</li>
<li>What is an episode?</li>
<li>What makes this difficult to approach using DP?</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="openai-gym-environment-example-webpage" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>OpenAI Gym Environment – Example Webpage</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="iframe print iframe" style="width:1280px;height:auto;">
<iframe style="width:100%;height:640px;" allow="fullscreen" data-src="https://www.gymlibrary.dev/environments/toy_text/blackjack/">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-blackjack-as-an-mdp" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Blackjack as an MDP</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-17" class="left box block">
<h2 class="left"></h2>
<p>State space is a 3-tuple of (<span class="math inline">\(32 \times 10 \times 2\)</span>):</p>
<ul>
<li>the player’s current sum <span class="math inline">\(\in \{0, 1, \ldots, 31\}\)</span>,</li>
<li>the dealer’s face up card <span class="math inline">\(\in \{1, \ldots, 10\}\)</span>, and</li>
<li>whether or not the player has a usable ace (<code>no</code> <span class="math inline">\(=0\)</span>, <code>yes</code> <span class="math inline">\(=1\)</span>).</li>
</ul>
<p>Actions:</p>
<ul>
<li>hit: take another card</li>
<li>stick: don’t take another card</li>
</ul>
</div>
</div><div class="area right">
<div id="section-18" class="right box block">
<h2 class="right"></h2>
<p>Reward for stick:</p>
<ul>
<li><span class="math inline">\(+1\)</span> if sum cards &gt; sum dealer cards</li>
<li>0 if sum cards = sum dealer cards</li>
<li><span class="math inline">\(-1\)</span> if sum cards &lt; sum dealer cards</li>
</ul>
<p>Reward for twist:</p>
<ul>
<li>-1 if sum of cards &gt; 21 (and terminate)</li>
<li>0 otherwise</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="problems-for-dynamic-programming-in-blackjack" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Problems for Dynamic Programming in Blackjack</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>There is complete knowledge of the environment in the blackjack task. Therefore, we could use DP.</p>
<p>But this is not easy as DP relies on</p>
<ul>
<li>the distribution of next events (for bootstrapping)</li>
<li>in particular, the environments dynamics as given by the joint probability <span class="math inline">\(p(s&#39;, r | s, a)\)</span></li>
</ul>
<p>This is difficult to determine for blackjack.</p>
<p>As a though experiment: consider the player’s sum is 14 and he chooses to stick. Computing of the win probability (based on the dealer’s showing card) requires computation of all of the probabilities which is quite complex (and error-prone).</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-convergence-of-value-function-over-time" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Convergence of Value Function over time</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-19" class="top box block">
<h2 class="top"></h2>
<p>Policy: take cards when count is lower 20.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="section-20" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/no_usable_ace_all.gif" style="height:480px;width:auto;" alt="../data/05/no_usable_ace_all.gif" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-21" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/usable_ace_all.gif" style="height:480px;width:auto;" alt="../data/05/usable_ace_all.gif" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-overview-results-after-10k-episode" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Overview Results after 10k episode</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-22" class="top box block">
<h2 class="top"></h2>
<p>Policy: take cards when count is lower 20.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="section-23" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/blackjack_noace_10k.png" style="height:480px;width:auto;" alt="../data/05/blackjack_noace_10k.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-24" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/blackjack_ace_10k.png" style="height:480px;width:auto;" alt="../data/05/blackjack_ace_10k.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-overview-results-after-500k-episode" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Overview Results after 500k episode</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-25" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/blackjack_noace_500k.png" style="height:480px;width:auto;" alt="../data/05/blackjack_noace_500k.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-26" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/blackjack_ace_500k.png" style="height:480px;width:auto;" alt="../data/05/blackjack_ace_500k.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="incremental-update-of-a-mean" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Incremental Update of a Mean</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In general, the mean <span class="math inline">\(\mu_1, \mu_2, \dots\)</span> of a sequence <span class="math inline">\(x_1, x_2, \dots\)</span> can be incrementally computed:</p>
<p><span class="math display">\[
\begin{eqnarray*}
\mu_k &amp;=&amp; \frac{1}{k} \sum_{j=1}^k x_j \\
&amp;=&amp; \fragment{\frac{1}{k} \Big( x_k + \sum_{j=1}^{k-1} x_j \Big)} \\
&amp;=&amp; \fragment{\frac{1}{k} ( x_k + (k-1) \mu_{k-1}) }\\
&amp;=&amp; \fragment{\mu_{k-1} + \frac{1}{k} ( x_k - \mu_{k-1}) }\\
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-27" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="incremental-monte-carlo-updates" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Incremental Monte-Carlo Updates</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Update <span class="math inline">\(v(s)\)</span> incrementally after epise <span class="math inline">\(S_1, A_1, R_2, \dots, S_T\)</span></p>
<p>For each state <span class="math inline">\(S_t\)</span> with return <span class="math inline">\(G_t\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
N(S_t) &amp;\leftarrow&amp; N(S_t) + 1 \\
v(S_t) &amp;\leftarrow&amp; v(S_t) + \frac{1}{N(S_t)} (G_t - v(S_t)) 
\end{eqnarray*}
\]</span></p>
<p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes:</p>
<p><span class="math display">\[
v(S_t) \leftarrow v(S_t) + \alpha (G_t - v(S_t)) \\
\]</span></p>
</div>
<div id="section-28" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-learning-characteristics---advantages" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Monte-Carlo Learning Characteristics - Advantages</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Monte-Carlo algorithms can learn value predictions from experience without knowing the underlying MDP.</p>
</div>
<div id="compared-to-dynamic-programming-mc-has-as-advantages" class="box block">
<h2>Compared to Dynamic Programming MC has as advantages</h2>
</div>
<div id="section-29" class="box block fragment">
<h2></h2>
<ul>
<li>Learn directly from interaction with the environment;</li>
<li>Can be used with simulation or sample models (in many cases: while sampling episodes is easy, it is often difficult to construct the full model of transition probabilities required for DP);</li>
<li>Efficient (and easy to apply) on small subset of the states (estimates are independent);</li>
<li>(may be less harmed by violations of Markov property because they do not bootstrap)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-learning-characteristics---disdvantages" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Monte-Carlo Learning Characteristics - Disdvantages</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>But when episodes are long, learning can be slow, as</p>
<ul>
<li>we have to wait until an episode ends before we can learn (we are in an episodic setting and need termination),</li>
<li>returns can have high variance.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="temporal-difference-learning" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Temporal Difference Learning</h1>
</div>
</div>
</section>
<section id="reinforcement-learning-algorithms-overview" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Reinforcement Learning Algorithms Overview</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/arulkumaran_drl.svg" style="height:460px;width:auto;" alt="../data/05/arulkumaran_drl.svg" />
</figure>
</div>
</div>
<div id="section-30" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-arulkumaran2017brief" role="doc-biblioref">Arulkumaran u. a. 2017</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="temporal-difference-learning-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Temporal Difference Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Temporal Difference …</p>
<ul>
<li>Learning is model-free: no knowledge on MDP transition or reward probabilities is needed</li>
<li>methods also learn directly from episodes of experience</li>
</ul>
</div>
<div id="bootstrapping" class="definition box block">
<h2 class="definition">Bootstrapping</h2>
<p>TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods.</p>
</div>
<div id="section-31" class="box block">
<h2></h2>
<p>The key idea in TD learning is to update the value function <span class="math inline">\(v(S_t)\)</span> towards an estimated return <span class="math inline">\(R_{t+1}+\gamma (S_{t+1})\)</span> (known as “TD target”).</p>
</div>
<div id="section-32" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="temporal-difference-learning-backup-diagram" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Temporal Difference Learning – Backup Diagram</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>Goal: Maximize return</p>
<p><span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + ... \]</span></p>
<ul>
<li>Model-free Approach.</li>
<li>Sample from experience (Monte Carlo.</li>
<li>Exploit sequential data.</li>
</ul>
<p>But: Bootstrap as in DP (use Bellman equation).</p>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-766ac014.tex.svg" style="height:auto;width:100%;" alt="code-766ac014.tex.svg" />
</figure>
</div>
<h2 class="fragment" id="section"></h2>
<p><span class="math display">\[
v(S_t) \leftarrow v(S_t) + \alpha (R_{t+1} + \gamma v(S_t+1) - v(S_t))
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="from-mc-to-temporal-difference-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>From MC to Temporal Difference Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In both: Goal is to learn <span class="math inline">\(v_\pi\)</span> online from experience under policy <span class="math inline">\(\pi\)</span></p>
<p>Incremental every-visit Monte-Carlo:</p>
<ul>
<li>Update value <span class="math inline">\(v(S_t)\)</span> towards <strong>actual</strong> return <span class="math inline">\(\color{red}G_t\)</span>: <span class="math display">\[v(S_t) \leftarrow v(S_t) + \alpha(\color{red}G_t\color{black} − v(S_t))\]</span></li>
</ul>
<p>Simple temporal-difference learning algorithm TD(0):</p>
<ul>
<li>Update value <span class="math inline">\(v(S_t)\)</span> towards <strong>estimated</strong> return <span class="math inline">\(\color{blue}R_{t+1} + \gamma v(S_{t+1})\)</span>:<br />
<span class="math display">\[v(S_t) \leftarrow v(S_t) + \alpha (\color{blue}R_{t+1} + \gamma v(S_t+1) \color{black}- v(S_t))\]</span></li>
</ul>
</div>
<div id="section-33" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="temporal-difference-learning-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Temporal Difference Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[v(S_t) \leftarrow v(S_t) + \alpha (\color{blue}R_{t+1} + \gamma v(S_t+1) \color{green}- v(S_t)\color{black})\]</span></p>
</div>
<div id="td-target" class="definition box block">
<h2 class="definition">TD Target</h2>
<p><span class="math inline">\(\color{blue}R_{t+1} + \gamma v(S_t+1)\)</span> is called the TD target.</p>
</div>
<div id="td-error" class="definition box block">
<h2 class="definition">TD Error</h2>
<p><span class="math inline">\(\delta_t = \color{blue}R_{t+1} + \gamma v(S_t+1) \color{green} - v(S_t)\)</span> is called the TD error.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="td-value-estimation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>TD: Value Estimation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Update of the value function is regulated by the learning rate <span class="math inline">\(\alpha\)</span>.</p>
<p>In brief: TD means update a guess (of the value function) towards a guess (experiencing a single step and a guess of what follows):</p>
<p><span class="math display">\[\begin{align*}
v(S_t) &amp;\leftarrow (1- \alpha) v(S_t) + \alpha G_t \\
v(S_t) &amp;\leftarrow v(S_t) + \alpha (G_t - v(S_t)) \\
v(S_t) &amp;\leftarrow v(S_t) + \alpha (R_{t+1} + \gamma v(S_{t+1}) - v(S_t))
\end{align*}\]</span></p>
<p>Similarly for the Q-function: <span class="math display">\[
q(S_t, A_t) \leftarrow q(S_t, A_t) + \alpha (R_{t+1} + \gamma q(S_{t+1}, A_{t+1}) - q(S_t, A_t))
\]</span></p>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="tabular-td0-for-estimating-v_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Tabular TD(0) for estimating <span class="math inline">\(v_\pi\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-629922ff.tex.svg" style="height:480px;width:auto;" alt="code-629922ff.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-driving-home" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Driving Home</h1>
<div class="layout">
<div class="area">
<div class="box block">
<table>
<thead>
<tr class="header">
<th>State</th>
<th>Elapsed time</th>
<th>Predicted time to Go</th>
<th>Predicted total time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Leaving office, Friday at 6</td>
<td>0</td>
<td>30</td>
<td>30</td>
</tr>
<tr class="even">
<td>Reach car, raining</td>
<td>5</td>
<td>35</td>
<td>40</td>
</tr>
<tr class="odd">
<td>Exiting highway</td>
<td>20</td>
<td>15</td>
<td>35</td>
</tr>
<tr class="even">
<td>small road, behind truck</td>
<td>30</td>
<td>10</td>
<td>40</td>
</tr>
<tr class="odd">
<td>Entering home street</td>
<td>40</td>
<td>3</td>
<td>43</td>
</tr>
<tr class="even">
<td>Arrive home</td>
<td>43</td>
<td>0</td>
<td>43</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-comparison-monte-carlo-and-td-approach" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Comparison Monte-Carlo and TD Approach</h1>
<div class="layout row columns">
<div class="area left">
<div id="monte-carlo-approach" class="left box block">
<h2 class="left">Monte-Carlo Approach</h2>
<p>Changes recommended, MC (<span class="math inline">\(\alpha=1\)</span>):</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/driving_mc.png" style="height:360px;width:auto;" alt="../data/05/driving_mc.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="temporal-difference-method" class="right box block">
<h2 class="right">Temporal Difference Method</h2>
<p>Changes recommended in TD (<span class="math inline">\(\alpha=1\)</span>):</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/driving_td.png" style="height:360px;width:auto;" alt="../data/05/driving_td.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="comparison-td-and-mc" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Comparison TD and MC</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>TD can learn before knowing the final outcome
<ul>
<li>TD can learn online after every step MC must wait until end of episode before return is known</li>
</ul></li>
<li>TD can learn without the final outcome
<ul>
<li>TD can learn from incomplete sequences</li>
<li>MC can only learn from complete sequences</li>
<li>TD works in continuing (non-terminating) environments</li>
<li>MC only works for episodic (terminating) environments</li>
</ul></li>
</ul>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-random-walk" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Random Walk</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1200px;">
<img src="../.decker/code/code-1ded5678.tex.svg" style="height:auto;width:100%;" alt="code-1ded5678.tex.svg" />
</figure>
</div>
<ul>
<li>all episodes start in the center state <span class="math inline">\(C\)</span>,</li>
<li>then proceed randomly left or right.</li>
<li>Termination states are on the extreme left and extreme right.</li>
<li>Reward: only given when terminating right (<span class="math inline">\(+1\)</span>).</li>
</ul>
<p>Value of a state is the probability of terminating in the right node.</p>
<p>Therefore: <span class="math inline">\(v_\pi(C) = \frac{1}{2}\)</span>, and for states <span class="math inline">\(A\)</span> through <span class="math inline">\(E\)</span> these are <span class="math inline">\(\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}, \frac{5}{6}\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-compare-mc-and-td-empirically" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Compare MC and TD empirically</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-36" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:1200px;">
<img src="../.decker/code/code-1ded5678.tex.svg" style="height:auto;width:100%;" alt="code-1ded5678.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="td0-estimates-for-v_pi" class="left box block">
<h2 class="left">TD(0) Estimates for <span class="math inline">\(v_\pi\)</span></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/sb_td_randomwalk.png" style="height:360px;width:auto;" alt="../data/05/sb_td_randomwalk.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="learning-curves" class="right box block">
<h2 class="right">Learning Curves</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/05/dm_randomwalk.png" style="height:360px;width:auto;" alt="../data/05/dm_randomwalk.png" />
</figure>
</div>
</div>
<div id="section-37" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>; <a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-arulkumaran2017brief" class="csl-entry">
Arulkumaran, Kai, Marc P. Deisenroth, Miles Brundage, und Anil A. Bharath. 2017. <span>„Deep Reinforcement Learning: A Brief Survey“</span>. <em>IEEE Signal Processing Magazine</em> 34 (6).
</div>
<div id="ref-gao2021bandit" class="csl-entry">
Gao, Chongming, Wenqiang Lei, Xiangnan He, Maarten Rijke, und Tat-Seng Chua. 2021. <span>„Advances and Challenges in Conversational Recommender Systems: A Survey“</span>.
</div>
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-huang2022soccer" class="csl-entry">
Huang, Xiaoyu, Zhongyu Li, Yanzhen Xiang, Yiming Ni, Yufeng Chi, Yunhao Li, Lizhi Yang, Xue Bin Peng, und Koushil Sreenath. 2022. <span>„Creating a Dynamic Quadrupedal Robotic Goalkeeper with Reinforcement Learning“</span>. arXiv. doi:<a href="https://doi.org/10.48550/ARXIV.2210.04435">10.48550/ARXIV.2210.04435</a>.
</div>
<div id="ref-karpathy_mdp" class="csl-entry">
Karpathy, Andrej. 2015. <span>„REINFORCEjs“</span>. <a href="https://github.com/karpathy/reinforcejs">https://github.com/karpathy/reinforcejs</a>.
</div>
<div id="ref-pignatelli_sb_python_2020" class="csl-entry">
Pignatelli, Eduardo. 2022. <span>„Python Repository for visualization for Sutton and Barto’s <span>‚Reinforcement Learning‘</span> book“</span>. <a href="https://github.com/epignatelli/reinforcement-learning-an-introduction">https://github.com/epignatelli/reinforcement-learning-an-introduction</a>.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-2010Szepesvari" class="csl-entry">
Szepesvári, Csaba. 2010. <em>Algorithms for Reinforcement Learning</em>. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers. <a href="http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009">http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009</a>.
</div>
<div id="ref-weng2018rl" class="csl-entry">
Weng, Lilian. 2018. <span>„A (Long) Peek into Reinforcement Learning“</span>. <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("f233054ce.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
