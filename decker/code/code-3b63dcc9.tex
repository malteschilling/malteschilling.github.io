\documentclass{standalone}
\usepackage[boxruled,lined]{algorithm2e}
\usepackage{mathtools}
\usepackage{amsfonts} 
\usepackage[dvipsnames]{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\begin{document}
\pagestyle{empty}
\begin{algorithm}[H]
  \textcolor{Green}{Input: a differentiable function $\hat{q}$ and a policy $\pi$}\\
  \textcolor{Cerulean}{Initializ parameter vector $\vec{w}$} \\
\For{episode $=1, \dots, M$} {
	$S,A \leftarrow$ initial state and action of episode\\
  Initialize sequence \textcolor{red}{$s_1 = {x_1}$}\\
  \For{$t = 1, \dots, T$} {
  \textcolor{blue}{with probability $\varepsilon$ select random action $a_t$, \\
  	otherwise $a_t = \arg\max_{a} Q(\phi(s_t), a; \theta)$} \\
  \textcolor{red}{Execute action $a_t$ and observe reward $r_t$ and image $x_{t+1}$}\\
  \textcolor{orange}{Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$}\\
  \textcolor{Green}{Store transition $(\phi_t,a_t ,r_t, \phi_{t+1})$ in $D$\\
  Sample random minibatch of transitions $(\phi_j,a_j ,r_j, \phi_{j+1})$ from $D$}\\
  Set \textcolor{blue}{$y_j = \begin{cases}
      r_j & \text{if terminates at } j+1\\
      r_j + \gamma \max_{a'} \hat{Q}(\phi_{j+1}, a', \theta^{-}) & \text{otherwise}
    \end{cases} $} \\
  \textcolor{blue}{Perform a gradient step on $(y_j - Q(\phi_j, a_j; \theta))^2$ wrt. $\theta$}\\
  \textcolor{Cerulean}{Every $C$ steps reset $\hat{Q} = Q$}
  }
}
\end{algorithm}
\end{document}