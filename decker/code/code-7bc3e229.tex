\documentclass{standalone}
\usepackage[boxruled,lined]{algorithm2e}
\usepackage{mathtools}
\usepackage{amsfonts} 
\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\begin{document}
\pagestyle{empty}
\begin{algorithm}[H]
  \textcolor{blue}{Initialize replay memory $D$ to capacity $N$\\
  Initialize action-value function $Q$ with random weights $\theta$\\
  Initialize target action-value function $\hat{Q}$ with weights $\theta^{-}=\theta$} \\
\For{episode $=1, T$} {
  \textcolor{blue}{Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$} \\
  \textcolor{red}{Generate an episode from $S_0, A_0$ following $\pi$: $S_0, A_0, R_1, \ldots,   S_{T-1}, A_{T-1}, R_T$.}\\
  \textcolor{blue}{$G \gets 0$.}\\
  \For{each step of the episode $t = T-1, T-2, \ldots, 0$} {
    \textcolor{blue}{$G \gets \gamma G + R_{t+1}$ \\
    \If{$S_t, A_t$ does not appear in $S_0, A_0, S_1, A_1, \ldots, S_{t-1},             A_{t-1}$} {
Append $G$ to $\texttt{Returns}(S_t, A_t)$ \\
$Q(S_t, A_t) \gets \texttt{average}(\texttt{Returns}(S_t, A_t))$ \tcp{Policy Evaluat.}
$\pi(S_t) \gets \argmax_a Q(S_t, a)$ \tcp{Policy Improvement}}
    }
  }
}
\end{algorithm}
\end{document}