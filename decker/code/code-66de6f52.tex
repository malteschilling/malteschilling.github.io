\documentclass{standalone}
\usepackage[boxruled,lined]{algorithm2e}
\usepackage{mathtools}
\usepackage{amsfonts} 
\usepackage{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\begin{document}
\pagestyle{empty}
\begin{algorithm}
Algorithm parameters: step size $\alpha \in (0, 1]$, small $epsilon > 0$\;
Initialize $Q(s, a)$, for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$, arbitrarily except that $Q(\mathrm{terminal}, \cdot) = 0$\;

\ForEach{episode}{
    Initialize S\;
    \ForEach{step of episode}{
        Choose $A$ from $S$ using policy derived from $Q$ (e.g., \textepsilon-greedy)\;
        Take action $A$, observe $R$, $S'$\;
        $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma \max_a Q(S', a) - Q(S, A)]$\;
        $S \leftarrow S'$\;
    }
}
\caption{Q-learning (off-policy TD control) for estimating $\pi \approx \pi_*$}
\end{algorithm}
\end{document}