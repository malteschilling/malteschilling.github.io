<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 10 - Deep Q-Network</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">

</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>10 - Deep Q-Network</h2>
                  </div>



                  <div class="author"> Prof. Dr. Malte Schilling </div>

                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>


         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Function Approximation in Reinforcement Learning
<ul>
<li>Difficulties, due to characteristics of RL</li>
<li>Approaches</li>
</ul></li>
<li>Deep Q-Networks (for playing ATARI)</li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/mnih_dqn_architecture.png" style="height:300px;width:auto;" alt="../data/09/mnih_dqn_architecture.png" />
</figure>
</div>
</div>
<div id="section" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="large-scale-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Large-Scale Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Reinforcement learning can be used to solve large problems, e.g. </p>
<ul>
<li>Backgammon: <span class="math inline">\(10^{20}\)</span> states</li>
<li>Go: <span class="math inline">\(10^{170}\)</span> states</li>
<li>Helicopter: continuous state space</li>
<li>Robots: real world tasks</li>
</ul>
<p>How can we apply our methods for prediction and control?</p>
</div>
<div id="section-1" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="function-approximation-and-deep-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Function approximation and deep reinforcement learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>The policy, value function, model, and agent state update are all functions.</li>
<li>Goal: learn these from experience.</li>
<li>But, if there are too many states, we need approximation.</li>
</ul>
<p>When using neural networks to represent these functions, this is called deep reinforcement learning.</p>
<p>While the term is fairly new (around 8 years) — the combination is fairly old (more then 50 years).</p>
</div>
<div id="section-2" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>So far, we mostly considered lookup tables</p>
<ul>
<li>Every state s has an entry <span class="math inline">\(v(s)\)</span> or every state-action pair <span class="math inline">\(s, a\)</span> has an entry <span class="math inline">\(q(s, a)\)</span></li>
</ul>
</div>
<div id="problem-with-large-mdps" class="box block">
<h2>Problem with large MDPs:</h2>
<ul>
<li>There are too many states and/or actions to store in memory.</li>
<li>It is too slow to learn the value of each state individually.</li>
<li>Individual environment states are often not fully observable</li>
</ul>
</div>
<div id="section-3" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="generalization-over-states" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Generalization over States</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In order to deal with continuous or large state spaces, we want to generalize. For this, we use <em>Function Approximation</em> to estimate value functions:</p>
<p><span class="math display">\[
v_\vec{w}(s) \approx v_\pi (s)\]</span> <span class="math display">\[q_\vec{w}(s,a) \approx q_\pi(s,a)
\]</span></p>
<ul>
<li>Learn about a small number of training states from experiences and update parameter <span class="math inline">\(\vec{w}\)</span> that describe our function approximation.</li>
<li>Generalize these experiences to new, similar situations.</li>
</ul>
<p>As a basic idea for <em>Deep Reinforcement Learning</em>: use Neural Networks for function approximation.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-value-function-in-grid-world-example" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Value Function in Grid World Example</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-4" class="top box block">
<h2 class="top"></h2>
<div class="grid-layout" style="grid-template-columns: 50fr 50fr;">
<ul>
<li>We considered the simple discrete grid environment.</li>
<li>For a real robot: We would consider a continuous state space (position).</li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_3_2_gridworld.svg" style="height:160px;width:auto;" alt="../data/02/sutton_3_2_gridworld.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="section-5" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:420px;">
<img src="../data/08/grid_vf_to_continous_1.png" style="height:auto;width:100%;" alt="../data/08/grid_vf_to_continous_1.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-6" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:420px;">
<img src="../data/08/grid_vf_to_continous_6.png" style="height:auto;width:100%;" alt="../data/08/grid_vf_to_continous_6.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="classes-of-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Classes of Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="incremental">
<ul class="incremental">
<li class="fragment"><strong>Tabular</strong>: a table with an entry for each MDP state</li>
<li class="fragment"><strong>State aggregation</strong>: Partition environment states (or observations) into a discrete set</li>
<li class="fragment">Linear function approximation
<ul class="incremental">
<li class="fragment">Consider fixed agent state update</li>
<li class="fragment">Fixed feature map <span class="math inline">\(\vec{x}:S \rightarrow \mathbb{R}^n\)</span></li>
<li class="fragment">Values are linear function of features: <span class="math inline">\(v_\mathbf{w}(s) = \mathbf{w}^\intercal \mathbf{x}(s)\)</span></li>
</ul></li>
<li class="fragment"><strong>Differentiable function approximation</strong>: <span class="math inline">\(v_\vec{w}(s)\)</span> is a differentiable function of <span class="math inline">\(\vec{w}\)</span> that could be non-linear
<ul class="incremental">
<li class="fragment">e.g., a convolutional neural network that takes pixels as input.</li>
<li class="fragment">Another interpretation: features are not fixed, but learnt.</li>
</ul></li>
</ul>
</div>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="classes-of-function-approximation-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Classes of Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In principle, any function approximator can be used, but RL has specific properties:</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">Experience is not i.i.d. — successive time-steps are correlated</li>
<li class="fragment">Agent’s policy affects the data it receives</li>
</ul>
</div>
</div>
<div id="section-8" class="box block fragment">
<h2></h2>
<p>Regression targets can be <strong>non-stationary</strong></p>
<ul>
<li>…because of changing policies (which can change the target and the data!)</li>
<li>…because of bootstrapping</li>
<li>…because of non-stationary dynamics (e.g., other learning agents)</li>
<li>…because the world is large (never quite in the same state)</li>
</ul>
</div>
<div id="section-9" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="classes-of-function-approximation-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Classes of Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The choice of function approximation depends on the task and yourgoals:</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment"><strong>Tabular</strong>: good theory but does not scale/generalise</li>
<li class="fragment"><strong>Linear</strong>: reasonably good theory, but requires good features</li>
<li class="fragment"><strong>Non-linear</strong>: less well-understood, but scales well. Flexible, and less reliant on picking good features first (e.g., by hand)</li>
</ul>
<p>(Deep) neural nets often perform quite well, and are a popular choice.</p>
</div>
</div>
<div id="section-10" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="the-data-scale-versus-the-model-performance." class="slide level1">
<div class="decker">
<div class="alignment">
<h1>The data scale versus the model performance.</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image">
<img src="../data/10/data_size_vs_model_performance.png" alt="../data/10/data_size_vs_model_performance.png" />
</figure>
</div>
</div>
<div id="section-11" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-gradient-descent" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Gradient Descent</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-12" class="left box block">
<h2 class="left"></h2>
<p>For a differentiable function <span class="math inline">\(J(\vec{w}\)</span> the gradient is given as</p>
<p><span class="math display">\[\nabla_\vec{w} J(\vec{w}) =
\begin{pmatrix}
    \frac{\partial J(\vec{w})}{\partial w_1} \\
    \vdots\\
     \frac{\partial J(\vec{w})}{\partial w_n}
\end{pmatrix}
\]</span></p>
<p><strong>Goal</strong>: Minimize <span class="math inline">\(J(\vec{w})\)</span></p>
<p><strong>Approach</strong>: Move <span class="math inline">\(\vec{w}\)</span> in the direction of negative gradient (step size parameter <span class="math inline">\(\alpha\)</span>)</p>
<p><span class="math display">\[
\Delta \vec{w} = - \frac{1}{2} \alpha \nabla_\vec{w} J(\vec{w})\]</span></p>
</div>
</div><div class="area right">
<div id="section-13" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/errorLandscape_Gradient.png" style="height:420px;width:auto;" alt="../data/09/errorLandscape_Gradient.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="approximate-values-by-stochastic-gradient-descent" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Approximate Values By Stochastic Gradient Descent</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><strong>Goal</strong>: find <span class="math inline">\(\vec{w}\)</span> that minimizes the difference between <span class="math inline">\(v_\vec{w}(s)\)</span> and <span class="math inline">\(v_\pi(s)\)</span></p>
<p><span class="math display">\[J(\vec{w}) = \mathbb{E}_{S \sim d} \big [ (v_\pi(S) − v_\vec{w}(S))^2 \big]\]</span></p>
<p>where <span class="math inline">\(d\)</span> is a distribution over states (induced by the policy and dynamics)</p>
</div>
<div id="gradient-descent" class="box block">
<h2>Gradient descent:</h2>
<p><span class="math display">\[ \Delta \vec{w} = −\frac{1}{2} \nabla_\vec{w} J(\vec{w}) = \alpha \mathbb{E}_{d} (v_\pi(S) − v_\vec{w}(S)) \nabla_\vec{w} v_\vec{w} (S)
\]</span></p>
</div>
<div id="stochastic-gradient-descent" class="box block fragment">
<h2>Stochastic Gradient Descent:</h2>
<p>Sample the gradient: <span class="math inline">\(\Delta \vec{w} = \alpha (G_t − v_\vec{w}(S_t)) \nabla_\vec{w} v_\vec{w} (S_t)\)</span></p>
<p>Note: MC return <span class="math inline">\(G_t\)</span> is a sample for <span class="math inline">\(v_\pi(S_t)\)</span>; <span class="math inline">\(\nabla_v (S_t)\)</span> is short hand for <span class="math inline">\(\nabla_\vec{w} v_\vec{w}(S_t) |_{\vec{w}=\vec{w_t}}\)</span></p>
</div>
<div id="section-14" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-model-free-prediction" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Linear model-free prediction</h1>
</div>
</div>
</section>
<section id="feature-vectors-as-observations" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Feature Vectors as Observations</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Represent the observed state as a feature vector:</p>
<p><span class="math display">\[
\vec{x}(S) =
\begin{pmatrix}
	x_1(S)\\
	\vdots\\
	x_n(S)
\end{pmatrix}
\]</span></p>
<p>For example:</p>
<ul>
<li>Distance of robot from landmarks</li>
<li>Trends in the stock market</li>
<li>Piece and pawn configurations in chess</li>
</ul>
</div>
<div id="section-15" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-model-formulation-as-a-simple-neural-network" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Linear Model Formulation (as a simple Neural network)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/05_2_withoutBias.png" style="height:480px;width:auto;" alt="../data/09/05_2_withoutBias.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Linear Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Approximate value function by a linear combination of features</p>
<p><span class="math display">\[v_\vec{w}(s) = \vec{w}^\intercal \vec{x}(s) = \sum_{j=1}^n x_j (s) w_j\]</span></p>
</div>
<div id="section-16" class="box block fragment">
<h2></h2>
<p>Objective function (‘loss’) is quadratic in <span class="math inline">\(\vec{w}\)</span></p>
<p><span class="math display">\[J(\vec{w}) = \mathbb{E}_{S \sim d} \Big[ (v_\pi(S) − \vec{w}^\intercal \vec{x}(S))^2 \Big]\]</span></p>
</div>
<div id="section-17" class="box block fragment">
<h2></h2>
<p>Stochastic gradient descent converges on global optimum</p>
<p>Update rule is simply (Update = step-size <span class="math inline">\(\times\)</span> prediction error <span class="math inline">\(\times\)</span> feature vector)</p>
<p><span class="math display">\[ \nabla_\vec{w} v_\vec{w}(S_t) = \vec{x}(S_t) = \vec{x}_t \Rightarrow \Delta \vec{w} = \alpha (v_\pi(S_t) − v_\vec{w}(S_t))\vec{x}_t\]</span></p>
</div>
<div id="section-18" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="computation-in-linear-model-overview-learning-cycle" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Computation in Linear Model: Overview Learning Cycle</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/05_6_activation.png" style="height:480px;width:auto;" alt="../data/09/05_6_activation.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="incremental-prediction-algorithms" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Incremental prediction algorithms</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can’t update towards the true value function <span class="math inline">\(v_\pi(s)\)</span>. Therefore, we (as done before) substitute a target for <span class="math inline">\(v_\pi(s)\)</span>:</p>
<ul>
<li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[ \Delta \vec{w}_t = \alpha(G_t − v_\vec{w}(s)) \nabla_\vec{w} v_\vec{w}(s)\]</span></p></li>
<li><p>For TD, the target is the TD target <span class="math inline">\(R_{t+1} + \gamma v_\vec{w}(S_{t+1})\)</span>: <span class="math display">\[ \Delta \vec{w}_t = \alpha(R_{t+1} + \gamma v_\vec{w}(S_{t+1}) - v_\vec{w}(S_t)) \nabla_\vec{w} v_\vec{w}(S_t)\]</span></p></li>
</ul>
</div>
<div id="section-19" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="monte-carlo-with-value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Monte-Carlo with Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>In MC: The return <span class="math inline">\(G_t\)</span> is an unbiased sample of <span class="math inline">\(v_\pi(s)\)</span>.</li>
<li>Can therefore apply “supervised learning” to (online) “training data”: <span class="math inline">\({(S_0,G_0),\dots, (S_t,G_t)}\)</span></li>
<li>For example, using <strong>linear Monte-Carlo policy evaluation</strong> <span class="math display">\[
\begin{eqnarray*}
\Delta \vec{w}_t &amp;=&amp;\alpha(G_t − v_\vec{w}(S_t)) \nabla_\vec{w} v_\vec{w}(S_t) \\
&amp;=&amp; \alpha(G_t − v_\vec{w}(S_t))\vec{x}_t
\end{eqnarray*}
\]</span></li>
</ul>
</div>
<div id="section-20" class="box block fragment">
<h2></h2>
<ul>
<li>Linear Monte-Carlo evaluation converges to the global optimum</li>
<li>Even when using non-linear value function approximation it converges (but perhaps to a local optimum)</li>
</ul>
</div>
<div id="section-21" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="td-learning-with-value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>TD Learning with Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>The TD-target <span class="math inline">\(R_{t+1} + \gamma v_\vec{w}(S_{t+1})\)</span> is a biased sample of true value <span class="math inline">\(v_\pi(S_t)\)</span>.</li>
<li>We still can apply supervised learning to “training data” <span class="math inline">\({(S_0,R_{1} + \gamma v_\vec{w}(S_{1})),\dots, (S_t,R_{t+1} + \gamma v_\vec{w}(S_{t+1}))}\)</span></li>
<li>For example, using <strong>linear TD</strong> <span class="math display">\[
\begin{eqnarray*}
\Delta \vec{w}_t &amp;=&amp;\alpha \underbrace{(R_{t+1} + \gamma v_\vec{w} (S_{t+1})− v_\vec{w}(S_t))}_{= \delta_t, \text{TD error}} \nabla_\vec{w} v_\vec{w}(S_t) \\
&amp;=&amp; \alpha \delta_t \vec{x}_t
\end{eqnarray*}
\]</span></li>
</ul>
</div>
<div id="section-22" class="box block fragment">
<h2></h2>
<ul>
<li>This is akin to a non-stationary regression problem</li>
<li>But: target depends on our parameters – therefore called <em>semi-gradient</em>!</li>
</ul>
</div>
<div id="section-23" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="control-with-value-function-approximation" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Control with Value Function Approximation</h1>
</div>
</div>
</section>
<section id="control-with-value-function-approximation-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Control with Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-0c284fe2.tex.svg" style="height:auto;width:100%;" alt="code-0c284fe2.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation</strong>: Approximate policy evaluation, <span class="math inline">\(\hat{q}(·, ·, \vec{w}) ≈ q_\pi\)</span></p>
<p><strong>Policy improvement</strong>: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="action-value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Action-Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li><p>Approximate the action-value function <span class="math inline">\(\hat{q}(S, A, \vec{w}) \approx q_\pi(S, A)\)</span></p></li>
<li><p>Minimize mean-squared error between approximate action-value function <span class="math inline">\(\hat{q}(S,A,\vec{w})\)</span> and true action-value function <span class="math inline">\(q_\pi(S,A)\)</span>:</p></li>
</ul>
<p><span class="math display">\[
J(\vec{w}) = \mathbb{E}_\pi [ (q_\pi(S,A) - \hat{q}(S,A,\vec{w}))^2 ]
\]</span></p>
<ul>
<li>Use stochastic gradient descent to find a local minimum <span class="math display">\[
\begin{eqnarray*}
− \frac{1}{2} \nabla_\vec{w} J(\vec{w}) = (q_\pi (S , A) − \hat{q}(S , A, \vec{w})) \nabla_\vec{w} \hat{q}(S , A, \vec{w})\\
\Delta \vec{w} = \alpha \Big( q_\pi(S, A) − \hat{q}(S, A, \vec{w})) \nabla_\vec{w} \hat{q}(S, A, \vec{w})
\end{eqnarray*}
\]</span></li>
</ul>
</div>
<div id="section-24" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-action-value-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Linear Action-Value Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Represent state and action by a feature vector</li>
</ul>
<p><span class="math display">\[
\vec{x}(S,A) =
\begin{pmatrix}
	x_1(S,A)\\
	\vdots\\
	x_n(S,A)
\end{pmatrix}
\]</span></p>
<ul>
<li>Represent action-value function by linear combination of features</li>
</ul>
<p><span class="math display">\[\hat{q}(S,A,\vec{w}) = \vec{x}(S,A)^\intercal \vec{w} = \sum_{j=1}^n x_j (S,A) w_j\]</span></p>
<ul>
<li>Stochastic gradient descent update</li>
</ul>
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\vec{w} \hat{q}(S,A,\vec{w}) = \vec{x}(S,A),
\Delta \vec{w} = \alpha (q_\pi(S,A) - \hat{q}(S,A,\vec{w}))\vec{x}(S,A)
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-25" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="incremental-control-algorithms" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Incremental Control Algorithms</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Like prediction, we must substitute a target for <span class="math inline">\(q_\pi(S,A)\)</span>:</p>
<ul>
<li><p>For MC, the target is the return <span class="math inline">\(G_t\)</span> <span class="math display">\[ \Delta \vec{w} = \alpha(G_t − \hat{q}(S_t, A_t, \vec{w})) \nabla_\vec{w} \hat{q}(S_t,A_t, \vec{w})\]</span></p></li>
<li><p>For TD, the target is the TD target <span class="math inline">\(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})\)</span>: <span class="math display">\[ \Delta \vec{w}_t = \alpha(R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \vec{w}) - \hat{q}(S_{t}, A_{t}, \vec{w})) \nabla_\vec{w} \hat{q} (S_t, A_t, \vec{w})\]</span></p></li>
</ul>
</div>
<div id="section-26" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="semi-gradient-sarsa-algorithm" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Semi-Gradient SARSA Algorithm</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-db5847e0.tex.svg" style="height:540px;width:auto;" alt="code-db5847e0.tex.svg" />
</figure>
</div>
</div>
<div id="section-27" class="footer box block">
<h2 class="footer"></h2>
<p><span class="math inline">\(\color{red}\text{ Environment interaction}\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-sarsa-for-mountain-car" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: SARSA for Mountain Car</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-28" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Observation: position, velocity</li>
<li>Action: discrete = left, none, right</li>
<li>Reward: <span class="math inline">\(r = −1\)</span>, goal is to terminate as quickly as possible</li>
<li>Episode terminates when car reaches the flag (or max steps or too far left)</li>
<li>Simplified longitudinal car physics</li>
<li>Init: Random position, zero velocity</li>
<li>car is underpowered, requires swing-up</li>
</ul>
</div>
</div><div class="area right">
<div id="section-29" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/sb_10_1_mountain_car.png" style="height:450px;width:auto;" alt="../data/10/sb_10_1_mountain_car.png" />
</figure>
</div>
</div>
<div id="section-30" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="coarse-coding-for-mountain-car" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Coarse Coding for Mountain Car</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Problem space is grouped into (overlapping) partitions / tiles.</li>
<li>Performs a discretization of the problem space.</li>
<li>Function approximation used for interpolation between tiles.</li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/sb_9_9_coarse_coding.png" style="height:320px;width:auto;" alt="../data/10/sb_9_9_coarse_coding.png" />
</figure>
</div>
</div>
<div id="section-31" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="linear-sarsa-with-coarse-coding-in-mountain-car" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Linear SARSA with Coarse Coding in Mountain Car</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/sb_10_1_mountain_results.png" style="height:480px;width:auto;" alt="../data/10/sb_10_1_mountain_results.png" />
</figure>
</div>
</div>
<div id="section-32" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
<!--# TASK

::: columns-30-70

![](../data/Discussion.png){width=180px}

TODO

:::-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="deep-reinforcement-learning" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Deep Reinforcement Learning</h1>
</div>
</div>
</section>
<section id="batch-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Batch Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Gradient descent is simple and appealing</li>
<li>But it is not sample efficient</li>
<li>Batch methods seek to find the best fitting value function</li>
<li>Given the agent’s experience (“training data”)</li>
</ul>
</div>
<div id="section-33" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Bringing together</p>
<ul>
<li>Deep Neural Networks for function approximation</li>
<li>and Q-Learning for action-value function learning</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-architecture-overview" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN Architecture Overview</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/mnih_dqn_architecture.png" style="height:400px;width:auto;" alt="../data/09/mnih_dqn_architecture.png" />
</figure>
</div>
<p>“we developed a novel agent, a deep Q-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network known as deep neural networks.”</p>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-overview-learning-cycle" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Overview Learning Cycle</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/LearningCycle_C3.png" style="height:480px;width:auto;" alt="../data/09/LearningCycle_C3.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-gradient-descent-iterative-search-for-minimum" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Gradient Descent: Iterative Search for Minimum</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-35" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/errorLandscape_Gradient.png" style="height:420px;width:auto;" alt="../data/09/errorLandscape_Gradient.png" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="iterative-optimization-algorithm" class="right box block">
<h2 class="right">Iterative optimization algorithm</h2>
<ul>
<li>start from an initial point <span class="math inline">\(\vec{u}\)</span> (initial guess) on the error function</li>
<li>Iterate (<span class="math inline">\(k\)</span> = iteration, <span class="math inline">\(\eta\)</span> = learning rate):</li>
</ul>
<p>Determine the gradient at that point and make a step: <span class="math inline">\(\vec{w}_{k+1} = \vec{w}_{k} - \eta \nabla_{\vec{w}} E^{(i)}(\vec{w})\)</span></p>
<p>Until: <span class="math inline">\(\nabla_{\vec{w}} E^{(i)}(\vec{w}) \approx 0\)</span></p>
<p>Then we found a minimum.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="summary-q-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Summary Q-Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Q-Learning is an off-policy approach for learning of action-values <span class="math inline">\(q(s,a)\)</span>:</p>
<ul>
<li>Next action is chosen using behaviour policy <span class="math inline">\(A_{t+1} \sim b(·|S_t)\)</span></li>
<li>But we consider alternative successor action <span class="math inline">\(a&#39; \sim \pi(·|S_t)\)</span></li>
<li>And update <span class="math inline">\(q(S_t, A_t)\)</span> towards value of alternative action</li>
</ul>
<p><span class="math display">\[
q&#39;(S_t,A_t) \leftarrow q(S_t,A_t) + \alpha_t \Big(R_{t+1} + \gamma \max_{a&#39;}q(S_{t+1},a&#39;) - q(S_t,A_t)\Big)
\]</span></p>
</div>
<div id="section-36" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="goal-of-dqn-approximation-of-q-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Goal of DQN: Approximation of Q-Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="incremental">
<ul class="incremental">
<li class="fragment">Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP).</li>
<li class="fragment">It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter.</li>
<li class="fragment">One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment.</li>
<li class="fragment">Q-learning learns estimates of the optimal Q-values of an MDP, which means that behavior can be dictated by taking actions greedily with respect to the learned Q-values.</li>
</ul>
</div>
</div>
<div id="section-37" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="problems-for-rl-and-deep-neural-networks" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Problems for RL and Deep Neural Networks</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Reinforcement learning is known to be unstable when a nonlinear function approximator such as a neural network is used to represent the action-value function.</p>
<p><br />
</p>
<p>This instability has several causes:</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">the correlations present in the sequence of observations,</li>
<li class="fragment">the fact that small updates to <span class="math inline">\(Q\)</span> may significantly change the policy and therefore change the data distribution,</li>
<li class="fragment">and the correlations between the action-values and the target values</li>
</ul>
</div>
</div>
<div id="section-38" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="possible-problems-for-function-approximation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Possible Problems for Function Approximation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Goal: apply efficiency and flexibility of TD methods to realistic problems</p>
</div>
<div id="problem-deadly-triad" class="box block">
<h2>Problem: Deadly Triad</h2>
<p>Approach is …</p>
<ul>
<li>off-policy,</li>
<li>employs non-linear function approximation,</li>
<li>and uses bootstrapping.</li>
</ul>
<p>Combined: can become unstable or does not converge!</p>
</div>
<div id="section-39" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="deep-q-networks" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Deep Q-Networks</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>… improved and stabilized training of Q-learning when using a Deep Neural Network for function approximation.</p>
<p>Two innovative mechanisms:</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment"><em>Experience Replay:</em> use a replay buffer for storing experiences.</li>
<li class="fragment">Periodically Update <em>Target network</em> that are employed for bootstrapping.</li>
</ul>
</div>
</div>
<div id="section-40" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="key-ideas-for-dqn-1.-experience-replay" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Key Ideas for DQN: 1. Experience Replay</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><em>“First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.”</em></p>
<ul>
<li>All episode steps <span class="math inline">\(e_t = (S_t, A_t, R_t, S_{t+1})\)</span> are collected in one replay memory.</li>
<li>During Q-learning updates: sample steps are drawn randomly from the replay memory.</li>
</ul>
</div>
<div id="section-41" class="box block fragment">
<h2></h2>
<p>Experience replay</p>
<ul>
<li>improves data efficiency,</li>
<li>removes correlations in the observation sequences,</li>
<li>and smooths over changes in the data distribution</li>
</ul>
</div>
<div id="section-42" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>; <a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="key-ideas-for-dqn-2.-stabilize-bootstrapping" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Key Ideas for DQN: 2. Stabilize Bootstrapping</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><em>“Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.”</em></p>
</div>
<div id="section-43" class="box block fragment">
<h2></h2>
<p>Periodically Updated Target:</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">Q is optimized towards target values that are only periodically updated.</li>
<li class="fragment">The Q network is cloned and kept frozen as the optimization target every <span class="math inline">\(C\)</span> steps (<span class="math inline">\(C\)</span> is a hyperparameter).</li>
</ul>
</div>
<p>This modification makes the training more stable as it overcomes the short-term oscillations.</p>
</div>
<div id="section-44" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="deep-q-network-overview" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Deep Q Network Overview</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/karpathy_qsa.jpg" style="height:360px;width:auto;" alt="../data/09/karpathy_qsa.jpg" />
</figure>
</div>
<p>3-dimensional state space (blue) and 2 actions (red); green nodes represent a NN.</p>
<p>Left: naive approach that takes multiple forward passes to find the argmax action. Right: more efficient approach, <span class="math inline">\(Q(s,a)\)</span> computation is effectively shared among the neurons in the network.</p>
</div>
<div id="section-45" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-karpathy_mdp" role="doc-biblioref">Karpathy 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-algorithm-preprocessing" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN Algorithm: Preprocessing</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Frame: <span class="math inline">\(210 \times 160\)</span> pixel images with a 128-colour palette</p>
</div>
<div id="preprocessing-mapping-colororangephis" class="box block">
<h2>Preprocessing: Mapping <span class="math inline">\(\color{orange}\phi(s)\)</span></h2>
<ul>
<li>remove flickering (max. value from two frames)</li>
<li>extract luminance from the RGB frame and rescale to <span class="math inline">\(84 \times 84\)</span></li>
<li>stack <span class="math inline">\(m=4\)</span> frames</li>
</ul>
</div>
<div id="input-to-convolutional-neural-network" class="box block">
<h2>Input to Convolutional Neural Network</h2>
<p><span class="math inline">\(84 \times 84 \times 4\)</span> image</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-q-network-architecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN: Q-Network Architecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Input <span class="math inline">\(84 \times 84 \times 4\)</span> image</p>
</div>
<div id="convolutional-network" class="box block">
<h2>Convolutional Network</h2>
<ul>
<li>Convolutional Layer, 32 filters, <span class="math inline">\(8 \times 8\)</span>, stride 4, ReLU</li>
<li>Convolutional Layer, 64 filters, <span class="math inline">\(4 \times 4\)</span>, stride 2, ReLU</li>
<li>Convolutional Layer, 64 filters, <span class="math inline">\(3 \times 3\)</span>, stride 1, ReLU</li>
</ul>
</div>
<div id="output-layer" class="box block">
<h2>Output Layer</h2>
<p>Fully-connected linear layer with a single output for each valid action (4 to 18 depending on game).</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-training" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN: Training</h1>
<div class="layout row columns">
<div class="area left">
<div id="tasks" class="left box block">
<h2 class="left">Tasks</h2>
<ul>
<li>49 Atari 2600 games</li>
<li>results available for comparison (human performance)</li>
<li>different network trained for each game</li>
<li>clip rewards to <span class="math inline">\(-1, 0, +1\)</span></li>
<li>frame skipping <span class="math inline">\(k=4\)</span></li>
</ul>
</div>
</div><div class="area center">
<div id="features-drl" class="center box block">
<h2 class="center">Features DRL</h2>
<ul>
<li><span class="math inline">\(\varepsilon\)</span>-greedy: during first million frames scale from <span class="math inline">\(1.0\)</span> to <span class="math inline">\(0.1\)</span>. Afterwards <span class="math inline">\(\varepsilon = 0.1\)</span>.</li>
<li>Replay buffer size 1 million recent frames</li>
<li>training time: 50 million frames (around 38 days of gaming)</li>
</ul>
</div>
</div><div class="area right">
<div id="hyperparameter-nn" class="right box block">
<h2 class="right">Hyperparameter NN</h2>
<ul>
<li>Optimizier: RMSProp</li>
<li>minibatch size <span class="math inline">\(32\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="evaluation-of-trained-agents" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Evaluation of Trained Agents</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>play each game 30 times</li>
<li>for up to 5 min each time</li>
<li>with different initial random conditions (‘no- op’)</li>
<li><span class="math inline">\(\varepsilon\)</span>-greedy policy with <span class="math inline">\(\varepsilon = 0.05\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-algorithm" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN Algorithm</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-4e8ce884.tex.svg" style="height:540px;width:auto;" alt="code-4e8ce884.tex.svg" />
</figure>
</div>
</div>
<div id="section-46" class="footer box block">
<h2 class="footer"></h2>
<p><span class="math inline">\(\color{blue}\text{Agent part of the algorithm }\color{Cerulean}\text{($\hat{Q}$)}; \color{red}\text{ Environment interaction} \color{orange}\text{ (history of inputs)}; \color{Green}\text{Experience Replay}\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dqn-example-puckworld" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DQN Example: Puckworld</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="iframe" style="width:1200px;height:auto;">
<iframe style="width:100%;height:480px;" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html">

</iframe>
</figure>
</div>
</div>
<div id="section-47" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-karpathy_mdp" role="doc-biblioref">Karpathy 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-game-space-invaders" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example Game: Space Invaders</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="video" data-data-autoplay="true" style="height:auto;width:800px;">
<video controls="1" style="height:auto;width:100%;" data-src="../data/09/41586_2015_BFnature14236_MOESM123_ESM.mov#t=1,">

</video>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="learning-over-time" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Learning over Time</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/41586_2015_Article_BFnature14236_Fig2_HTML.jpg" style="height:400px;width:auto;" alt="../data/09/41586_2015_Article_BFnature14236_Fig2_HTML.jpg" />
</figure>
</div>
</div>
<div id="section-48" class="footer box block">
<h2 class="footer"></h2>
<p>Average score achieved per episode. a) Space Invaders. b) Seaquest. c) Average predicted action-value on a held-out set of states on Space Invaders. d) Average predicted action-value on Seaquest.</p>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-learning-in-breakout" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Learning in Breakout</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="video" data-data-autoplay="true" style="height:auto;width:800px;">
<video controls="1" style="height:auto;width:100%;" data-src="../data/09/41586_2015_BFnature14236_MOESM124_ESM.mov#t=1,">

</video>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-learning-in-breakout-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Learning in Breakout 2</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/09/mnih_breakoutresults.svg" style="height:auto;width:100%;" alt="../data/09/mnih_breakoutresults.svg" />
</figure>
</div>
</div>
<div id="section-49" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="results---superhuman-performance" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Results - “Superhuman” Performance</h1>
<div class="layout row columns">
<div class="area left">
<div id="summary" class="left box block">
<h2 class="left">Summary</h2>
<p><em>“Our DQN method outperforms the best existing reinforcement learning methods on 43 [out of 49] of the games without incorporating any of the additional prior knowledge about Atari 2600 games used by other approaches.”</em></p>
</div>
</div><div class="area right">
<div id="section-50" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/mnih_dqn_results.png" style="height:450px;width:auto;" alt="../data/09/mnih_dqn_results.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-51" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="drawbacks-of-dqn-and-other-drl-methods" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Drawbacks of DQN (and other DRL methods)</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-52" class="left box block">
<h2 class="left"></h2>
<div class="incremental">
<ul class="incremental">
<li class="fragment">Delayed Rewards (makes Credit Assignment even more difficult)</li>
<li class="fragment">Overfitting towards a specific niche and showing no generalization</li>
<li class="fragment">many real world scenarios are non-Markovian or non-stationary (e.g. when other agents are co-adapting)</li>
</ul>
</div>
</div>
</div><div class="area right">
<div id="section-53" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/mnih_dqn_results.png" style="height:480px;width:auto;" alt="../data/09/mnih_dqn_results.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-54" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="delayed-rewards" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Delayed Rewards</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-55" class="left box block">
<h2 class="left"></h2>
<p><em>“games demanding more temporally extended planning strategies still constitute a major challenge for all existing agents including DQN (e.g., Montezuma’s Revenge)”</em></p>
<ul>
<li>It’s difficult to explore large state spaces with sparse and delayed rewards.</li>
<li>An Objective Function might not provide good guidance where to continue exploration.</li>
</ul>
</div>
</div><div class="area right">
<div id="section-56" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezumas_revenge.jpg" style="height:480px;width:auto;" alt="../data/10/montezumas_revenge.jpg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-57" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>; <a href="#ref-kulkarni2016" role="doc-biblioref">Kulkarni u. a. 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="evolutionary-robotics-perspective" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Evolutionary Robotics Perspective</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Landscapes induced by objective functions are often deceptive – the objective function is misleading.</p>
<p>Often, stepping stones are required — initially, objective might get worse.</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/lehman2011deceptive.svg" style="height:320px;width:auto;" alt="../data/10/lehman2011deceptive.svg" />
</figure>
</div>
</div>
<div id="section-58" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-lehman2011" role="doc-biblioref">Lehman und Stanley 2011</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dealing-with-delayed-rewards" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Dealing with Delayed Rewards</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p><em>“When the environment provides delayed rewards, we adopt a strategy to first learn ways to achieve intrinsically generated goals, and subsequently learn an optimal policy to chain them together.”</em></p>
<h2 id="approach">Approach</h2>
<ul>
<li>Use a hierarchical representation.</li>
<li>Exploration: Driven by a search for novelty (<strong>Intrinsic Motivation</strong>). This tries to cover all possible behaviors during exploration, find stepping stones.</li>
</ul>
</div>
<div class="col40">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/kulkarni_hierarchical.svg" style="height:480px;width:auto;" alt="../data/10/kulkarni_hierarchical.svg" />
</figure>
</div>
</div>
</div>
<div id="section-59" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kulkarni2016" role="doc-biblioref">Kulkarni u. a. 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="intrinsic-motivation---constructing-a-representation" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Intrinsic Motivation - Constructing a Representation</h1>
<div class="layout row columns">
<div class="area left">
<div id="early-learning-phase" class="left box block">
<h2 class="left">Early Learning Phase</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezuma_im_early.jpg" style="height:420px;width:auto;" alt="../data/10/montezuma_im_early.jpg" />
</figure>
</div>
<p>Select key as (sub)goal – but fails.</p>
</div>
</div><div class="area right">
<div id="intermediate-phase" class="right box block">
<h2 class="right">Intermediate Phase</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezuma_im_ladder.jpg" style="height:420px;width:auto;" alt="../data/10/montezuma_im_ladder.jpg" />
</figure>
</div>
<p>Select ladder successful as goal.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-60" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-kulkarni2016" role="doc-biblioref">Kulkarni u. a. 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="intrinsic-motivation---constructing-an-abstraction" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Intrinsic Motivation - Constructing an Abstraction</h1>
<div class="layout row columns">
<div class="area left">
<div id="intermediate-phase-1" class="left box block">
<h2 class="left">Intermediate Phase</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezuma_im_ladder.jpg" style="height:420px;width:auto;" alt="../data/10/montezuma_im_ladder.jpg" />
</figure>
</div>
<p>Select ladder successful as goal.</p>
</div>
</div><div class="area right">
<div id="intermediate-phase-2" class="right box block">
<h2 class="right">Intermediate Phase</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezuma_im_key.jpg" style="height:420px;width:auto;" alt="../data/10/montezuma_im_key.jpg" />
</figure>
</div>
<p>Select key successful as goal.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-61" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-kulkarni2016" role="doc-biblioref">Kulkarni u. a. 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="learning-with-intrinsic-motivation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Learning with Intrinsic Motivation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/10/montezuma_learning.svg" style="height:480px;width:auto;" alt="../data/10/montezuma_learning.svg" />
</figure>
</div>
</div>
<div id="section-62" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-kulkarni2016" role="doc-biblioref">Kulkarni u. a. 2016</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="problematic-markov-assumption" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Problematic: Markov Assumption</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In many real world scenarios the Markov Property does not hold.</p>
<p><br />
</p>
<p>In ATARI games: many require information on direction of movement.</p>
<p><br />
</p>
<p>Simple Solution: add information from different time steps – as an input 4 frames were used.</p>
</div>
<div id="but-difficult-in-non-stationary-environments" class="box block fragment">
<h2>But difficult in non-stationary environments</h2>
<ul>
<li>in game like scenarios, opponents can use different strategies (rock-paper-scissor),</li>
<li>or other agents co-adapt and learn over time as well.</li>
</ul>
<!--# Recap: Approximation used in RL

**Goal**: In value-based approaches: estimate the long-term return for a given state (or state, action pair).

$$
\begin{eqnarray*}
v(s) &=& \mathbb{E}[G_t \vert S_t = s] \\
&=& \fragment{\mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] }
\end{eqnarray*}$$

**General Approach**: Learn from experience -- how to balance exploration-exploitation? This can be very unefficient and costly.

## Different dimensions / decisions to consider: {.fragment}

* bootstrapping: Monte-Carlo $\leftrightarrow$ Temporal Difference Learning
* on-policy $\leftrightarrow$ off-policy
* function approximation

# 1) Bootstrapping {.columns}

## Monte-Carlo {.left}

Compute estimate directly from samples without bootstrapping. Averaging over many trajectories.

* requires full episodes and termination
* no bias
* but variance, quite noisy

## {.right}

::: r-stack

![](../data/10/cliffworld-path1.svg){height=450px}
![](../data/10/cliffworld-path2.svg){height=450px .fragment}
![](../data/10/cliffworld-mc.svg){height=450px .fragment}

:::

$$v(S_t) \leftarrow R_{t+1} + \gamma R_{t+2} + \dots $$

## {.footer}

[@greydanus2019the]

# 1) Bootstrapping {.columns}

## Temporal Difference Learning {.left}

Use single-step (or $n-$step) reward from experience, but estimate following return from current value function

* can be applied online and in non-terminating environments
* variance is low
* but can be biased (over-estimation bias)

## {.right}

::: r-stack

![](../data/10/cliffworld-path1.svg){height=450px}
![](../data/10/cliffworld-path2.svg){height=450px .fragment}
![](../data/10/cliffworld-td.svg){height=450px .fragment}

:::

$$v(S_t) \leftarrow R_{t+1} + \gamma v(S_{t+1}) $$

## {.footer}

[@greydanus2019the]

# 1) Bootstrapping - Path-Perspective

Intersections between two trajectories are handled differently:


![](../data/10/cliffworld-comparison.png){width=1200px}

Unlike MC, TD updates merge intersections so that the return flows backwards to all preceding states.

## {.footer}

[@greydanus2019the]

# 1) Bootstrapping - Comparison Estimation of values {.columns}

## Monte-Carlo {.left}

Monte Carlo is averaging over real trajectories.

## TD learning {.right}

TD Learning is averaging over all possible paths. The nested expectation corresponds to averaging across all possible paths.

## Comparison {.bottom}

* TD learning never averages over fewer trajectories than Monte Carlo (there are never fewer simulated trajectories than real ones)
* Therefore, TD learning has the chance to average over more of the experience.
* TD learning is the better estimator (lower variance) which explains why TD tends to outperform Monte Carlo in tabular environments.

## {.footer}

[@greydanus2019the]

#

TODO

Silver FA.pdf - 30, 31 convergence tables

# Q-Functions: SARSA

TODO: https://distill.pub/2019/paths-perspective-on-value-learning/

And handling overestimation bias = double q learning

SARSA

# 2) Going Off-policy

+ learn from different policy, from observation, reuse

as solution: Importance sampling

Introduces deviation : Action selection deviates
Ansatz: importance sampling

Q-Learning

# 3) function approximation
Only appr.
Again: select action and estimate action value using same function
Danger: change FCt from one sample -> affects whole fct approximation - moving target

see as well https://distill.pub/2019/paths-perspective-on-value-learning/
-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-karpathy_mdp" class="csl-entry">
Karpathy, Andrej. 2015. <span>„REINFORCEjs“</span>. <a href="https://github.com/karpathy/reinforcejs">https://github.com/karpathy/reinforcejs</a>.
</div>
<div id="ref-kulkarni2016" class="csl-entry">
Kulkarni, Tejas D., Karthik Narasimhan, Ardavan Saeedi, und Joshua B. Tenenbaum. 2016. <span>„Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation“</span>. <em>CoRR</em> abs/1604.06057. <a href="http://arxiv.org/abs/1604.06057">http://arxiv.org/abs/1604.06057</a>.
</div>
<div id="ref-lehman2011" class="csl-entry">
Lehman, J., und K. O. Stanley. 2011. <span>„Abandoning Objectives: Evolution Through the Search for Novelty Alone“</span>. <em>Evolutionary Computation</em> 19 (2): 189–223. doi:<a href="https://doi.org/10.1162/EVCO_a_00025">10.1162/EVCO_a_00025</a>.
</div>
<div id="ref-mnih-dqn-2015" class="csl-entry">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, u. a. 2015. <span>„Human-level control through deep reinforcement learning“</span>. <em>Nature</em> 518 (7540): 529–33. <a href="http://dx.doi.org/10.1038/nature14236">http://dx.doi.org/10.1038/nature14236</a>.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-weng2018rl" class="csl-entry">
Weng, Lilian. 2018. <span>„A (Long) Peek into Reinforcement Learning“</span>. <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("3a8b4e3c7.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
