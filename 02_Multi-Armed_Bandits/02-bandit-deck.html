<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 2 - Multi-Armed Bandits to
Sequences of Decisions</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">
  
</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>2 - Multi-Armed Bandits to Sequences
of Decisions</h2>
                  </div>

         
         
                  <div class="author"> Prof. Dr. Malte Schilling </div>
         
                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>
         
         
         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Exploration-Exploitation Tradeoff</li>
<li>Decision Making: Multi-Armed Bandit
<ul>
<li>Strategies
<ul>
<li><span class="math inline">\(\varepsilon\)</span>-greedy</li>
<li>UCB</li>
<li>Gradient-based Action Selection</li>
</ul></li>
</ul></li>
<li>Sequences of States – towards Sequential Decision Making</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="admin---exercises" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Admin - Exercises</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Exercise slot is Friday morning, 10:15 AM to 11:45, M5.</li>
<li>Solutions (for programming exercises in python) should be submitted by small teams of two or three persons. Everybody has to be able to present the solution during meetings.</li>
<li>First exercise sheet out today – due next Wednesday (upload in learnweb).</li>
</ul>
<p>Register groups: Send an email to malte.schilling@uni-muenster.de with name and account information for each group member until Tuesday, 25.10.2022!</p>
<p>Join the learnweb course!</p>
<!--
- There will be a new exercise sheet every two weeks. Discussion and presentation of the solution will be during the exercise slot as well every second week.
- For the weeks in between: will be additional time for introduction of concepts, discussion, questions ...
- First exercise slot: 21.10.2021, brief introduction to python.
- Honor code: Do collaborate and discuss together, but write up and code independently. Do not show anyone else your writeup or code or post it online (unless specified).-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="übungstermin-freitag-21.10.2022" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Übungstermin Freitag, 21.10.2022</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Inhalte:</p>
<ul>
<li>Troubleshooting für Python Aufgaben</li>
<li>Basierend auf dem Übungszettel: zusammenstellen der notwendigen numpy Funktionen (arrays, grundlegende statistische Funktionen, …) und plot Funktionen (für eine random policy).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-types-of-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Types of Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/01/learning_forms_6.png" aria-label="Distinction of different types of learning" title="Distinction of different types of learning" style="height:540px;width:auto;" alt="../data/01/learning_forms_6.png" />
<figcaption>
Distinction of different types of learning
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="parts-of-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Parts of Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_1.png" style="height:300px;width:auto;" alt="../data/02/rl_cycle_1.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>A <strong>policy</strong> <em>is</em> the agent’s behavior – chooses an action.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Provides reward</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="examples-of-rewards" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Examples of Rewards</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Fly stunt manoeuvres in a helicopter
<ul>
<li>positive reward: if following a desired trajectory</li>
<li>negative reward: when crashing</li>
</ul></li>
<li>Manage an investment portfolio – reward is given as the money in that account</li>
<li>playing computer games – reward directly given as score</li>
<li>Locomotion of a robot
<ul>
<li>positive reward for movement in the correct direction</li>
<li>negative reward / cost: falling over (not maintaining hight); energy consumed</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-explore-or-exploit-information-for-decision-making" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Explore or Exploit Information for Decision Making</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:480px;">
<img src="../data/01/ucb_ai_exploration_vs_exploitation.png" style="height:auto;width:100%;" alt="../data/01/ucb_ai_exploration_vs_exploitation.png" />
</figure>
</div>
<p>Decision Making: sticking to a good past experience might make you miss out on even better options, but at least you can be confident to get something good.</p>
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:300px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<ul>
<li>Homework answers: What is your example of an exploration-exploitation tradeoff?</li>
<li>What would be the reward?</li>
</ul>
</div>
</div>
<div id="section-1" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kleinCS188" role="doc-biblioref">Klein und Abbeel 2014</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="exploitation-exploration-tradeoff" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Exploitation-Exploration Tradeoff</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-2" class="top box block">
<h2 class="top"></h2>
<p>As information of a novel environment is incomplete, we need to gather information for good decisions and want to keep the risk under control.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="exploitation" class="left box block">
<h2 class="left">Exploitation</h2>
<p>Taking advantage of the best known option.</p>
</div>
</div><div class="area right">
<div id="exploration" class="right box block">
<h2 class="right">Exploration</h2>
<p>Take some risk to collect information about unknown options.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-3" class="bottom box block">
<h2 class="bottom"></h2>
<p>An optimal long-term strategy may involve short-term sacrifices, e.g. learning from failure during exploration helps us avoid a certain action.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="examples-for-exploitation-exploration-tradeoff" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Examples for Exploitation-Exploration Tradeoff</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Restaurant Selection
<ul>
<li>Exploitation: Go to your favourite restaurant</li>
<li>Exploration: Try a new restaurant</li>
</ul></li>
<li>Oil Drilling
<ul>
<li>Exploitation: Drill at the best known location</li>
<li>Exploration: Drill at a new location</li>
</ul></li>
<li>Game Playing
<ul>
<li>Exploitation: Play the move you believe is best</li>
<li>Exploration: Play an experimental move</li>
</ul></li>
</ul>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rewards" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Rewards</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>A <strong>reward</strong> <span class="math inline">\(R_t\)</span> is a scalar feedback given to the agent from the environment</li>
<li>that indicates how well the agent is doing (at time t).</li>
<li>The agent aims to maximise the cumulative reward over time.</li>
</ul>
</div>
<div id="reward-hypothesis" class="box block">
<h2>Reward Hypothesis</h2>
<p>All goals can be represented as maximization of a scalar reward (an expected cumulative reward).</p>
</div>
<div id="return-preliminary-def." class="definition box block">
<h2 class="definition">Return (preliminary Def.)</h2>
<p>The accumulated reward over time is called the return <span class="math inline">\(G_t\)</span>.</p>
<p>$G_t = R_{t+1} + R_{t+2} + … $</p>
</div>
<div id="section-5" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="multi-armed-bandit" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Multi-Armed Bandit</h1>
</div>
</div>
</section>
<section id="multi-armed-bandit-overview" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Multi-Armed Bandit Overview</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:100%;">
<img src="../data/02/An-illustration-of-the-multi-armed-bandit-problem.png" style="height:auto;width:100%;" alt="../data/02/An-illustration-of-the-multi-armed-bandit-problem.png" />
</figure>
</div>
</div>
<div id="section-6" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-gao2021bandit" role="doc-biblioref">Gao u. a. 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="multi-armed-bandit-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Multi-Armed Bandit</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Idea: in a casino with multiple slot machines of unknown probabilities.</p>
<p>Which action (slot machine) should you choose for optimal reward?</p>
<div class="media">
<figure class="image" style="height:auto;width:800px;">
<img src="../data/02/weng_bern_bandit.png" style="height:auto;width:100%;" alt="../data/02/weng_bern_bandit.png" />
</figure>
</div>
<p>Following a naive approach, one would gather information over a long time to get a true estimate of each of the probabilities. But as a consequence one will spend too much time on suboptimal actions.</p>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="multi-armed-bandit-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Multi-Armed Bandit</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Bernoulli multi-armed bandit can be described as a tuple of <span class="math inline">\(\langle \mathcal{A}, \mathcal{R} \rangle\)</span>, where:</p>
<ul>
<li><span class="math inline">\(K\)</span> machines with reward probabilities, <span class="math inline">\({\theta_1,...,\theta_K}\)</span></li>
<li>for each time step <span class="math inline">\(t\)</span>, take an action <span class="math inline">\(a\)</span> on one slot machine and receive a reward <span class="math inline">\(r\)</span>.</li>
<li><span class="math inline">\(\mathcal{A}\)</span> is a set of actions (one for each slot machine) – the value of action <span class="math inline">\(a\)</span> is the expected reward <span class="math inline">\(Q(a) = \mathbb{E} [r_t \vert a] = \theta\)</span> (when at time <span class="math inline">\(t\)</span> action <span class="math inline">\(a_t\)</span> is choosing the <span class="math inline">\(i\)</span>-th machine <span class="math inline">\(Q(a_t)=\theta_i\)</span>).</li>
<li><span class="math inline">\(\mathcal{R}\)</span> is the reward function (a distribution on rewards). For a Bernoulli bandit,we observe a reward in a stochastic fashion (<span class="math inline">\(r_t= \mathcal{R}(a_t)\)</span> may return 1 with probability <span class="math inline">\(Q(a_t)\)</span>).</li>
</ul>
<p>This is a simplified version of a Markov decision process (there is no state <span class="math inline">\(\mathcal{S}\)</span>).</p>
</div>
<div id="section-8" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="parts-of-decision-making-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Parts of Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-9" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_1.png" style="height:300px;width:auto;" alt="../data/02/rl_cycle_1.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-1" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>A <strong>policy</strong> <em>is</em> the agent’s behavior – chooses an action.</li>
<li><strong>Value function</strong>: Keep track of the value of an action.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-1" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Provides reward</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="action-values" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Action values</h1>
<div class="layout">
<div class="area">
<div id="action-value" class="definition box block">
<h2 class="definition">Action value</h2>
<p>The action value for action a is the expected reward</p>
<p><span class="math display">\[Q_t(a) = \mathbb{E}\Big[ R_t \mid A_{t}=a \Big]\]</span></p>
</div>
<div id="section-10" class="box block">
<h2></h2>
<p>A simple estimate is the average of the sampled rewards:</p>
<p><span class="math display">\[\begin{eqnarray*}
Q_t(a) &amp;=&amp; \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} \\
&amp;=&amp; \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
\end{eqnarray*}\]</span></p>
<p>with <span class="math inline">\(\mathbb{1}\)</span> being the indicator function, therefore we count choosing action <span class="math inline">\(a\)</span> as <span class="math inline">\(\sum_{i=1}^t \mathbb{1}(A_i = a)\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maximization-of-cumulative-reward" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Maximization of cumulative reward</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Goal is to maximize <strong>cumulative reward</strong> <span class="math inline">\(\sum_{t=1}^T r_t\)</span> (the return <span class="math inline">\(G\)</span>).<br />
</p>
<p>The optimal action produces the maximal reward. Deviating from that action leads to a potential loss or <strong>regret</strong>.<br />
</p>
<p>The probability for the optimal reward <span class="math inline">\(\theta^*\)</span> of the optimal action <span class="math inline">\(a^*\)</span> is <span class="math display">\[\theta^{*}=Q(a^{*})=\max_{a \in \mathcal{A}} Q(a) = \max_{1 \leq i \leq K} \theta_i\]</span><br />
</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="regret" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Regret</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The regret of an action <span class="math inline">\(a\)</span> is given as</p>
<p><span class="math display">\[\Delta_a = v_{*} - q(a)
\]</span></p>
<p>and the regret for choosing the optimal action is zero.</p>
</div>
<div id="total-regret-as-a-loss" class="definition box block">
<h2 class="definition">Total regret as a loss</h2>
<p>The loss function is the total regret for not selecting the optimal action up to the time step <span class="math inline">\(T\)</span> <span class="math display">\[
L_T = \mathbb{E} \Big[ \sum_{t=1}^T \big( \theta^{*} - Q(a_t) \big) \Big] = \sum_{t=1}^T \Delta_{A_t}
\]</span></p>
</div>
<div id="section-11" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="an-example-bandit-problem" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>An example bandit problem</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:720px;">
<img src="../data/02/sutton_2_1_bandit.svg" style="height:auto;width:100%;" alt="../data/02/sutton_2_1_bandit.svg" />
</figure>
</div>
<p>Random action values <span class="math inline">\(q_*(a), a=1,...,10\)</span> (selected from a normal distribution, zero mean, unit std. dev.).</p>
</div>
<div id="section-12" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-maximize-the-return" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Maximize the return</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:300px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>Your task: * Come up with an algorithm that maximizes the accumulated reward and improves over time. * Consider advantages and disadvantages of your approach. * Why is the experimental procedure given in the python example problematic?</p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="beispiel-in-python" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Beispiel in Python</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="sized sageCell print" style="width:1200px;height:500px;">
<p>Generating 10 bandits.</p>
<div class="media">
<figure class="code">
<pre class="processed python"><code>import time
import numpy as np
# Bandit class is taken from https://github.com/lilianweng/multi-armed-bandit
class BernoulliBandit(object):

    def __init__(self, n, probas=None):
        assert probas is None or len(probas) == n
        self.n = n
        if probas is None:
            np.random.seed(int(time.time()))
            self.probas = [np.random.random() for _ in range(self.n)]
        else:
            self.probas = probas

        self.best_proba = max(self.probas)

    def generate_reward(self, i):
        # The player selected the i-th machine.
        if np.random.random() &lt; self.probas[i]:
            return 1
        else:
            return 0
</code></pre>
</figure>
</div>
<p>Printing the different assigned reward probabilities (reward is one).</p>
<div class="media">
<figure class="code">
<pre class="processed python"><code>b = BernoulliBandit(10)
print(&quot;Randomly generated Bernoulli bandit has reward probabilities:\n&quot;, b.probas)
print(&quot;The best machine has index: {} and proba: {}&quot;.format(max(range(10), key=lambda i: b.probas[i]), max(b.probas)))</code></pre>
</figure>
</div>
<p>Now implement an algorithm that learns to select a bandit (follow the example for random_alg).</p>
<div class="media">
<figure class="code">
<pre class="processed python"><code>rand_alg_return = np.zeros(1000)
rand_return = 0.
your_alg_return = np.zeros(1000)
your_return = 0.
for i in range(0, 1000):
	select_bandit = np.random.randint(10)
	rand_return += b.generate_reward(select_bandit)
	rand_alg_return[i] = rand_return
	
	your_return += b.generate_reward(0)
	your_alg_return[i] = your_return
print(rand_return/1000., your_return/1000.)</code></pre>
</figure>
</div>
<p>Plot the accumulated return over time.</p>
<div class="media">
<figure class="code">
<pre class="processed python"><code>import matplotlib.pyplot as plt
plt.plot(rand_alg_return)
plt.plot(your_alg_return)
plt.show()</code></pre>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="the-greedy-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>The greedy policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Produce an estimate for action values</li>
<li>Select action with highest value <span class="math inline">\(A_t = \arg\max\limits_{a \in \mathcal{A}} \hat{Q}_t(a)\)</span></li>
<li>This is exploiting the currently available information.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="varepsilon-greedy-strategy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><span class="math inline">\(\varepsilon\)</span>-Greedy Strategy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Take the best action most of the time: <span class="math inline">\(\hat{a}^{*}_t = \arg\max_{a \in \mathcal{A}} \hat{Q}_t(a)\)</span></li>
<li>But with <span class="math inline">\(p=\varepsilon\)</span> do random exploration.</li>
</ul>
<p>Best action is estimated from the collected action values from past experience (averaging the rewards for that action): <span class="math display">\[
\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^t r_i \cdot\mathbb{1}(a_i = a)
\]</span></p>
<p><span class="math inline">\(\mathbb{1}\)</span> – binary indicator function for selecting an action</p>
<p><span class="math inline">\(N_t(a) = \sum_{i=1}^t \mathbb{1}(a_i = a)\)</span> – counting how many times an action was selected</p>
</div>
<div id="section-13" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="varepsilon-greedy-algorithm" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><span class="math inline">\(\varepsilon\)</span>-greedy Algorithm</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1200px;">
<img src="../data/02/sb_epsilongreedy.png" style="height:auto;width:100%;" alt="../data/02/sb_epsilongreedy.png" />
</figure>
</div>
<p>Note: Incremental implementation to update estimates of value function is efficient.</p>
</div>
<div id="section-14" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="performance-of-varepsilon-greedy-strategy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Performance of <span class="math inline">\(\varepsilon\)</span>-Greedy Strategy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/02/sutton_2_2_epsilongreedy.svg" style="height:auto;width:100%;" alt="../data/02/sutton_2_2_epsilongreedy.svg" />
</figure>
</div>
<p>Average performance of <span class="math inline">\(\varepsilon\)</span>-greedy method (averaged over 2000 runs) for 10-arm bandit problem.</p>
</div>
<div id="section-15" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="adaptation-of-varepsilon-greedy-strategy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Adaptation of <span class="math inline">\(\varepsilon\)</span>-Greedy Strategy</h1>
<div class="layout">
<div class="area">
<div id="drawback-of-varepsilon-greedy-strategy" class="box block">
<h2>Drawback of <span class="math inline">\(\varepsilon\)</span>-Greedy Strategy:</h2>
<div class="incremental">
<ul class="incremental">
<li class="fragment">during exploration we are randomly selecting actions — even though we might already have established bad actions</li>
</ul>
</div>
</div>
<div id="possible-solutions" class="box block fragment">
<h2>Possible Solutions</h2>
<div class="incremental">
<ul class="incremental">
<li class="fragment">decrease <span class="math inline">\(\varepsilon\)</span> over time</li>
<li class="fragment">keep track of actions – of an estimate of how uncertain we are about this action (addressing the exploitation-exploration tradeoff)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="übungszettel-hinweise" class="section slide level1" data-background-color="#ff7f0e">
<div class="decker">
<div class="alignment">
<h1>Übungszettel Hinweise</h1>
</div>
</div>
</section>
<section id="aufgabe-1.1-implementation-multi-armed-bandit" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Aufgabe 1.1: Implementation Multi-Armed Bandit</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 60fr 40fr;">
<ul>
<li>Implementierung in Python eines <span class="math inline">\(k=4\)</span>-Bandit-Problems</li>
<li>Abbildungen zu return über die Zeit und Anteil an Auswahl der optimalen Option (mehrere runs durchführen und auswerten)</li>
<li>Einfluss des Parameters <span class="math inline">\(\varepsilon\)</span></li>
<li>und der Initialbedingungen.</li>
<li><span class="math inline">\(\varepsilon\)</span> über die Zeit verringern und Beobachtungen erklären.</li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sb_fig2_2.png" aria-label="(Sutton und Barto 2018)" title="(Sutton und Barto 2018)" style="height:480px;width:auto;" alt="../data/02/sb_fig2_2.png" />
<figcaption>
<span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="aufgabe-2-stationarität-von-multi-armed-bandits" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Aufgabe 2: Stationarität von Multi-Armed Bandits</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Voraussetzung der untersuchten Verfahren: <strong>Stationäre Probleme</strong> – die Wahrscheinlichkeiten für das Vergeben von rewards (und später Zustandsübergängen) bleibt gleich.</p>
<p>Aufgabe:</p>
<ul>
<li>Warum ist dies ein Problem für die vorgestellten Verfahren?</li>
<li>Stellen sie dies dar durch eine Anpassung des Mult-Armed-Bandit-Problems.</li>
</ul>
<p>(als Zusatzpunkte!)</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="übungstermin-freitag-21.10.2022-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Übungstermin Freitag, 21.10.2022</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Inhalte:</p>
<ul>
<li>Troubleshooting für Python Aufgaben</li>
<li>Basierend auf dem Übungszettel: zusammenstellen der notwendigen numpy Funktionen (arrays, grundlegende statistische Funktionen, …) und plot Funktionen</li>
<li>für eine random policy</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimism-in-the-face-of-uncertainty" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimism in the Face of Uncertainty</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="r-stack">
<p><span class="media"><span class="figure image" style="height:auto;width:auto;"><img src="../data/02/dm_actionvf_uncertainties.png" style="height:400px;width:auto;" alt="../data/02/dm_actionvf_uncertainties.png" /></span></span> <span class="media"><span class="figure fragment image" style="height:auto;width:auto;"><img src="../data/02/dm_actionvf_uncertainties_2.png" style="height:400px;width:auto;" alt="../data/02/dm_actionvf_uncertainties_2.png" /></span></span> <span class="media"><span class="figure fragment image" style="height:auto;width:auto;"><img src="../data/02/dm_actionvf_uncertainties_3.png" style="height:400px;width:auto;" alt="../data/02/dm_actionvf_uncertainties_3.png" /></span></span></p>
</div>
<p>The more uncertain we are about an action-value, the more important it is to explore that action as it could turn out to be the best action.</p>
<p>One approach: Keep an (over-)optimistic estimate for each action.</p>
</div>
<div id="section-16" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="upper-confidence-bounds-ucb" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Upper Confidence Bounds (UCB)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Idea: favor exploration of actions that still have a strong potential to have an optimal value.</p>
<p>This potential is measured as an upper confidence bound of the reward value <span class="math inline">\(\hat{U}_t(a)\)</span>. It depends on how often we have tried an action (<span class="math inline">\(N_t(a)\)</span>).</p>
<p>Therefore, the true reward value is bound to:</p>
<p><span class="math display">\[Q(a) \leq \hat{Q}_t(a) + \hat{U}_t(a)\]</span></p>
<p><br />
</p>
<p>In UCB algorithm, actions are selected greedily in order to maximize the upper confidence bound: <span class="math display">\[a^{UCB}_t = \arg\max\limits_{a \in \mathcal{A}} \hat{Q}_t(a) + \hat{U}_t(a)
= \arg\max\limits_{a \in \mathcal{A}} \hat{Q}_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \]</span></p>
</div>
<div id="section-17" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="average-performance-of-ucb" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Average Performance of UCB</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/02/sutton_2_4_ucb.svg" style="height:auto;width:100%;" alt="../data/02/sutton_2_4_ucb.svg" />
</figure>
</div>
<p>Average performance of UCB (averaged over 2000 runs) for 10-arm bandit problem.</p>
<p>UCB outperforms <span class="math inline">\(\varepsilon\)</span>-greedy.</p>
</div>
<div id="section-18" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-regret" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering regret</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Regret was defined as <span class="math inline">\(\Delta_a = v_{*} - q(a)\)</span>.</p>
<p>It depends on</p>
<ul>
<li>the action count – how often each action was selected</li>
<li>and on the overall collected regret <span class="math inline">\(L_T = \mathbb{E} \Big[ \sum_{t=1}^T \big( \theta^{*} - Q(a_t) \big) \Big]\)</span>.</li>
</ul>
</div>
<div id="section-19" class="box block fragment">
<h2></h2>
<p>Therefore, we should aim for algorithm that avoid large regrets.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="regret-in-the-case-of-ucb" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Regret in the case of UCB</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Selection of <span class="math inline">\(a_t\)</span>:</p>
<p><span class="math display">\[ a_t = \arg\max\limits_{a \in \mathcal{A}} Q_t(a) + c\sqrt{\frac{\log t}{N_t(a)}}
\]</span></p>
<p>Intuitively,</p>
<ul>
<li>When <span class="math inline">\(\Delta_a\)</span> is large, the action will not be selected</li>
<li>unless <span class="math inline">\(N_t(a)\)</span> is (comparatively) small.</li>
</ul>
<p>It can be shown, that <span class="math inline">\(\Delta_a \cdot N_t(a) \leq O(\log t)\)</span> for all <span class="math inline">\(a\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="comparison-of-regret" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Comparison of regret</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/silver_regret.png" style="height:240px;width:auto;" alt="../data/02/silver_regret.png" />
</figure>
</div>
</div>
<div id="asymptotic-total-regret-lai1985" class="theorem box block">
<h2 class="theorem">Asymptotic total regret <span class="citation">(<a href="#ref-lai1985" role="doc-biblioref">Lai und Robbins 1985</a>)</span></h2>
<p>Asymptotic total regret is at least logarithmic in number of steps</p>
<p><span class="math display">\[
\lim\limits_{t\rightarrow \infty}L_t \geq \log t \sum\limits_{a | \Delta_a&gt; 0} \frac{\Delta_a}{KL(\mathcal{R}_a||\mathcal{R}_{a*})}
\]</span></p>
</div>
<div id="section-20" class="box block">
<h2></h2>
<p>With <span class="math inline">\(KL(\mathcal{R}_a||\mathcal{R}_{a*}) \propto \Delta_a^2\)</span> – note: Hard problems have similar-looking arms with different means.</p>
</div>
<div id="section-21" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-directly-learning-how-to-act" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy – Directly learning how to act</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Consider action selection as a probability distribution:</p>
<ul>
<li>For each action: Consider an (estimated) preference <span class="math inline">\(H_t(a)\)</span> of that action which</li>
<li>can be directly used to express a probability for selecting that action (as a soft-max distribution)</li>
</ul>
<p><span class="math display">\[p(A_t = a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)\]</span></p>
</div>
<div id="gradient-bandit-algorithm" class="box block fragment">
<h2>Gradient Bandit Algorithm</h2>
<p>Learn / adapt the action preference function directly using stochastic gradient ascent:</p>
<p><span class="math display">\[\begin{eqnarray*}
H_{t+1}(A_t) &amp;=&amp;  H_{t}(A_t) + \alpha (R_t - \bar{R}_t) (1-\pi_t(A_t)), &amp;\text{ and} \\
H_{t+1}(a) &amp;=&amp; H_{t}(A_t) - \alpha (R_t - \bar{R}_t) \pi_t(a) &amp; \text{ for all }a \neq A_t
\end{eqnarray*}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="average-performance-of-gradient-bandit-algorithm" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Average Performance of Gradient Bandit Algorithm</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/gradient_bandit.png" style="height:360px;width:auto;" alt="../data/02/gradient_bandit.png" />
</figure>
</div>
<p>Shown are variations of the <span class="math inline">\(\alpha\)</span> parameter and the importance of including the mean reward in the update for unbiasing (for brown curve this baseline is removed and the added bias of <span class="math inline">\(+4\)</span> affects learning).</p>
</div>
<div id="section-22" class="footer box block">
<h2 class="footer"></h2>
<p>Following <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="further-variations-for-solving-bandit-problems" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Further variations for Solving Bandit Problems</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>employing an optimistic estimate of the action value instead of a default initialization</li>
<li>Bayesian UCB: introduce a prior assumption for the reward distribution – as a Gaussian – and use confidence intervals</li>
<li>Thompson Sampling – formulate action selection as a probabilistic process itself (selecting an action with a probability that estimates it is optimal)</li>
</ul>
</div>
<div id="section-23" class="footer box block">
<h2 class="footer"></h2>
<p>For further explanation see <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span> or <span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sequences-of-states" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Sequences of States</h1>
</div>
</div>
</section>
<section id="recap-forms-of-learning" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Forms of Learning</h1>
<div class="layout row columns">
<div class="area left">
<div id="what-is-special-in-reinforcement-learning" class="left box block">
<h2 class="left">What is special in Reinforcement Learning?</h2>
<ul>
<li>There is no supervisor, only a reward signal.</li>
<li><strong>Sequential</strong>: Time really matters (non i.i.d data).</li>
<li>Feedback can be delayed, not instantaneous.</li>
<li>Agent’s actions <em>affect the subsequent data</em> it receives</li>
</ul>
</div>
</div><div class="area right">
<div id="section-24" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:480px;">
<img src="../data/02/silver_formsOfLearning.svg" style="height:auto;width:100%;" alt="../data/02/silver_formsOfLearning.svg" />
</figure>
</div>
</div>
<div id="section-25" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sequential-decision-making" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><em>Sequential</em> Decision Making</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The interaction between agent and environment is a <strong>sequence</strong> of actions and returned observations plus rewards.</p>
<p><br />
</p>
<p>Goal of the agent: select actions to maximise total future reward</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">But actions may have long term consequences and reward may be delayed.</li>
<li class="fragment">It may be better to sacrifice immediate reward to gain more long-term reward</li>
</ul>
</div>
<p>An agent’s <strong>policy</strong> <span class="math inline">\(\pi(s) = a\)</span> describes which action <span class="math inline">\(a\)</span> an agent selects depending on the current state.</p>
<p>For the stochastic state, a policy is a probability distribution over actions: <span class="math inline">\(\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]\)</span>.</p>
</div>
<div id="section-26" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-27" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:360px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-2" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action.</li>
<li>Value-Function: Estimate of achievable return from a state.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-2" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li><strong>State</strong>: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov process is a memoryless random process – a sequence of random states <span class="math inline">\(S_1, S_2, ...\)</span> with the Markov property.</p>
</div>
<div id="markov-property" class="definition box block">
<h2 class="definition">Markov Property</h2>
<p>A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math inline">\(P(S_{t+1} | S_t) = P(S_{t+1} | S_1,...,S_t)\)</span></p>
<p>States captures all relevant information from the history (“The future is independent of the past given the present”).</p>
</div>
<div id="markov-process-markov-chain" class="box block">
<h2>Markov Process (Markov Chain)</h2>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>A Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/silver_student_1.svg" style="height:450px;width:auto;" alt="../data/02/silver_student_1.svg" />
</figure>
</div>
<p>Before turning to action, we focus on a simpler class of Markov Chains: the Markov Reward Process. We could sample trajectories from it.</p>
</div>
<div id="section-28" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov reward process is a Markov Chain with associated reward values.</p>
</div>
<div id="markov-reward-process-consists-of-def." class="definition box block">
<h2 class="definition">Markov reward process consists of (def.)</h2>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> is a reward function, <span class="math inline">\(\mathcal{R}_s = \mathbb{E}[R_{t+1}|S_t=s]\)</span></li>
<li><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0,1]\)</span> (used to give a higher value to rewards that are closer in time)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-29" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:360px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-3" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action <strong>depending on current state</strong>.</li>
<li>Value-Function: Estimate of achievable return from a state.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-3" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li><strong>State</strong>: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-policy" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Policy</h1>
<div class="layout row columns">
<div class="area left">
<div id="task" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="policy-representation" class="right box block">
<h2 class="right">Policy Representation</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_policy.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_policy.svg" />
</figure>
</div>
<p>Arrows represent policy <span class="math inline">\(\pi(s)\)</span> for all the states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-30" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="task-1" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="state-value" class="right box block">
<h2 class="right">State Value</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_value.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_value.svg" />
</figure>
</div>
<p>Shown are values <span class="math inline">\(v_{\pi}(s)\)</span> for the different states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-31" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="partially-observable-environments" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Partially Observable Environments</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-32" class="top box block">
<h2 class="top"></h2>
<p>We will focus mostly on fully observable environments: The agent state contains all necessary information required for making an informed decision.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="partial-observability" class="left box block">
<h2 class="left">Partial Observability</h2>
<p>The agent only indirectly experiences the environment, e.g. no exact position information, but only relying on a camera.</p>
<p>Importantly, he might not be able to distinguish states.</p>
<p>The agent, therefore, must construct its own internal state representation (which could, e.g., include information on history).</p>
</div>
</div><div class="area right">
<div id="section-33" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/dm_maze_pomdp.png" style="height:400px;width:auto;" alt="../data/02/dm_maze_pomdp.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making-2" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-34" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:240px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-4" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action <em>depending on current state</em>.</li>
<li>Value-Function: Estimate of achievable return from a state (following <span class="math inline">\(\pi\)</span>).</li>
<li><strong>Model</strong>: A <em>predictor</em> of the environment.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-4" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li>State: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="model" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Model</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A model allows to predict how the environment will react (as a probability distribution).</p>
<ul>
<li><span class="math inline">\(\mathcal{P}\)</span> predicts the subsequent state: <span class="math display">\[\mathcal{P}^a_{ss&#39;} \approx p(S_{t+1}=s&#39; | S_t = s, A_t = a)\]</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> predicts the next reward: <span class="math display">\[\mathcal{R}^a_{s} = \mathbb{E}(R_{t+1} | S_t = s, A_t = a)\]</span></li>
</ul>
<p>A model does not give us immediately a good policy.</p>
<p>But it allows us to plan – test possible alternative actions.</p>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="categorization-of-reinforcement-learning-agents" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Categorization of Reinforcement Learning Agents</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-36" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Value Based
<ul>
<li>No Policy (Implicit)</li>
<li>Value Function</li>
</ul></li>
<li>Policy Based
<ul>
<li>Policy</li>
<li>No Value Function</li>
</ul></li>
<li>Actor Critic
<ul>
<li>Policy</li>
<li>Value Function</li>
</ul></li>
</ul>
</div>
</div><div class="area right">
<div id="section-37" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/01/silver_RL_categorization.svg" style="height:auto;width:100%;" alt="../data/01/silver_RL_categorization.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-38" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gao2021bandit" class="csl-entry">
Gao, Chongming, Wenqiang Lei, Xiangnan He, Maarten Rijke, und Tat-Seng Chua. 2021. <span>„Advances and Challenges in Conversational Recommender Systems: A Survey“</span>.
</div>
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-kleinCS188" class="csl-entry">
Klein, Dan, und Pieter Abbeel. 2014. <span>„UC Berkeley CS188 Intro to AI“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-lai1985" class="csl-entry">
Lai, T. L., und H. Robbins. 1985. <span>„Asymptotically Efficient Adaptive Allocation Rules“</span>. <em>Advances in Applied Mathematics</em> 6: 4–22.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-weng2018bandit" class="csl-entry">
Weng, Lilian. 2018. <span>„The Multi-Armed Bandit Problem and Its Solutions“</span>. <a href="https://The Multi-Armed Bandit Problem and Its Solutions">The Multi-Armed Bandit Problem and Its Solutions</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("1de720785.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
