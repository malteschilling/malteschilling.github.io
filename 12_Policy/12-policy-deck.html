<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 12 - Policy Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">

</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>12 - Policy Learning</h2>
                  </div>



                  <div class="author"> Prof. Dr. Malte Schilling </div>

                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>


         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="recap-convergence-of-prediction-algorithms" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Convergence of Prediction Algorithms</h1>
<div class="layout">
<div class="area">
<div class="box block">
<table>
<thead>
<tr class="header">
<th align="center">On/Off-Policy</th>
<th>Algorithm</th>
<th align="center">Table Lookup</th>
<th align="center">Linear Approx.</th>
<th align="center">Non-Linear</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">On-Policy</td>
<td>MC</td>
<td align="center">✔︎</td>
<td align="center">✔︎</td>
<td align="center">✔︎</td>
</tr>
<tr class="even">
<td align="center"></td>
<td>TD</td>
<td align="center">✔︎</td>
<td align="center">✔︎</td>
<td align="center">❌</td>
</tr>
<tr class="odd">
<td align="center">Off-Policy</td>
<td>MC</td>
<td align="center">✔︎</td>
<td align="center">✔︎</td>
<td align="center">✔︎</td>
</tr>
<tr class="even">
<td align="center"></td>
<td>TD</td>
<td align="center">✔︎</td>
<td align="center">❌</td>
<td align="center">❌</td>
</tr>
</tbody>
</table>
</div>
<div id="section" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-deadly-triad" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Deadly Triad</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Three elements that can interfere with divergence: With only two present, we can avoid instability.</p>
<ul>
<li>Function approximation: Allows for generalization, required for realistic tasks.</li>
<li>Bootstrapping: Update targets based on the existing estimates leads to more efficient learning.</li>
<li>Off-Policy training: Crucial for Online Learning, meaning learning from a single stream of experience how to adapt many policies.</li>
</ul>
</div>
<div id="countermeasures" class="box block">
<h2>Countermeasures</h2>
<p>We can’t counter all these with at once, but DQN showed a way how to balance these issues (target networks for example). Other possibilities include, e.g., using <span class="math inline">\(n\)</span>-step returns.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-drawbacks-of-dqn-and-other-drl-methods" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Drawbacks of DQN (and other DRL methods)</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-1" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Delayed Rewards (makes Credit Assignment even more difficult)</li>
<li>Overfitting towards a specific niche and showing no generalization</li>
<li>many real world scenarios are non-Markovian or non-stationary (e.g. when other agents are co-adapting)</li>
</ul>
</div>
</div><div class="area right">
<div id="section-2" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/09/mnih_dqn_results.png" style="height:480px;width:auto;" alt="../data/09/mnih_dqn_results.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-3" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-mnih-dqn-2015" role="doc-biblioref">Mnih u. a. 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-deceptive-objectives" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Deceptive Objectives</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/deceptive_function.png" style="height:320px;width:auto;" alt="../data/11/deceptive_function.png" />
</figure>
</div>
<p>Landscapes induced by objective functions are often deceptive – the objective function is misleading.</p>
<p>Often, stepping stones are required — initially, objective might get worse.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Policy Gradient Methods</p>
<ul>
<li>Stochastically sample variations of parameters</li>
<li>Analytical Policy Gradient derivation and collecting
<ul>
<li>For Contextual PGs</li>
<li>For Episodic Cases</li>
<li>For the average reward case</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-learning" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Policy Learning</h1>
</div>
</div>
</section>
<section id="policy-based-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy-Based Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We already approximated the value or action-value function using parameters <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
V_\theta(s) ≈ V_\pi(s), Q_\theta(s, a) ≈ Q_\pi(s, a)
\]</span></p>
<p>A policy was generated directly from the value function, e.g., using <span class="math inline">\(\varepsilon\)</span>-greedy.</p>
<p>But we can also directly parametrise the policy <span class="math inline">\(\pi_\theta(s,a) = P(a | s,\theta)\)</span></p>
<p>We will focus again on model-free reinforcement learning.</p>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-based-and-policy-based-rl" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Value-Based and Policy-Based RL</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-5" class="left box block">
<h2 class="left"></h2>
<h3 id="value-based">Value Based</h3>
<ul>
<li>Learnt Value Function</li>
<li>Implicit policy (e.g. <span class="math inline">\(\varepsilon\)</span>-greedy)</li>
</ul>
<h3 id="policy-based">Policy Based</h3>
<ul>
<li>No Value Function</li>
<li>Learnt Policy</li>
</ul>
<h3 id="actor-critic">Actor-Critic</h3>
<ul>
<li>Learnt Value Function</li>
<li>Learnt Policy</li>
</ul>
</div>
</div><div class="area right">
<div id="overview-approaches" class="right box block">
<h2 class="right">Overview Approaches</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:640px;">
<img src="../decker/code/code-b9383c81.tex.svg" style="height:auto;width:100%;" alt="code-b9383c81.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="advantages-of-policy-based-rl" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Advantages of Policy-Based RL</h1>
<div class="layout row columns">
<div class="area left">
<div id="advantages" class="left box block">
<h2 class="left">Advantages:</h2>
<ul>
<li>True objective</li>
<li>Easy extended to high-dimensional or continuous action spaces</li>
<li>Can learn stochastic policies</li>
<li>Sometimes policies are simple while values and models are complex, e.g., complicated dynamics, but optimal policy is always “move forward”</li>
</ul>
</div>
</div><div class="area right">
<div id="disadvantages" class="right box block">
<h2 class="right">Disadvantages:</h2>
<ul>
<li>Could get stuck in local optima</li>
<li>Obtained knowledge can be specific, does not always generalise well</li>
<li>Does not necessarily extract all useful information from the data (when used in isolation)</li>
</ul>
</div>
<div id="section-6" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-learning-objective" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Policy Learning Objective</h1>
</div>
</div>
</section>
<section id="policy-objective-functions" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Objective Functions</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><strong>Goal</strong>: given policy <span class="math inline">\(\pi_\theta(s, a)\)</span>, find best parameters <span class="math inline">\(\theta\)</span></p>
<p>How do we measure the quality of a policy <span class="math inline">\(\pi_\theta\)</span>?</p>
<ul>
<li>In episodic environments: We can use the average total return per episode</li>
<li>In continuing environments: We can use the average reward per step.</li>
</ul>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-objective-functions-episodic-environments" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Objective Functions: Episodic Environments</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Episodic-return objective:</p>
<p><span class="math display">\[
\begin{eqnarray*}
J_G(\theta) &amp;=&amp; \mathbb{E}_{S_0 \sim d_0, \pi_0 } \Big( \sum_{t=0}^{\infty} \gamma^t R_{t+1}\Big)\\
&amp;=&amp; \mathbb{E}_{S_0 \sim d_0, \pi_0 } \Big(G_0\Big)\\
&amp;=&amp; \mathbb{E}_{S_0 \sim d_0 } \Big( \mathbb{E}_{\pi_0 } (G_t | S_t = S_0) \Big)\\
&amp;=&amp; \mathbb{E}_{S_0 \sim d_0 } \Big( v_{\pi_0} (G_t | S_t = S_0) \Big)
\end{eqnarray*}
\]</span></p>
<p>where <span class="math inline">\(d_0\)</span> is the start-state distribution. This objective equals the expected value of the start state.</p>
</div>
<div id="section-8" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-objective-functions-average-reward-objective" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Objective Functions: Average Reward Objective</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\begin{eqnarray*}
J_R(\theta) &amp;=&amp; \mathbb{E}_{\pi_\theta } \Big( R_{t+1}\Big)\\
&amp;=&amp; \mathbb{E}_{S_t \sim d_{\pi_\theta} } \Big( \mathbb{E}_{A_t \sim \pi_{\theta}(S_t) } (R_{t+1} | S_t) \Big)\\
&amp;=&amp; \sum_s d_{\pi_\theta} (s) \sum_a \pi_\theta(s,a) \sum_r p(r |s,a)r
\end{eqnarray*}
\]</span></p>
<p>where <span class="math inline">\(d_\pi(s) = p(S_t = s | \pi)\)</span> is the probability of being in state <span class="math inline">\(s\)</span> in the long run (Think of it as the ratio of time spent in <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>).</p>
</div>
<div id="section-9" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="learning-on-policy-objective-function-episodic-case" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Learning on Policy Objective Function (episodic case)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{S_0 \sim d_0, \pi } (G) = v_{\pi_\theta} (S_0)
\]</span></p>
<p>For optimization: We want to use gradient ascent on <span class="math inline">\(j\)</span> over <span class="math inline">\(\theta\)</span>. Why could this be difficult?</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">When we change <span class="math inline">\(\theta\)</span>, action selection (<span class="math inline">\(\pi\)</span>) is affected.</li>
<li class="fragment">But this further affects which states will be visited and how often (plus corresponding rewards). This depends on the environment!</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradients" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Policy Gradients</h1>
</div>
</div>
</section>
<section id="policy-optimisation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Optimisation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Policy based reinforcement learning is an optimization problem.</p>
<ul>
<li>Find <span class="math inline">\(\theta\)</span> that maximises <span class="math inline">\(J(\theta)\)</span></li>
<li>We will focus on stochastic gradient ascent, which is often quite efficient (and easy to use with deep nets).</li>
<li>Different approaches that do not use gradient
<ul>
<li>Hill climbing / simulated annealing</li>
<li>Genetic algorithms / evolutionary strategies</li>
</ul></li>
</ul>
</div>
<div id="section-10" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-11" class="left box block">
<h2 class="left"></h2>
<p><strong>Approach</strong>: Ascent the gradient of the objective <span class="math inline">\(J(\theta)\)</span></p>
<p><span class="math display">\[
\Delta \theta = \alpha \nabla_\theta J(\theta)
\]</span></p>
<ul>
<li>Policy Gradient <span class="math display">\[\nabla_\theta J(\theta) = \begin{pmatrix}
 \frac{\partial J(\theta)}{\partial \theta_1}\\
 \vdots \\
 \frac{\partial J(\theta)}{\partial \theta_n}
  \end{pmatrix}\]</span></li>
<li><span class="math inline">\(\alpha\)</span> is a step-size parameter</li>
</ul>
<p>Stochastic policies help ensure that <span class="math inline">\(J(\theta)\)</span> is (mostly) smooth.</p>
</div>
</div><div class="area right">
<div id="gradient-ascent" class="right box block">
<h2 class="right">Gradient Ascent</h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/11/gradient_ascent.png" style="height:auto;width:100%;" alt="../data/11/gradient_ascent.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gradients-on-parameterized-policies" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Gradients on parameterized policies</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>How to compute this gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span>?</p>
<ul>
<li>Approximate stochastically.</li>
<li>Assume policy <span class="math inline">\(\pi_\theta\)</span> is differentiable almost everywhere (e.g., neural net).</li>
</ul>
<!--## Reward

For average reward $\nabla_\theta J(\theta) = \nabla \theta \mathbb{E}_{\pi_\theta} (R)$

How does $\mathbb{E}(R)$ depend on $\theta$?
-->
</div>
<div id="section-12" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="approximate-gradients-by-finite-differences" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>1) Approximate Gradients by Finite Differences</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>To evaluate policy gradient of <span class="math inline">\(\pi_\theta (s, a)\)</span></p>
<ul>
<li>For each dimension <span class="math inline">\(k \in [1, n]\)</span>
<ul>
<li>Estimate <span class="math inline">\(k-\)</span>th partial derivative of objective function w.r.t. <span class="math inline">\(\theta_k\)</span></li>
<li>By perturbing <span class="math inline">\(\theta_k\)</span> by small amount <span class="math inline">\(\varepsilon\)</span> in <span class="math inline">\(k-\)</span>th dimension <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_k} ≈ \frac{J(\theta_k + \varepsilon) - J(\theta)}{\varepsilon}\]</span> <!--where $u_k$ is unit vector with $1$ in $k-$th component, $0$ elsewhere --></li>
</ul></li>
<li>Uses <span class="math inline">\(n\)</span> evaluations to compute policy gradient in <span class="math inline">\(n\)</span> dimensions</li>
</ul>
</div>
<div id="characteristics" class="box block fragment">
<h2>Characteristics:</h2>
<ul>
<li>Simple, noisy, inefficient - but sometimes effective</li>
<li>Works for arbitrary policies, even if policy is not differentiable</li>
</ul>
</div>
<div id="section-13" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-learning-to-walk-on-aibo" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Learning to Walk on AIBO</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-14" class="left box block">
<h2 class="left"></h2>
<p><strong>Goal</strong>: learn a fast walk on AIBO robot that can be applied in RoboCup</p>
<p>Parametrize AIBO walking policy and learn directly these parameters through reinforcement learning</p>
</div>
</div><div class="area right">
<div id="section-15" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image">
<img src="../data/11/kohl_2004_training_env.png" alt="../data/11/kohl_2004_training_env.png" />
</figure>
</div>
</div>
<div id="section-16" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="aibo-parameters-of-gait-for-stepping" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>AIBO Parameters of gait for Stepping</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-17" class="left box block">
<h2 class="left"></h2>
<p>Each leg is performing a half-elliptical locus. Each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two.</p>
<p>Four parameters define this elliptical locus:</p>
<ul>
<li>length of the ellipse;</li>
<li>height of the ellipse;</li>
<li>position of the ellipse on x axis;</li>
<li>position of the ellipse on y axis.</li>
</ul>
<p><span class="math inline">\(= 12\)</span> param. (front, rear, <span class="math inline">\(+\)</span> height, timing)</p>
</div>
</div><div class="area right">
<div id="section-18" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/kohl_2004_aibo_leg.png" style="height:480px;width:auto;" alt="../data/11/kohl_2004_aibo_leg.png" />
</figure>
</div>
</div>
<div id="section-19" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-policy-gradient-approach" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Policy Gradient Approach</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-20" class="left box block">
<h2 class="left"></h2>
<p><strong>Goal</strong>: optimize forward speed as the sole objective function.</p>
<p><strong>Approach</strong>: Policy gradient reinforcement learning – consider possible sets of parameter assignments that define a policy which is then executed on the robot.</p>
<p>Gradient is estimated in parameter space, and then moved towards an optimum.</p>
</div>
</div><div class="area right">
<div id="section-21" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/kohl_2004_averaging_gradient.png" style="height:480px;width:auto;" alt="../data/11/kohl_2004_averaging_gradient.png" />
</figure>
</div>
</div>
<div id="section-22" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-policy-gradient-approach-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Policy Gradient Approach</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-23" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Start with <span class="math inline">\(t\)</span> random policies <span class="math inline">\({R_1, \dots, R_t}\)</span> near initial policiy <span class="math inline">\(\pi\)</span>: <span class="math inline">\(R_i = {\theta_1 + \Delta_1, \dots, \theta_N + \Delta_N}\)</span> with <span class="math inline">\(\Delta_j\)</span> chosen randomly from <span class="math inline">\(+\epsilon_j, 0, −\epsilon_j\)</span> (<span class="math inline">\(\epsilon_j\)</span> is a fixed value that is small relative to <span class="math inline">\(\theta_j\)</span>)</li>
<li>Evaluate all policies on actual robot.</li>
<li>Estimate gradient in each dimension: averaging over score variations wrt. variation in that parameter.</li>
</ul>
</div>
</div><div class="area right">
<div id="section-24" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/kohl_2004_averaging_gradient.png" style="height:480px;width:auto;" alt="../data/11/kohl_2004_averaging_gradient.png" />
</figure>
</div>
</div>
<div id="section-25" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-results-learning-of-gaits" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Results – Learning of Gaits</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-26" class="left box block">
<h2 class="left"></h2>
<p>Use <span class="math inline">\(t = 15\)</span> policies per iteration.</p>
<p>As there was significant noise in each evaluation, each set of parameters was evaluated three times.</p>
<p>Training was stopped after reaching a peak policy at 23 iterations, which amounted to just over 1000 field traversals in about 3 hours.</p>
</div>
</div><div class="area right">
<div id="section-27" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/11/kohl_2004_learning_curve.png" style="height:auto;width:100%;" alt="../data/11/kohl_2004_learning_curve.png" />
</figure>
</div>
</div>
<div id="section-28" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-results-comparison" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Results Comparison</h1>
<div class="layout row columns">
<div class="area left">
<div id="aibo-performance" class="left box block">
<h2 class="left">Aibo Performance</h2>
<p>Velocity (given in <span class="math inline">\(mm/s\)</span>) from different teams as of 2004.</p>
</div>
<div id="section-29" class="small box block">
<h2 class="small"></h2>
<table>
<thead>
<tr class="header">
<th>Team</th>
<th align="right">Hand-Tuned Gaits</th>
<th align="right">Learned Gaits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CMU (2002)</td>
<td align="right">200</td>
<td align="right"></td>
</tr>
<tr class="even">
<td>German Team</td>
<td align="right">230</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td><strong>UT Austin Villa</strong></td>
<td align="right">245</td>
<td align="right"><strong>291</strong></td>
</tr>
<tr class="even">
<td>UNSW</td>
<td align="right"><strong>254</strong></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td>Hornby (1999)</td>
<td align="right"></td>
<td align="right">170</td>
</tr>
<tr class="even">
<td>UNSW</td>
<td align="right"></td>
<td align="right">270</td>
</tr>
</tbody>
</table>
</div>
</div><div class="area right">
<div id="section-30" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/kohl_2004_learned_gait.png" style="height:80%;width:auto;" alt="../data/11/kohl_2004_learned_gait.png" />
</figure>
</div>
</div>
<div id="section-31" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="results-aibo-parameters-for-stepping" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Results – AIBO Parameters for Stepping</h1>
<div class="layout row columns">
<div class="area left">
<div id="found-parameters" class="left box block">
<h2 class="left">Found Parameters</h2>
</div>
<div id="section-33" class="tiny box block">
<h2 class="tiny"></h2>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th align="right">Initial V.</th>
<th align="right"><span class="math inline">\(\epsilon\)</span></th>
<th align="right">Best V.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Front locus:</strong></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td>(height)</td>
<td align="right">4.2</td>
<td align="right">0.35</td>
<td align="right">4.081</td>
</tr>
<tr class="odd">
<td>(x offset)</td>
<td align="right">2.8</td>
<td align="right">0.35</td>
<td align="right">0.574</td>
</tr>
<tr class="even">
<td>(y offset)</td>
<td align="right">4.9</td>
<td align="right">0.35</td>
<td align="right">5.152</td>
</tr>
<tr class="odd">
<td><strong>Rear locus:</strong></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td>(height)</td>
<td align="right">5.6</td>
<td align="right">0.35</td>
<td align="right">6.02</td>
</tr>
<tr class="odd">
<td>(x offset)</td>
<td align="right">0.0</td>
<td align="right">0.35</td>
<td align="right">0.217</td>
</tr>
<tr class="even">
<td>(y offset)</td>
<td align="right">-2.8</td>
<td align="right">0.35</td>
<td align="right">-2.982</td>
</tr>
<tr class="odd">
<td>Locus length</td>
<td align="right">4.893</td>
<td align="right">0.35</td>
<td align="right">5.285</td>
</tr>
<tr class="even">
<td>Locus skew mult.</td>
<td align="right">0.035</td>
<td align="right">0.175</td>
<td align="right">0.049</td>
</tr>
<tr class="odd">
<td>Front height</td>
<td align="right">7.7</td>
<td align="right">0.35</td>
<td align="right">7.483</td>
</tr>
<tr class="even">
<td>Rear height</td>
<td align="right">11.2</td>
<td align="right">0.35</td>
<td align="right">10.843</td>
</tr>
<tr class="odd">
<td>Cycle time</td>
<td align="right">0.704</td>
<td align="right">0.016</td>
<td align="right">0.679</td>
</tr>
<tr class="even">
<td>Time on ground</td>
<td align="right">0.5</td>
<td align="right">0.05</td>
<td align="right">0.430</td>
</tr>
</tbody>
</table>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div><div class="area right">
<div id="section-32" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/11/kohl_2004_aibo_leg.png" style="height:480px;width:auto;" alt="../data/11/kohl_2004_aibo_leg.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-learned-aibo-gaits" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Learned AIBO Gaits</h1>
<div class="layout row columns">
<div class="area left">
<div id="initial-gait" class="left box block">
<h2 class="left">Initial Gait</h2>
<div class="media">
<figure class="video" style="height:auto;width:100%;">
<video controls="1" allow="autoplay" data-autoplay="1" style="height:auto;width:100%;" data-src="../data/11/initial.mp4">

</video>
</figure>
</div>
</div>
</div><div class="area center">
<div id="learned-gait" class="center box block">
<h2 class="center">Learned Gait</h2>
<div class="media">
<figure class="video" style="height:auto;width:100%;">
<video controls="1" allow="autoplay" data-autoplay="1" style="height:auto;width:100%;" data-src="../data/11/finished.mp4">

</video>
</figure>
</div>
<div class="media">
<figure class="video" style="height:auto;width:100%;">
<video controls="1" allow="autoplay" data-autoplay="1" style="height:auto;width:100%;" data-src="../data/11/finished-front.mp4">

</video>
</figure>
</div>
</div>
</div><div class="area right">
<div id="learned-gait-images" class="right box block">
<h2 class="right">Learned Gait Images</h2>
<div class="media">
<figure class="image" style="height:auto;width:100%;">
<img src="../data/11/kohl_2004_learned_gait.png" style="height:auto;width:100%;" alt="../data/11/kohl_2004_learned_gait.png" />
</figure>
</div>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kohl2004aibo" role="doc-biblioref">Kohl und Stone 2004</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-gradients-on-parameterized-policies" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Gradients on parameterized policies</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>How to compute this gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span>?</p>
<ul>
<li>Approximate stochastically.</li>
<li>Assume policy <span class="math inline">\(\pi_\theta\)</span> is differentiable almost everywhere (e.g., neural net).</li>
</ul>
</div>
<div id="reward" class="box block">
<h2>Reward</h2>
<p>For average reward <span class="math inline">\(\nabla_\theta J(\theta) = \nabla \theta \mathbb{E}_{\pi_\theta} (R)\)</span></p>
<p>Remember: <span class="math inline">\(\mathbb{E}(R)\)</span> depend on <span class="math inline">\(\theta\)</span> because of policy, but also the state distribution is affected.</p>
</div>
<div id="section-36" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem-contextual-bandits" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient Theorem – Contextual Bandits</h1>
</div>
</div>
</section>
<section id="recap-policy-learning-in-multi-armed-bandits" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Policy Learning in Multi-Armed Bandits</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-37" class="left box block">
<h2 class="left"></h2>
<p>Consider action selection as a probability distribution:</p>
<ul>
<li>For each action: Consider an (estimated) preference <span class="math inline">\(H_t(a)\)</span> of that action which</li>
<li>can be directly used to express a probability for selecting that action (as a soft-max distribution)</li>
</ul>
<p><span class="math display">\[p(A_t = a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-bandits-with-states-contextual-bandits" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Bandits with States – Contextual bandits</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col40">
<p>How can we extend the multiarmed bandit approach to multiple states? Consider bandits with different states</p>
<ul>
<li>but episodes are still one step</li>
<li>actions do not affect state transitions</li>
<li>no long-term consequences</li>
</ul>
<p>Then, we want to estimate: <span class="math display">\[
q(s,a) = \mathbb{E} (R_{t+1} | S_t = s,A_t=a)
\]</span></p>
</div>
<div class="col60">
<p><div style="display:block; clear:both; height:100px;"></div></p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:720px;">
<img src="../decker/code/code-0abf4e62.tex.svg" style="height:auto;width:100%;" alt="code-0abf4e62.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-monte-carlo-sampling" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Monte Carlo Sampling</h1>
<div class="layout">
<div class="area">
<div id="gradient-bandit-algorithm" class="right box block">
<h2 class="right">Gradient Bandit Algorithm</h2>
<p>Learn / adapt the action preference function directly using stochastic gradient ascent:</p>
<p><span class="math display">\[\begin{eqnarray*}
H_{t+1}(A_t) &amp;=&amp;  H_{t}(A_t) + \alpha (R_t - \bar{R}_t) (1-\pi_t(A_t)), &amp;\text{ and} \\
H_{t+1}(a) &amp;=&amp; H_{t}(a) - \alpha (R_t - \bar{R}_t) \pi_t(a) &amp; \text{ for all }a \neq A_t
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-38" class="footer box block">
<h2 class="footer"></h2>
<p>Following <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Richard S. Sutton und Barto 2018</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="contextual-bandits-policy-gradient" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Contextual Bandits Policy Gradient</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>For the one-step case (a contextual bandit) we use <span class="math inline">\(J(\theta) = \mathbb{E}_{\pi_\theta} ( R(S, A))\)</span>. (Expectation is over <span class="math inline">\(d\)</span> (states) and <span class="math inline">\(\pi\)</span> (actions) and, for now, <span class="math inline">\(d\)</span> does not depend on <span class="math inline">\(\pi\)</span>).</p>
<ul>
<li>We cannot sample <span class="math inline">\(R_{t+1}\)</span> and then take a gradient as <span class="math inline">\(R_{t+1}\)</span> is just a number and does not depend on <span class="math inline">\(\theta\)</span>!</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="contextual-bandits-policy-gradient-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Contextual Bandits Policy Gradient</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>For the one-step case (a contextual bandit) we use <span class="math inline">\(J(\theta) = \mathbb{E}_{\pi_\theta} ( R(S, A))\)</span>. (Expectation is over <span class="math inline">\(d\)</span> (states) and <span class="math inline">\(\pi\)</span> (actions) and, for now, <span class="math inline">\(d\)</span> does not depend on <span class="math inline">\(\pi\)</span>).</p>
<ul>
<li>We cannot sample <span class="math inline">\(R_{t+1}\)</span> and then take a gradient as <span class="math inline">\(R_{t+1}\)</span> is just a number and does not depend on <span class="math inline">\(\theta\)</span>!</li>
<li>Instead, we use the identity: <span class="math display">\[
\nabla_\theta \mathbb{E}_{\pi_\theta} (R(S, A)) = \mathbb{E}_{\pi_\theta} (R(S, A) \nabla_\theta \log \pi(A|S)).
\]</span></li>
</ul>
<p>The right-hand side gives an expected gradient that can be sampled.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="proof-for-identity" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Proof for Identity</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Let <span class="math inline">\(r_{sa} = \mathbb{E} (R(S,A)|S=s,A=a)\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\theta \mathbb{E}_{\pi_\theta} (R(S, A)) &amp;&amp;= \color{blue}\nabla_\theta \sum_s d(s) \sum_a \pi_\theta (a|s) r_{sa} \\
&amp;&amp;\fragment{= \sum_s d(s) \sum_a r_{sa} \color{blue} \nabla_\theta \pi_\theta (a|s)}\\
&amp;&amp;\fragment{= \sum_s d(s) \sum_a r_{sa} \color{red}\pi_\theta (a|s) \frac{\color{blue} \nabla_\theta \pi_\theta (a|s)}{\pi_\theta (a|s)}\color{black}, \color{green}(\log f(x))&#39; \color{black} = \frac{\color{blue}f&#39;(x)}{\color{red}f(x)}}\\
&amp;&amp;\fragment{= \sum_s d(s) \sum_a r_{sa} \pi_\theta (a|s) \color{green}\nabla_\theta \log \pi_\theta (a|s)}\\
&amp;&amp;\fragment{= \mathbb{E}_{d, \pi_\theta} (R(S,A) \nabla_\theta \log \pi_\theta (A|S) )}
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-39" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="contextual-bandit-policy-gradient" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Contextual Bandit Policy Gradient</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\nabla_\theta \mathbb{E}_{\pi_\theta} (R(S, A)) = \mathbb{E}_{d, \pi_\theta} (R(S,A) \nabla_\theta \log \pi_\theta (A|S) )
\]</span></p>
<p>This is something we can sample!</p>
<ul>
<li>Our stochastic policy-gradient update is then <span class="math display">\[ \theta_{t+1} = \theta_t + \alpha R_{t+1} \nabla_\theta \log \pi_{\theta_t} (A_t|S_t).\]</span></li>
<li>In the expectation, this is following the actual gradient – so this is a pure (unbiased) stochastic gradient algorithm.</li>
<li>Intuition: increase probability for actions with high rewards.</li>
</ul>
</div>
<div id="section-40" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradients-reduce-variance" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradients: reduce variance</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Note that, in general</p>
<p><span class="math display">\[
\begin{eqnarray*}
\mathbb{E} ( b \nabla_\theta \log \pi (A_t|S_t) ) &amp;=&amp; \mathbb{E} \Big( \sum_a \color{blue}\pi (a|S_t) \color{black}b \color{green}\nabla_\theta \log \pi (a|S_t)\Big)\color{black}, \color{green}(\log f(x))&#39; \color{black} = \frac{\color{red}f&#39;(x)}{\color{blue}f(x)}\\
&amp;=&amp; \mathbb{E} \Big( b \color{red}\nabla_\theta \sum_a \pi_\theta(a|S_t) \color{black} \Big)\\
&amp;=&amp; \mathbb{E} (b \nabla_\theta \color{red}1 \color{black}) = 0
\end{eqnarray*}
\]</span></p>
<ul>
<li>This is true if <span class="math inline">\(b\)</span> does not depend on the action (but it can depend on the state).</li>
<li>Implies, we can subtract a baseline to reduce variance <span class="math display">\[
\theta_{t+1} =\theta_t + \alpha (R_{t+1} −b(S_t)) \nabla_\theta \log \pi_{\theta_t} (A_t|S_t).
\]</span></li>
</ul>
</div>
<div id="section-41" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-softmax-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Softmax Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Consider a softmax policy on action preferences <span class="math inline">\(h(s, a)\)</span> as an example</li>
<li>Probability of action is proportional to exponentiated weight</li>
</ul>
<p><span class="math display">\[
\pi_\theta (a|s) = \frac{e^{h(s,a)}}{\sum_b e^{h(s,b)}}
\]</span></p>
<ul>
<li>The gradient of the log probability is</li>
</ul>
<p><span class="math display">\[
\nabla_\theta \log \pi_\theta (A_t | S_t) = \underbrace{\nabla_\theta h(S_t, A_t)}_{\text{gradient of preference}} - \underbrace{\sum_a \pi_\theta (a | S_t) \nabla_\theta h(S_t,a)}_{\text{expected gradient of preference}}
\]</span></p>
</div>
<div id="section-42" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient Theorem</h1>
</div>
</div>
</section>
<section id="policy-gradient-theorem-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient Theorem</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The policy gradient approach also applies to (multi-step) MDPs.</p>
<ul>
<li>Replaces reward <span class="math inline">\(R\)</span> with long-term return <span class="math inline">\(G_t\)</span> or value <span class="math inline">\(q_\pi (s, a)\)</span></li>
<li>There are actually two policy gradient theorems <span class="citation">(<a href="#ref-sutton:nips12" role="doc-biblioref">R. S. Sutton u. a. 2000</a>)</span>:
<ul>
<li>average return per episode</li>
<li>average reward per step</li>
</ul></li>
</ul>
</div>
<div id="section-43" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
<div id="section-44" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem-episodic" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradient theorem (episodic)</h1>
<div class="layout">
<div class="area">
<div id="pg-theorem" class="theorem box block">
<h2 class="theorem">PG Theorem</h2>
<p>For any differentiable policy <span class="math inline">\(\pi_\theta (s, a)\)</span>, let <span class="math inline">\(d_0\)</span> be the starting distribution over states in which we begin an episode.</p>
<p>Then, the policy gradient of <span class="math inline">\(J(\theta) = \mathbb{E} (G_0 | S_0 ∼ d_0)\)</span> is</p>
<p><span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \Big( \sum_{t=0}^T \gamma^t q_{\pi_\theta} (S_t, A_t) \nabla_\theta \log \pi_\theta (A_t | S_t) | S_0 \sim d_0 \Big)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{eqnarray*}
q_\pi (s,a) &amp;=&amp; \mathbb{E}_\pi (G_t | S_t = s, A_t = a) \\
&amp;=&amp; \mathbb{E}_\pi ( R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a )
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-45" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradients-on-trajectories" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradients on trajectories</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Observation / Important as step from Bandit to MDP case:</p>
<ul>
<li>Policy gradients do not need to know the MDP dynamics</li>
<li>Kind of surprising: shouldn’t we know how the policy influences the states?</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="reminder-colorgreentextscore-function-trick-from-the-bandit-case" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Reminder: <span class="math inline">\(\color{green}\text{Score}\)</span> function trick from the Bandit Case</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Let <span class="math inline">\(r_{sa} = \mathbb{E} (R(S,A)|S=s,A=s)\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\theta \mathbb{E}_{\pi_\theta} (R(S, A)) &amp;=&amp;\nabla_\theta \sum_s d(s) \sum_a \pi_\theta (a|s) r_{sa} \\
&amp;=&amp; \sum_s d(s) \sum_a r_{sa} \nabla_\theta \pi_\theta (a|s)\\
&amp;=&amp; \sum_s d(s) \sum_a r_{sa} \pi_\theta (a|s) \frac{ \color{blue}\nabla_\theta \pi_\theta (a|s)}{\color{red}\pi_\theta (a|s)}\color{black}, \color{green}(\log f(x))&#39; \color{black} = \frac{\color{blue}f&#39;(x)}{\color{red}f(x)}\\
&amp;=&amp; \sum_s d(s) \sum_a r_{sa} \pi_\theta (a|s) \color{green}\nabla_\theta \log \pi_\theta (a|s)\\
&amp;=&amp; \mathbb{E}_{d, \pi_\theta} (R(S,A) \nabla_\theta \log \pi_\theta (A|S) )
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-46" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="episodic-policy-gradients-proof-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Episodic policy gradients: proof 1</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Consider trajectory <span class="math inline">\(\tau = S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots\)</span> with return <span class="math inline">\(G(\tau)\)</span>.</p>
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\theta J_\theta(\pi) &amp;=&amp; \nabla_\theta \mathbb{E} (G(\tau)) = \mathbb{E} ( G(\tau) \nabla_\theta \log p(\tau))\\
\end{eqnarray*}
\]</span></p>
<p>We used the score function “trick”: <span class="math display">\[
\begin{eqnarray*}
\nabla_\theta \log p(\tau) &amp;&amp;= \nabla_\theta \log \big(  p(S_0)\pi(A_0|S_0)p(S_1|S_0,A_0)\pi(A_1|S_1)\dots \big)\\
&amp;&amp;\fragment{= \nabla_\theta \big( \log p(S_0) + \log \pi(A_0|S_0) + \log p(S_1|S_0,A_0) + \log \pi(A_1|S_1)+ \dots \big)}\\
&amp;&amp;\fragment{= \nabla_\theta \big( \log \pi(A_0|S_0) + \log \pi(A_1|S_1)+ \dots \big)}\\
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-47" class="box block fragment">
<h2></h2>
<p>Therefore: <span class="math display">\[
\nabla_\theta J_\theta(\pi) = \mathbb{E}_\pi \big( G(\tau) \nabla_\theta \sum_{t=0}^T \log \pi(A_t|S_t) \big)
\]</span></p>
</div>
<div id="section-48" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="episodic-policy-gradients-proof-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Episodic policy gradients: proof 2</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\theta J_\theta(\pi) &amp;&amp;= \mathbb{E}_\pi \big( \color{blue}G(\tau) \color{black} \sum_{t=0}^T \nabla_\theta \log \pi(A_t|S_t) \big) \\
&amp;&amp;\fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  \color{blue}G(\tau)\color{black} \nabla_\theta \log \pi(A_t|S_t) \big)}\\
&amp;&amp;\fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  \color{blue}\Big( \sum_{\color{red}k=0?}^T \gamma^k R_{k+1} \Big)\color{black} \nabla_\theta \log \pi(A_t|S_t) \big) }\\
&amp;&amp;\fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  \Big( \sum_{k=\color{red}t}^T \gamma^k R_{k+1} \Big) \nabla_\theta \log \pi(A_t|S_t) \big)}\\
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-49" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="episodic-policy-gradients-proof-3" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Episodic policy gradients: proof 3</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\begin{eqnarray*}
\nabla_\theta J_\theta(\pi) &amp;&amp;= \mathbb{E}_\pi \big( \sum_{t=0}^T  \Big( \sum_{k=t}^T \color{blue}\gamma^k \color{black} R_{k+1} \Big) \nabla_\theta \log \pi(A_t|S_t) \big) \\
&amp;&amp;\fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  \Big(\color{blue}\gamma^t \color{green} \sum_{k=t}^T \color{blue}\gamma^{k-t} \color{green} R_{k+1} \color{black} \Big) \nabla_\theta \log \pi(A_t|S_t) \big) }\\
&amp;&amp; \fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  (\gamma^t \color{green}G_t\color{black} ) \nabla_\theta \log \pi(A_t|S_t) \big) }\\
&amp;&amp; \fragment{= \mathbb{E}_\pi \big( \sum_{t=0}^T  \gamma^t \color{green} q_\pi(S_t, A_t) \color{black} \nabla_\theta \log \pi(A_t|S_t) \big) }\\
\end{eqnarray*}
\]</span></p>
</div>
<div id="section-50" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="episodic-policy-gradients" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Episodic policy gradients</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[
\nabla_\theta J_\theta(\pi) = \mathbb{E}_\pi \big( \sum_{t=0}^T  \gamma^t q_\pi(S_t, A_t) \nabla_\theta \log \pi(A_t|S_t) \big) \\
\]</span></p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">We can sample this, given a whole episode.</li>
<li class="fragment">Typically, people pull out the sum, and split up this into separate gradients, e.g., <span class="math inline">\(\Delta \theta_t = \gamma^t G_t \nabla_\theta \log \pi (A_t|S_t)\)</span> such that <span class="math inline">\(\mathbb{E}_\pi (\sum_t \Delta \theta_t) = \nabla_\theta J_\theta(\pi)\)</span></li>
<li class="fragment">Typically, people ignore the <span class="math inline">\(\gamma^t\)</span> term, use <span class="math inline">\(\Delta \theta_t = G_t \nabla_\theta \log \pi(A_t | S_t)\)</span></li>
<li class="fragment">This is a (doable) approximation as we pretend on each step that we could have started an episode in that state instead (alternative view: as a slightly biased gradient)</li>
</ul>
</div>
</div>
<div id="section-51" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem-average-reward" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradient theorem (average reward)</h1>
<div class="layout">
<div class="area">
<div id="pg-theorem-average-reward" class="theorem box block">
<h2 class="theorem">PG Theorem (Average Reward)</h2>
<p>For any differentiable policy <span class="math inline">\(\pi_\theta(s, a)\)</span>, the policy gradient of <span class="math inline">\(J(\theta) = \mathbb{E} (R | \pi)\)</span> is</p>
<p><span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi} \Big( q_{\pi_\theta} (S_t, A_t) \nabla_\theta \log \pi_\theta (A_t | S_t) \Big)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{eqnarray*}
q_\pi (s,a) &amp;=&amp; \mathbb{E}_\pi ( R_{t+1} - \rho + q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a )\\
\rho &amp;=&amp; \mathbb{E}_\pi ( R_{t+1} ), \text{Note: global average, not conditioned on state or action}
\end{eqnarray*}
\]</span></p>
<p>(Expectation is over both states and actions)</p>
</div>
<div id="section-52" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem-average-reward-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradient theorem (average reward) 2</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Alternatively (but equivalently):</p>
<p>For any differentiable policy <span class="math inline">\(\pi_\theta(s, a)\)</span>, the policy gradient of <span class="math inline">\(J(\theta) = \mathbb{E} (R | \pi)\)</span> is</p>
<p><span class="math display">\[
\nabla_\theta J(\theta) = \mathbb{E}_\pi \Big( R_{t+1} \sum_{n=0}^\infty  \nabla_\theta \log \pi_\theta (A_{t-n} | S_{t-n}) \Big)
\]</span></p>
<p>(Expectation is over both states and actions)</p>
</div>
<div id="section-53" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-training" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient Training</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>For comparison: Training of a NN using supervised learning:</p>
<div class="media">
<figure class="image" style="height:auto;width:800px;">
<img src="../data/11/karpathy_sl.png" style="height:auto;width:100%;" alt="../data/11/karpathy_sl.png" />
</figure>
</div>
</div>
<div id="section-54" class="box block fragment">
<h2></h2>
<p>Training a policy network in reinforcement learning:</p>
<div class="media">
<figure class="image" style="height:auto;width:800px;">
<img src="../data/11/karpathy_rl.png" style="height:auto;width:100%;" alt="../data/11/karpathy_rl.png" />
</figure>
</div>
</div>
<div id="section-55" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kaparthyblogPG" role="doc-biblioref">Karpathy 2016</a>)</span>, for more information and overview of PG algorithms see <span class="citation">(<a href="#ref-weng2018PG" role="doc-biblioref">Weng 2018</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-theorem-general-outline" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Gradient Theorem – General Outline</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Problem: Estimate <span class="math inline">\(f\)</span> and optimize using gradient ascent. How to estimate the gradient ?</p>
<p><span class="math display">\[\begin{align*}
\nabla_{\theta} E_x[f(x)] &amp;= \nabla_{\theta} \sum_x p(x) f(x) &amp; \text{definition of expectation} \\
&amp; = \sum_x \nabla_{\theta} p(x) f(x) &amp; \text{swap sum and gradient} \\
&amp; = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) &amp; \text{both multiply and divide by } p(x) \\
&amp; = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) &amp; \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
&amp; = E_x[f(x) \nabla_{\theta} \log p(x) ] &amp; \text{definition of expectation}
\end{align*}\]</span></p>
<p><span class="math inline">\(p(x) = p(a \mid \text{Image})\)</span> will be our policy - note: gradient comes from policy.</p>
</div>
<div id="section-56" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-kaparthyblogPG" role="doc-biblioref">Karpathy 2016</a>)</span>, for more information and overview of PG algorithms see <span class="citation">(<a href="#ref-weng2018PG" role="doc-biblioref">Weng 2018</a>)</span>.</p>
<!--# Backpropagation of Gradient Information

Problem: stochastic sampling (select an action) is non-differentiable

![](../data/11/karpathy_nondiff1.png){height=120px}

## {.fragment}

Solution for 'red' parameters: update independently using policy gradients

= encouraging samples that led to low loss.

![](../data/11/karpathy_nondiff2.png){height=200px}

## {.footer}

[@kaparthyblogPG]-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="actor-critics" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Actor Critics</h1>
</div>
</div>
</section>
<section id="recall-policy-gradients-reduce-variance" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recall – Policy gradients: reduce variance</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Note that, in general</p>
<p><span class="math display">\[
\begin{eqnarray*}
\mathbb{E} ( b \nabla_\theta \log \pi (A_t|S_t) ) &amp;=&amp; \mathbb{E} \Big( \sum_a \color{blue}\pi (a|S_t) \color{black}b \color{green}\nabla_\theta \log \pi (a|S_t)\Big)\color{black}, \color{green}(\log f(x))&#39; \color{black} = \frac{\color{red}f&#39;(x)}{\color{blue}f(x)}\\
&amp;=&amp; \mathbb{E} \Big( b \color{red}\nabla_\theta \sum_a \pi_\theta(a|S_t) \color{black} \Big)\\
&amp;=&amp; \mathbb{E} (b \nabla_\theta \color{red}1 \color{black}) = 0
\end{eqnarray*}
\]</span></p>
<ul>
<li>This is true if <span class="math inline">\(b\)</span> does not depend on the action (but it can depend on the state).</li>
<li>Implies, we can subtract a baseline to reduce variance <span class="math display">\[
\theta_{t+1} =\theta_t + \alpha (R_{t+1} −b(S_t)) \nabla_\theta \log \pi_{\theta_t} (A_t|S_t).
\]</span></li>
</ul>
</div>
<div id="section-57" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradients-reduce-variance-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradients: reduce variance</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Recall <span class="math inline">\(\mathbb{E}_\pi ( b(S_t) \nabla \log \pi (A_t|S_t) ) = 0\)</span> for any <span class="math inline">\(b(S_t)\)</span> that does not depend on <span class="math inline">\(A_t\)</span></li>
<li>A common baseline is the value function <span class="math inline">\(\color{red}v_\pi (S_t)\)</span> <span class="math display">\[
\nabla_\theta J_\theta(\pi) = \mathbb{E}_\pi \Big( \sum_{t=0}^T \gamma^t (q_\pi(S_t, A_t) - \color{red}v_\pi(S_t) \color{black}) \nabla_\theta \log \pi(A_t|S_t) \Big)
\]</span></li>
<li>Typically, we estimate <span class="math inline">\(v_w (s) ≈ v_\pi (s)\)</span> explicitly, and sample <span class="math inline">\(q_\pi(S_t, A_t) ≈ G_t\)</span></li>
<li>We can minimise variance further by bootstrapping, e.g., <span class="math inline">\(G_t = R_{t+1} + \gamma v_w(S_{t+1})\)</span></li>
</ul>
</div>
<div id="section-58" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="actor-critic-method" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Actor-Critic Method</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-59" class="left box block">
<h2 class="left"></h2>
<p>Combination of both methods is used in <strong>Actor-Critic</strong> approaches, learning both:</p>
<ul>
<li>an actor policy allowing to use Policy Gradients and</li>
<li>a value-based function that allows to do the updates during each timestep using bootstrapping.</li>
</ul>
<p>A critic is a value function, learnt via policy evaluation: What is the value <span class="math inline">\(v_{\pi_\theta}\)</span> of policy <span class="math inline">\(\pi_\theta\)</span> for current parameters <span class="math inline">\(\theta\)</span>?</p>
</div>
</div><div class="area right">
<div id="section-60" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:480px;">
<img src="../data/11/aralkumaran_actorCritic.svg" style="height:auto;width:100%;" alt="../data/11/aralkumaran_actorCritic.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-61" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-arulkumaran2017brief" role="doc-biblioref">Arulkumaran u. a. 2017</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="actor-critic-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Actor-Critic</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math inline">\(\color{red}\text{Critic}\)</span> – Update parameters <span class="math inline">\(\vec{w}\)</span> of <span class="math inline">\(v_{\vec{w}}\)</span> by TD (e.g., one-step) or MC</p>
<p><span class="math inline">\(\color{blue}\text{Actor}\)</span> – Update <span class="math inline">\(\theta\)</span> by policy gradient</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-8d115cd2.tex.svg" style="height:540px;width:auto;" alt="code-8d115cd2.tex.svg" />
</figure>
</div>
</div>
<div id="section-62" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-gradient-variations" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy gradient variations</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Many extensions and variants exist</li>
<li>Take care: bad policies lead to bad data</li>
<li>This is different from supervised learning (where learning and data are independent)</li>
</ul>
</div>
<div id="section-63" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="increasing-robustness-with-trust-regions" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Increasing robustness with trust regions</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>One way to increase stability is to <strong>regularise</strong>.</p>
<ul>
<li>A popular method is to limit the difference between subsequent policies, e.g., use the Kullbeck-Leibler divergence: <span class="math display">\[
KL(\pi_{\text{old}} \Vert \pi_\theta) = \mathbb{E} \Big( \int \pi_{\text{old}}(a|S) \log \frac{\pi_\theta(a|S)}{\pi_{\text{old}}(a|S)} \text{d}a\Big)
\]</span> (Expectation is over states)</li>
<li>A divergence is like a distance between distributions.</li>
<li>Then maximise <span class="math inline">\(J(\theta) - \eta KL(\pi_{\text{old}} \Vert \pi_\theta)\)</span>, for some hyperparameter <span class="math inline">\(\eta\)</span>.</li>
</ul>
</div>
<div id="section-64" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span> and see as examples, TRPO <span class="citation">(<a href="#ref-schulman2015trpo" role="doc-biblioref">Schulman u. a. 2015</a>)</span> or PPO <span class="citation">(<a href="#ref-schulman2017ppo" role="doc-biblioref">Schulman u. a. 2017</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="continuous-action-spaces" class="section slide level1" data-background-color="#2CA02C">
<div class="decker">
<div class="alignment">
<h1>Continuous Action Spaces</h1>
</div>
</div>
</section>
<section id="continuous-actions" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Continuous actions</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Pure value-based RL can be non-trivial to extend to continuous action spaces
<ul>
<li>How to approximate <span class="math inline">\(q(s, a)\)</span>?</li>
<li>How to compute <span class="math inline">\(\max_a q(s, a)\)</span>?</li>
</ul></li>
<li>When directly updating the policy parameters, continuous actions are easier</li>
<li>Most algorithms discussed today can be used for discrete and continuous actions</li>
</ul>
<p>Note: exploration in high-dimensional continuous spaces can be challenging.</p>
</div>
<div id="section-65" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-gaussian-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Gaussian policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>As example, consider a Gaussian policy,</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">for example: mean is some function of state <span class="math inline">\(\mu_\theta(s)\)</span></li>
<li class="fragment">For simplicity, lets consider fixed variance of <span class="math inline">\(\sigma^2\)</span> (can be parametrized as well)</li>
<li class="fragment">Policy is Gaussian, <span class="math inline">\(A_t \sim \mathcal{N}(\mu_\theta(S_t),\sigma^2)\)</span> (here <span class="math inline">\(\mu_\theta\)</span> is the mean — not to be confused with the behaviour policy!)</li>
<li class="fragment">The gradient of the log of the policy is then <span class="math display">\[
\nabla_\theta \log \pi_\theta(s,a) = \frac{A_t - \mu_\theta(S_t)}{\sigma^2} \nabla \mu_\theta(s)
\]</span></li>
<li class="fragment">This can be used, for instance, in REINFORCE / actor critic.</li>
</ul>
</div>
</div>
<div id="section-66" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-policy-gradient-with-gaussian-policy-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Policy gradient with Gaussian policy 2</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Gaussian policy gradient update: <span class="math display">\[
\begin{eqnarray*}
\theta_{t+1} &amp;=&amp; \theta_t + \beta (G_t -v(S_t)) \nabla_\theta \log \pi_\theta (A_t | S_t) \\
&amp;=&amp; \theta_t + \beta (G_t -v(S_t)) \frac{A_t - \mu_\theta(S_t)}{\sigma^2} \nabla \mu_\theta(S_t)
\end{eqnarray*}
\]</span></p>
<p>Intuition: if return was high, move <span class="math inline">\(\mu_\theta (S_t)\)</span> toward <span class="math inline">\(A_t\)</span></p>
</div>
<div id="section-67" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="gradient-ascent-on-value" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Gradient ascent on value</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Policy gradients work well, but do not strongly exploit the critic</p>
<p>If values generalise well, perhaps we can rely on them more?</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(q_{\vec{w}} ≈ q_\pi\)</span> , e.g., with Sarsa</li>
<li>Define deterministic actor: <span class="math inline">\(A_t = \pi_\theta(S_t)\)</span></li>
<li>Improve actor (policy improvement) by gradient ascent on the value: <span class="math display">\[
\Delta \theta \propto \frac{\partial Q_\pi (s,a)}{\partial \theta} = \frac{\partial Q_\pi (s,\pi_\theta(S_t))}{\partial \pi_\theta(S_t)} \frac{\partial \pi_\theta(S_t)}{\partial \theta}
\]</span></li>
</ol>
<p>Known nowadays as “Deterministic policy gradient” (DPG) <span class="citation">(<a href="#ref-silver2014dpg" role="doc-biblioref">Silver u. a. 2014</a>)</span> and it’s a form of policy iteration.</p>
</div>
<div id="section-68" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
<div id="section-69" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
<!--# Recap - Attention Mechanisms in NN

:::col40

A goal is to learn this as well: Attend to which part of the context?

For example, a RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.

In order to learn to attend, attention has to be differentiable.

:::

:::col60

![](../data/11/rnn_attentional_01.svg){width=800px}

:::

## {.footer}

[@colahsBlog_RNN]-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="comparison-advantages-of-methods" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Comparison: Advantages of Methods</h1>
<div class="layout row columns">
<div class="area left">
<div id="value-based-methods" class="left box block">
<h2 class="left">Value-based Methods</h2>
<ul>
<li>Simple – can be realized as tables, still convergence guarantees.</li>
<li>Efficiency and Speed – bootstraping speeds up learning</li>
</ul>
</div>
</div><div class="area right">
<div id="policy-gradient-methods" class="right box block fragment">
<h2 class="right">Policy Gradient Methods</h2>
<ul>
<li>Applicable in large and continuous action spaces</li>
<li>Employ stochastic policies</li>
</ul>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="further-considerations" class="bottom box block fragment">
<h2 class="bottom">Further considerations:</h2>
<ul>
<li>Do you want to access directly a value, e.g. for other methods?</li>
<li>The state representation of the problem might lends itself more easily to either a value function or a policy function.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-arulkumaran2017brief" class="csl-entry">
Arulkumaran, Kai, Marc P. Deisenroth, Miles Brundage, und Anil A. Bharath. 2017. <span>„Deep Reinforcement Learning: A Brief Survey“</span>. <em>IEEE Signal Processing Magazine</em> 34 (6).
</div>
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-kaparthyblogPG" class="csl-entry">
Karpathy, Andrej. 2016. <span>„Deep Reinforcement Learning: Pong from Pixels“</span>. <a href="http://karpathy.github.io/2016/05/31/rl/">http://karpathy.github.io/2016/05/31/rl/</a>.
</div>
<div id="ref-kohl2004aibo" class="csl-entry">
Kohl, Nate, und Peter Stone. 2004. <span>„Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion.“</span> In <em>ICRA</em>, 2619–24. IEEE.
</div>
<div id="ref-mnih-dqn-2015" class="csl-entry">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, u. a. 2015. <span>„Human-level control through deep reinforcement learning“</span>. <em>Nature</em> 518 (7540): 529–33. <a href="http://dx.doi.org/10.1038/nature14236">http://dx.doi.org/10.1038/nature14236</a>.
</div>
<div id="ref-schulman2015trpo" class="csl-entry">
Schulman, John, Sergey Levine, Philipp Moritz, Michael I. Jordan, und Pieter Abbeel. 2015. <span>„Trust Region Policy Optimization“</span>. <em>CoRR</em> abs/1502.05477. <a href="http://arxiv.org/abs/1502.05477">http://arxiv.org/abs/1502.05477</a>.
</div>
<div id="ref-schulman2017ppo" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, und Oleg Klimov. 2017. <span>„Proximal policy optimization algorithms“</span>. <em>arXiv preprint arXiv:1707.06347</em>.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-silver2014dpg" class="csl-entry">
Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, und Martin Riedmiller. 2014. <span>„Deterministic Policy Gradient Algorithms“</span>. In <em>Proceedings of the 31st International Conference on Machine Learning</em>, herausgegeben von Eric P. Xing und Tony Jebara, 32:387–95. Proceedings of Machine Learning Research 1. Bejing, China: PMLR. <a href="https://proceedings.mlr.press/v32/silver14.html">https://proceedings.mlr.press/v32/silver14.html</a>.
</div>
<div id="ref-sutton:nips12" class="csl-entry">
Sutton, R. S., D. Mcallester, S. Singh, und Y. Mansour. 2000. <span>„Policy gradient methods for reinforcement learning with function approximation“</span>. In <em>Advances in Neural Information Processing Systems 12</em>, 12:1057–63. MIT Press.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-weng2018PG" class="csl-entry">
Weng, Lilian. 2018. <span>„Policy Gradient Algorithms“</span>. <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("ee67b1c62.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
