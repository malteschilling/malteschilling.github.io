<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 3 - Markov Decision
Process</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">

</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>3 - Markov Decision Process</h2>
                  </div>



                  <div class="author"> Prof. Dr. Malte Schilling </div>

                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>


         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="recap-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_1.png" style="height:300px;width:auto;" alt="../data/02/rl_cycle_1.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>A <strong>policy</strong> <em>is</em> the agent’s behavior – chooses an action.</li>
<li><strong>Value function</strong>: <span class="math inline">\(Q_t(a) = \mathbb{E}\Big[ R_t \mid A_{t}=a \Big]\)</span></li>
</ul>
</div>
</div><div class="area right">
<div id="environment" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Provides reward</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-multi-armed-bandits" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Multi-Armed Bandits</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1200px;">
<img src="../data/03/bandit_solution_summary.png" style="height:auto;width:100%;" alt="../data/03/bandit_solution_summary.png" />
</figure>
</div>
</div>
<div id="exploitation-exploration-tradeoff" class="box block">
<h2>Exploitation-Exploration Tradeoff</h2>
<p>Decision Making: sticking to a good past experience might make you miss out on even better options, but at least you can be confident to get something good.</p>
</div>
<div id="section-1" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018b</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="note-multi-armed-bandits" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Note – Multi-Armed Bandits</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-2" class="top box block">
<h2 class="top"></h2>
<p>Can be realized in different ways and using different forms of distributions.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="normal-distribution" class="left box block">
<h2 class="left">Normal Distribution</h2>
<p>In the simplest case, we assume the reward as following a normal distribution. Such a bandit can be represented through a mean and standard deviation. Rewards are sampled from this distribution.</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_2_1_bandit.svg" style="height:240px;width:auto;" alt="../data/02/sutton_2_1_bandit.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="bernoulli-distribution" class="right box block">
<h2 class="right">Bernoulli Distribution</h2>
<p>For discrete rewards: a Bernoulli Distribution can be used. The probability describes how often a fixed reward is given (otherwise zero would be returned).</p>
<p>The PD of the expected reward should be described using a beta distribution.</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/bernoulli-distribution-graph-1634631289.png" style="height:240px;width:auto;" alt="../data/02/bernoulli-distribution-graph-1634631289.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Models for Sequences</li>
<li>Markov Decision Process</li>
<li>Value Functions and Bellman Equation</li>
<li>Overview Approaches in RL</li>
<li>Optimal Policy and Optimal Value Function</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="models-of-sequences" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Models of Sequences</h1>
</div>
</div>
</section>
<section id="sequential-decision-making" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><em>Sequential</em> Decision Making</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The interaction between agent and environment is a <strong>sequence</strong> of actions and returned observations plus rewards.</p>
<p><br />
</p>
<p>Goal of the agent: select actions to maximise total future reward</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">But actions may have long term consequences and reward may be delayed.</li>
<li class="fragment">It may be better to sacrifice immediate reward to gain more long-term reward</li>
</ul>
</div>
<p>An agent’s <strong>policy</strong> <span class="math inline">\(\pi(s) = a\)</span> describes which action <span class="math inline">\(a\)</span> an agent selects depending on the current state.</p>
<p>For the stochastic state, a policy is a probability distribution over actions: <span class="math inline">\(\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]\)</span>.</p>
</div>
<div id="section-3" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>A Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_1.svg" style="height:400px;width:auto;" alt="../data/03/silver_student_1.svg" />
</figure>
</div>
<p>Before turning to action, we focus on a simpler class of Markov Chains: the Markov Reward Process. We could sample trajectories from it.</p>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov process is a memoryless random process – a sequence of random states <span class="math inline">\(S_1, S_2, ...\)</span> with the Markov property.</p>
</div>
<div id="markov-property" class="definition box block">
<h2 class="definition">Markov Property</h2>
<p>A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math inline">\(P(S_{t+1} | S_t) = P(S_{t+1} | S_1,...,S_t)\)</span></p>
<p>States captures all relevant information from the history (“The future is independent of the past given the present”).</p>
</div>
<div id="markov-process-markov-chain" class="box block">
<h2>Markov Process (Markov Chain)</h2>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov reward process is a Markov Chain with associated reward values.</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> is a reward function, <span class="math inline">\(\mathcal{R}_s = \mathbb{E}[R_{t+1}|S_t=s]\)</span></li>
<li><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A <strong>Value Function</strong> <span class="math inline">\(V(s)\)</span> measures the estimated goodness of a state, i.e. how rewarding a state is by a prediction of the cumulative future reward.</p>
</div>
<div id="return" class="definition box block">
<h2 class="definition">Return</h2>
<p>cumulative future reward is a total sum of discounted rewards going forward, starting from time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[ G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \]</span></p>
<p>As future rewards are usually more uncertain, they are weighted less through the <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1]\)</span>.</p>
</div>
<div id="section-5" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="discouting-of-reward" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Discouting of reward</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Most Markov reward and decision processes are discounted. Why?</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">Mathematically convenient to discount rewards</li>
<li class="fragment">Avoids infinite returns in cyclic Markov processes</li>
<li class="fragment">Uncertainty about the future may not be fully represented</li>
<li class="fragment">Animal/human behaviour shows preference for immediate reward</li>
<li class="fragment">It can be possible to use undiscounted Markov reward processes for terminating sequences.</li>
</ul>
</div>
</div>
<div id="section-6" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sampling-in-the-student-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Sampling in the student example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_1.svg" style="height:320px;width:auto;" alt="../data/03/silver_student_1.svg" />
</figure>
</div>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_samples.svg" style="height:320px;width:auto;" alt="../data/03/silver_student_samples.svg" />
</figure>
</div>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-the-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of the value function</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-8" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/03/silver_student_2.svg" style="height:auto;width:100%;" alt="../data/03/silver_student_2.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-9" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/03/silver_student_3.svg" style="height:auto;width:100%;" alt="../data/03/silver_student_3.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-10" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-equation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Equation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The value function can be decomposed into two parts:</p>
<ul>
<li>immediate reward <span class="math inline">\(R_{t+1}\)</span></li>
<li>discounted reward from this point forward for the next reached state – which is estimated by the state-value function: <span class="math inline">\( v(S_{t+1})\)</span></li>
</ul>
<p><span class="math display">\[
\begin{eqnarray*}
V(s) &amp;=&amp; \mathbb{E}[G_t \vert S_t = s] \\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]}
\end{eqnarray*}\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-11" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:360px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-1" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action.</li>
<li>Value-Function: Estimate of achievable return from a state.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-1" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li><strong>State</strong>: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-decision-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Decision Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Markov decision processes formally describe an environment for reinforcement learning.</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:500px;">
<img src="../decker/code/code-060b05af.tex.svg" style="height:auto;width:100%;" alt="code-060b05af.tex.svg" />
</figure>
</div>
</div>
<div id="requirements" class="box block">
<h2>Requirements:</h2>
<p>The environment is fully observable (current state completely characterises the process).</p>
<p>Almost all RL problems can be formalised as MDPs</p>
<ul>
<li>Optimal control primarily deals with continuous MDPs</li>
<li>Partially observable problems can be converted into MDPs</li>
<li>Bandits are MDPs with one state</li>
</ul>
</div>
<div id="section-12" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-decision-process-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Decision Process</h1>
<div class="layout">
<div class="area">
<div id="mdp-definition-sutton2018" class="definition box block">
<h2 class="definition">MDP Definition <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></h2>
<p>A Markov Decision Process is a tuple <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, \gamma)\)</span>, consisting of</p>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span></li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span></li>
<li>a joint probability <span class="math inline">\(p(r, s&#39; | s, a)\)</span> describing the dynamics of the environment as
<ul>
<li>transition probabilities for switching states <span class="math display">\[p(s&#39; | s,a)= \sum_r p(r, s&#39; | s, a)\]</span></li>
<li>and expected reward <span class="math display">\[\mathbb{E}(R | s,a)= \sum_r r \sum_{s&#39;} p(r, s&#39; | s, a)\]</span></li>
</ul></li>
<li>the discount factor <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-policy" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Policy</h1>
<div class="layout row columns">
<div class="area left">
<div id="task" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="policy-representation" class="right box block">
<h2 class="right">Policy Representation</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_policy.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_policy.svg" />
</figure>
</div>
<p>Arrows represent policy <span class="math inline">\(\pi(s)\)</span> for all the states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-13" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="task-1" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="state-value" class="right box block">
<h2 class="right">State Value</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_value.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_value.svg" />
</figure>
</div>
<p>Shown are values <span class="math inline">\(v_{\pi}(s)\)</span> for the different states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-14" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="partially-observable-environments" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Partially Observable Environments</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-15" class="top box block">
<h2 class="top"></h2>
<p>We will focus on fully observable environments: The agent state contains all necessary information required for making an informed decision.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="partial-observability" class="left box block">
<h2 class="left">Partial Observability</h2>
<p>The agent only indirectly experiences the environment, e.g. no exact position information, but only relying on a camera.</p>
<p>Importantly, he might not be able to distinguish states.</p>
<p>The agent, therefore, must construct its own internal state representation (which could, e.g., include information on history).</p>
</div>
</div><div class="area right">
<div id="section-16" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/dm_maze_pomdp.png" style="height:400px;width:auto;" alt="../data/02/dm_maze_pomdp.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="mdp-alternative-definition" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>MDP Alternative Definition</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov Decision Process is a tuple <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, \color{red}{r}\color{black}, \gamma)\)</span>, consisting of</p>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span></li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span></li>
<li> <span class="math inline">\(p(s&#39; | s, a)\)</span> is the probability of transitioning to <span class="math inline">\(s&#39;\)</span>, given a state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span></li>
<li><span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span> the expected reward</li>
<li>the discount factor <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
<div id="section-17" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="reminder-markov-property" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Reminder – Markov Property</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math display">\[p(S_{t+1} | S_t) = p(S_{t+1} | S_1,...,S_t)\]</span></p>
<p>States captures all relevant information from the history (“The future is independent of the past given the present”).</p>
<p>In a Markov Decision Process all states are assumed to have the Markov Property.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-understanding-the-markov-property" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Understanding the Markov Property?</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-18" class="top box block">
<h2 class="top"></h2>
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:300px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>In a Markov Decision Process all states are assumed to have the Markov Property. <br> Which of the following statements are therefore true for an MDP?</p>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area center">
<div id="section-19" class="center box block">
<h2 class="center"></h2>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1}, A_1, \dots, A_{t-1}, S_t = s)
\]</span></p>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1},  S_t = s, A_t = a)
\]</span></p>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1},  S_t = s)
\]</span></p>
<p><span class="math display">\[
p( R_{t+1} = r, S_{t+1} = s&#39;| S_t = s) = p( R_{t+1} = r, S_{t+1} = s&#39;| S_1, \dots, S_{t-1}, S_t = s)
\]</span></p>
</div>
<div id="section-20" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="selection-of-action" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Selection of Action</h1>
<div class="layout">
<div class="area">
<div id="policies-def." class="definition box block">
<h2 class="definition">Policies (def.)</h2>
<p>A policy <span class="math inline">\(\pi\)</span> is a distribution over actions given states,</p>
<p><span class="math display">\[
\pi (a | s) = p ( A_t = a | S_t = s)
\]</span></p>
</div>
<div id="section-21" class="box block">
<h2></h2>
<ul>
<li>A policy fully defines the behaviour of an agent.</li>
<li>MDP policies depend only on the current state (but not the history), <br> which means: Policies are stationary (time-independent)</li>
</ul>
</div>
<div id="section-22" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="tabular-approaches-in-rl" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Tabular Approaches in RL</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-23" class="left box block">
<h2 class="left"></h2>
<p>A policy can be a deterministic mapping from states to actions.</p>
<p>Important: policies depend only on the state, which defines all the things relevant for action selection, and not on other things like time.</p>
</div>
</div><div class="area center">
<div id="action-mapping" class="center box block">
<h2 class="center">Action Mapping</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-e7a616a8.tex.svg" style="height:auto;width:100%;" alt="code-e7a616a8.tex.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="table-representation" class="right box block">
<h2 class="right">Table Representation</h2>
<p>A deterministic policy can also be represented as a table: Each row describes the selected action.</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:360px;">
<img src="../decker/code/code-59dbb0c8.tex.svg" style="height:auto;width:100%;" alt="code-59dbb0c8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="state-value-function-in-an-mdp" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>State-Value Function in an MDP</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The <strong>state-value function</strong> of a state <span class="math inline">\(s\)</span> is the expected return if we are in this state at time <span class="math inline">\(t, S_t=s\)</span>:</p>
<p><span class="math display">\[V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]\]</span></p>
<ul>
<li>Used to evaluate the goodness/badness of states,</li>
<li>and therefore can be used to select between actions.</li>
</ul>
</div>
<div id="section-24" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-v_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(V_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:720px;">
<img src="../decker/code/code-71fac017.tex.svg" style="height:auto;width:100%;" alt="code-71fac017.tex.svg" />
</figure>
</div>
</div>
<div id="recursive-formulation-for-state-value-function" class="box block">
<h2>Recursive Formulation for State-Value Function</h2>
<p><span class="math display">\[
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) \sum_{r} \sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) \Big(r + \gamma v_{\pi}(s&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[ v_{\pi}(s)= \sum_{a \in \mathcal{A}} \pi (a|s) \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-iteratively-calculating-the-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Iteratively Calculating the Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-25" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:240px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>Given is the MDP on the right with two possible actions <span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span> for state <span class="math inline">\(X\)</span>.</p>
<p>These lead deterministically to following staes <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> for which only a single action is available.</p>
<p>Calculate the value function for <span class="math inline">\(X\)</span> for two different policies: either prefering <span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span>. How does the value function depend on selection of <span class="math inline">\(\gamma\)</span>?</p>
</div>
</div><div class="area right">
<div id="section-26" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-calculation-of-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Calculation of Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="focus-on-immediate-reward" class="left box block">
<h2 class="left">Focus on Immediate Reward</h2>
<p>For <span class="math inline">\(\gamma = 0\)</span>:</p>
<p>The value <span class="math inline">\(V(X)\)</span> under <span class="math inline">\(\pi_1(X)=A_1\)</span> is <span class="math inline">\(+1\)</span>.</p>
<p>While the value under <span class="math inline">\(\pi_2(X)=A_2\)</span> is <span class="math inline">\(0\)</span>.</p>
</div>
</div><div class="area right">
<div id="section-27" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-calculation-of-value-function-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Calculation of Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="discounting-future-returns" class="left box block">
<h2 class="left">Discounting Future Returns</h2>
<p>For <span class="math inline">\(\gamma = 0.9\)</span>:</p>
<p><span class="math display">\[
\begin{eqnarray*}
  v_{\pi_1}(X) &amp;=&amp; \fragment{1 + 0.9 * 0 + (0.9)^2 * 1 + \ldots }\\
  &amp;=&amp; \fragment{\sum_{k=0}^{\infty} 0.9^{2k} = \frac{1}{1-0.9^2} \approx 5.3 }\\ \\
  v_{\pi_2}(X) &amp;=&amp; \fragment{0 + 0.9 * 2 + (0.9)^2 * 0 + \ldots }\\
  &amp;=&amp; \fragment{\sum_{k=0}^{\infty} (0.9)^{2k+1}*2 = \frac{0.9}{1-0.9^2} * 2 \approx 9.5}
\end{eqnarray*}
\]</span></p>
</div>
</div><div class="area right">
<div id="section-28" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid</h1>
<div class="layout row columns">
<div class="area left">
<div id="simple-mdp-moving-around" class="left box block">
<h2 class="left">Simple MDP – Moving around</h2>
<p>States: four states, labeled <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(D\)</span> on a grid.</p>
<p>Action space: moving up, down, left, and right. Actions which would move off the grid, keep the agent in place.</p>
<p>Reward: everywhere <span class="math inline">\(0\)</span> except for landing in state <span class="math inline">\(B\)</span>, where the agent gets a reward of <span class="math inline">\(+5\)</span> (also when staying in <span class="math inline">\(B\)</span>).</p>
<p>Policy: uniform random (probab. <span class="math inline">\(0.25\)</span>).</p>
<p><span class="math inline">\(\gamma = 0.7\)</span> for continuing task.</p>
</div>
</div><div class="area right">
<div id="section-29" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-30" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-rl_notes_2019" role="doc-biblioref">Santucci 2019</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid-2" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid (2)</h1>
<div class="layout row columns">
<div class="area left">
<div id="value-function-for-random-policy" class="left box block">
<h2 class="left">Value function for random policy</h2>
<p><span class="math display">\[
  v_\pi(s) = \color{purple} \sum_a \pi(a|s) \color{orange} \sum_r \sum_{s&#39;} \color{black} P\left(s&#39;, r|s,a\right) \left[ \color{orange} r + \gamma v_\pi(s&#39;) \color{black} \right]
\]</span></p>
<div class="fragment">
<p>Simple MDP: Deterministic state update and reward, therefore, for <span class="math inline">\(A\)</span>: <span class="math display">\[
\begin{eqnarray*}
v_\pi(A) &amp;=&amp; \color{purple} \sum_a \pi(a|s) \color{orange} \left(r + 0.7 v_\pi(s&#39;)\right)\\
&amp;=&amp; \fragment{\color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 v_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} \left(0.7 v_\pi(C)\right) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(A)}
\end{eqnarray*}
\]</span></p>
</div>
</div>
</div><div class="area right">
<div id="section-31" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid-3" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid (3)</h1>
<div class="layout row columns">
<div class="area left">
<div id="value-function-for-random-policy-1" class="left box block">
<h2 class="left">Value function for random policy</h2>
<p><span class="math display">\[
\begin{eqnarray*}
v_\pi(A) &amp;=&amp; \color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 v_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} \left(0.7 v_\pi(C)\right) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(A)\\
  v_\pi(B) &amp;=&amp; \frac{1}{2} \left(5 + 0.7 v_\pi(B)\right) + \frac{1}{4} 0.7              v_\pi(A) + \frac{1}{4} 0.7 v_\pi (D) \\
  v_\pi(C) &amp;=&amp; \frac{1}{4} 0.7 v_\pi(A) + \frac{1}{4} 0.7 v_\pi(D) + \frac{1}{2}              0.7 v_\pi(C) \\
  v_\pi(D) &amp;=&amp; \color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 V_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} 0.7 v_\pi(C) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(D).
\end{eqnarray*}
\]</span></p>
<div class="fragment">
<p>Iterative solution (or solving lin. equ.)</p>
<p><span class="math display">\[
  v_\pi(A) = 4.2, \hspace{15pt} v_\pi(B) = 6.1, \hspace{15pt} v_\pi(C) = 2.2, \hspace{15pt} v_\pi(D) = 4.2.
\]</span></p>
</div>
</div>
</div><div class="area right">
<div id="section-32" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-gridworld-value-function-for-a-random-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>A Gridworld: Value-function for a Random Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/02/sutton_3_2_gridworld.svg" style="height:auto;width:100%;" alt="../data/02/sutton_3_2_gridworld.svg" />
</figure>
</div>
<p>Each location is a state.</p>
<p>Actions: North, West, South, East</p>
<p>Reward: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise</p>
<p>For state <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: all actions lead to <span class="math inline">\(A&#39;\)</span> and a reward of <span class="math inline">\(+10\)</span> (respectively <span class="math inline">\(B&#39;, +5\)</span>).</p>
</div>
<div id="section-33" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The <strong>action-value function</strong> (“Q-value”) of a state-action pair is defined as:</p>
<p><span class="math display">\[Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]\]</span></p>
<p>When following a target policy <span class="math inline">\(\pi\)</span>, we can integrate over the probility distribution of possible actions which again leads to the state-value function:</p>
<p><span class="math display">\[V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)\]</span></p>
</div>
<div id="advantage-function" class="box block">
<h2>Advantage Function</h2>
<p>The difference between action-value and state-value is the <strong>action advantage function</strong></p>
<p><span class="math display">\[A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)\]</span></p>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-762d4f05.tex.svg" style="height:auto;width:100%;" alt="code-762d4f05.tex.svg" />
</figure>
</div>
</div>
<div id="action-value-function-using-bellman-expectation" class="box block">
<h2>Action-Value Function using Bellman Expectation</h2>
<p><span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_pi-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{\pi}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-ea9fe2e0.tex.svg" style="height:auto;width:100%;" alt="code-ea9fe2e0.tex.svg" />
</figure>
</div>
</div>
<div id="action-value-function-using-bellman-expectation-1" class="box block">
<h2>Action-Value Function using Bellman Expectation</h2>
<p><span class="math display">\[
q_{\pi}(s,a) = \sum_{r}\sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a)
\Big(r + \gamma \sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a)
\sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-v_pi-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(V_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-65083fad.tex.svg" style="height:auto;width:100%;" alt="code-65083fad.tex.svg" />
</figure>
</div>
</div>
<div id="recursive-formulation-for-state-value-function-1" class="box block">
<h2>Recursive Formulation for State-Value Function</h2>
<p><span class="math display">\[
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) q_{\pi}(s,a)
\]</span></p>
<p>Important: A value function depends on a given policy which the agent follows.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="approaches-to-reinforcement-learning" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Approaches to Reinforcement Learning</h1>
</div>
</div>
</section>
<section id="model-of-the-environment" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Model of the Environment</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The reaction of the environment to certain actions can be represented by a <strong>model</strong> which the agent may or may not know.</p>
<p>The model defines the reward function and transition probabilities.</p>
<div class="media">
<figure class="image" style="height:auto;width:720px;">
<img src="../data/03/weng_RL_algorithm_categorization.png" style="height:auto;width:100%;" alt="../data/03/weng_RL_algorithm_categorization.png" />
</figure>
</div>
<ul>
<li>Model-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Use planning on learned or given model.</li>
<li>Model-free: No dependency on the model during learning. Learning with imperfect information.</li>
</ul>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-36" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:240px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-2" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action <em>depending on current state</em>.</li>
<li>Value-Function: Estimate of achievable return from a state (following <span class="math inline">\(\pi\)</span>).</li>
<li><strong>Model</strong>: A <em>predictor</em> of the environment.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-2" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li>State: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="model" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Model</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A model allows to predict how the environment will react (as a probability distribution).</p>
<ul>
<li><span class="math inline">\(\mathcal{P}\)</span> predicts the subsequent state: <span class="math display">\[\mathcal{P}^a_{ss&#39;} \approx p(S_{t+1}=s&#39; | S_t = s, A_t = a)\]</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> predicts the next reward: <span class="math display">\[\mathcal{R}^a_{s} = \mathbb{E}(R_{t+1} | S_t = s, A_t = a)\]</span></li>
</ul>
<p>A model does not give us immediately a good policy.</p>
<p>But it allows us to plan – test possible alternative actions.</p>
</div>
<div id="section-37" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="categorization-of-reinforcement-learning-agents" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Categorization of Reinforcement Learning Agents</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-38" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Value Based
<ul>
<li>No Policy (Implicit)</li>
<li>Value Function</li>
</ul></li>
<li>Policy Based
<ul>
<li>Policy</li>
<li>No Value Function</li>
</ul></li>
<li>Actor Critic
<ul>
<li>Policy</li>
<li>Value Function</li>
</ul></li>
</ul>
</div>
</div><div class="area right">
<div id="section-39" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/01/silver_RL_categorization.svg" style="height:auto;width:100%;" alt="../data/01/silver_RL_categorization.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-40" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-rl_notes_2019" class="csl-entry">
Santucci, Andreas. 2019. <span>„Course Notes from RL Specialization, University of Alberta“</span>. https://github.com/asantucci/rl_notes.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-weng2018rl" class="csl-entry">
Weng, Lilian. 2018a. <span>„A (Long) Peek into Reinforcement Learning“</span>. <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a>.
</div>
<div id="ref-weng2018bandit" class="csl-entry">
———. 2018b. <span>„The Multi-Armed Bandit Problem and Its Solutions“</span>. <a href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/">https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("66ff34f2d.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
