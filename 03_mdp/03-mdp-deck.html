<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 3 - Markov Decision
Process</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">
  
</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>3 - Markov Decision Process</h2>
                  </div>

         
         
                  <div class="author"> Prof. Dr. Malte Schilling </div>
         
                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>
         
         
         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="recap-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_1.png" style="height:300px;width:auto;" alt="../data/02/rl_cycle_1.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>A <strong>policy</strong> <em>is</em> the agent’s behavior – chooses an action.</li>
<li><strong>Value function</strong>: <span class="math inline">\(Q_t(a) = \mathbb{E}\Big[ R_t \mid A_{t}=a \Big]\)</span></li>
</ul>
</div>
</div><div class="area right">
<div id="environment" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Provides reward</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-multi-armed-bandits" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Multi-Armed Bandits</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1200px;">
<img src="../data/03/bandit_solution_summary.png" style="height:auto;width:100%;" alt="../data/03/bandit_solution_summary.png" />
</figure>
</div>
</div>
<div id="exploitation-exploration-tradeoff" class="box block">
<h2>Exploitation-Exploration Tradeoff</h2>
<p>Decision Making: sticking to a good past experience might make you miss out on even better options, but at least you can be confident to get something good.</p>
</div>
<div id="section-1" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018bandit" role="doc-biblioref">Weng 2018b</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="note-multi-armed-bandits" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Note – Multi-Armed Bandits</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-2" class="top box block">
<h2 class="top"></h2>
<p>Can be realized in different ways and using different forms of distributions.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="normal-distribution" class="left box block">
<h2 class="left">Normal Distribution</h2>
<p>In the simplest case, we assume the reward as following a normal distribution. Such a bandit can be represented through a mean and standard deviation. Rewards are sampled from this distribution.</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_2_1_bandit.svg" style="height:240px;width:auto;" alt="../data/02/sutton_2_1_bandit.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="bernoulli-distribution" class="right box block">
<h2 class="right">Bernoulli Distribution</h2>
<p>For discrete rewards: a Bernoulli Distribution can be used. The probability describes how often a fixed reward is given (otherwise zero would be returned).</p>
<p>The PD of the expected reward should be described using a beta distribution.</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/bernoulli-distribution-graph-1634631289.png" style="height:240px;width:auto;" alt="../data/02/bernoulli-distribution-graph-1634631289.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>Models for Sequences</li>
<li>Markov Decision Process</li>
<li>Value Functions and Bellman Equation</li>
<li>Overview Approaches in RL</li>
<li>Optimal Policy and Optimal Value Function</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="models-of-sequences" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Models of Sequences</h1>
</div>
</div>
</section>
<section id="sequential-decision-making" class="slide level1">
<div class="decker">
<div class="alignment">
<h1><em>Sequential</em> Decision Making</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The interaction between agent and environment is a <strong>sequence</strong> of actions and returned observations plus rewards.</p>
<p><br />
</p>
<p>Goal of the agent: select actions to maximise total future reward</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">But actions may have long term consequences and reward may be delayed.</li>
<li class="fragment">It may be better to sacrifice immediate reward to gain more long-term reward</li>
</ul>
</div>
<p>An agent’s <strong>policy</strong> <span class="math inline">\(\pi(s) = a\)</span> describes which action <span class="math inline">\(a\)</span> an agent selects depending on the current state.</p>
<p>For the stochastic state, a policy is a probability distribution over actions: <span class="math inline">\(\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]\)</span>.</p>
</div>
<div id="section-3" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>A Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_1.svg" style="height:400px;width:auto;" alt="../data/03/silver_student_1.svg" />
</figure>
</div>
<p>Before turning to action, we focus on a simpler class of Markov Chains: the Markov Reward Process. We could sample trajectories from it.</p>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov process is a memoryless random process – a sequence of random states <span class="math inline">\(S_1, S_2, ...\)</span> with the Markov property.</p>
</div>
<div id="markov-property" class="definition box block">
<h2 class="definition">Markov Property</h2>
<p>A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math inline">\(P(S_{t+1} | S_t) = P(S_{t+1} | S_1,...,S_t)\)</span></p>
<p>States captures all relevant information from the history (“The future is independent of the past given the present”).</p>
</div>
<div id="markov-process-markov-chain" class="box block">
<h2>Markov Process (Markov Chain)</h2>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-reward-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Reward Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov reward process is a Markov Chain with associated reward values.</p>
<ul>
<li><span class="math inline">\(\mathcal{S}\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal{P}\)</span> is a state transition probability matrix: <span class="math inline">\(P_{ss&#39;} =P(S_{t+1}=s&#39;|S_t=s)\)</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> is a reward function, <span class="math inline">\(\mathcal{R}_s = \mathbb{E}[R_{t+1}|S_t=s]\)</span></li>
<li><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A <strong>Value Function</strong> <span class="math inline">\(V(s)\)</span> measures the estimated goodness of a state, i.e. how rewarding a state is by a prediction of the cumulative future reward.</p>
</div>
<div id="return" class="definition box block">
<h2 class="definition">Return</h2>
<p>cumulative future reward is a total sum of discounted rewards going forward, starting from time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[ G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \]</span></p>
<p>As future rewards are usually more uncertain, they are weighted less through the <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1]\)</span>.</p>
</div>
<div id="section-5" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="discouting-of-reward" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Discouting of reward</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Most Markov reward and decision processes are discounted. Why?</p>
<div class="incremental">
<ul class="incremental">
<li class="fragment">Mathematically convenient to discount rewards</li>
<li class="fragment">Avoids infinite returns in cyclic Markov processes</li>
<li class="fragment">Uncertainty about the future may not be fully represented</li>
<li class="fragment">Animal/human behaviour shows preference for immediate reward</li>
<li class="fragment">It can be possible to use undiscounted Markov reward processes for terminating sequences.</li>
</ul>
</div>
</div>
<div id="section-6" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sampling-in-the-student-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Sampling in the student example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_1.svg" style="height:320px;width:auto;" alt="../data/03/silver_student_1.svg" />
</figure>
</div>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_student_samples.svg" style="height:320px;width:auto;" alt="../data/03/silver_student_samples.svg" />
</figure>
</div>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-the-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of the value function</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-8" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/03/silver_student_2.svg" style="height:auto;width:100%;" alt="../data/03/silver_student_2.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="section-9" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/03/silver_student_3.svg" style="height:auto;width:100%;" alt="../data/03/silver_student_3.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-10" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-equation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Equation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The value function can be decomposed into two parts:</p>
<ul>
<li>immediate reward <span class="math inline">\(R_{t+1}\)</span></li>
<li>discounted reward from this point forward for the next reached state – which is estimated by the state-value function: <span class="math inline">\( v(S_{t+1})\)</span></li>
</ul>
<p><span class="math display">\[
\begin{eqnarray*} 
V(s) &amp;=&amp; \mathbb{E}[G_t \vert S_t = s] \\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] }\\
&amp;=&amp; \fragment{\mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]}
\end{eqnarray*}\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-11" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:360px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-1" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action.</li>
<li>Value-Function: Estimate of achievable return from a state.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-1" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li><strong>State</strong>: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-decision-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Decision Process</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Markov decision processes formally describe an environment for reinforcement learning.</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:500px;">
<img src="../.decker/code/code-060b05af.tex.svg" style="height:auto;width:100%;" alt="code-060b05af.tex.svg" />
</figure>
</div>
</div>
<div id="requirements" class="box block">
<h2>Requirements:</h2>
<p>The environment is fully observable (current state completely characterises the process).</p>
<p>Almost all RL problems can be formalised as MDPs</p>
<ul>
<li>Optimal control primarily deals with continuous MDPs</li>
<li>Partially observable problems can be converted into MDPs</li>
<li>Bandits are MDPs with one state</li>
</ul>
</div>
<div id="section-12" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="markov-decision-process-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Markov Decision Process</h1>
<div class="layout">
<div class="area">
<div id="mdp-definition-sutton2018" class="definition box block">
<h2 class="definition">MDP Definition <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></h2>
<p>A Markov Decision Process is a tuple <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, \gamma)\)</span>, consisting of</p>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span></li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span></li>
<li>a joint probability <span class="math inline">\(p(r, s&#39; | s, a)\)</span> describing the dynamics of the environment as
<ul>
<li>transition probabilities for switching states <span class="math display">\[p(s&#39; | s,a)= \sum_r p(r, s&#39; | s, a)\]</span></li>
<li>and expected reward <span class="math display">\[\mathbb{E}(R | s,a)= \sum_r r \sum_{s&#39;} p(r, s&#39; | s, a)\]</span></li>
</ul></li>
<li>the discount factor <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-policy" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Policy</h1>
<div class="layout row columns">
<div class="area left">
<div id="task" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="policy-representation" class="right box block">
<h2 class="right">Policy Representation</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_policy.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_policy.svg" />
</figure>
</div>
<p>Arrows represent policy <span class="math inline">\(\pi(s)\)</span> for all the states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-13" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maze-example-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Maze Example: Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="task-1" class="left box block">
<h2 class="left">Task</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze.svg" />
</figure>
</div>
<ul>
<li>Reward of <span class="math inline">\(-1\)</span> per time step in maze</li>
<li>Actions are move N, S, W, E</li>
<li>State is location</li>
</ul>
</div>
</div><div class="area right">
<div id="state-value" class="right box block">
<h2 class="right">State Value</h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/sutton_maze_value.svg" style="height:360px;width:auto;" alt="../data/02/sutton_maze_value.svg" />
</figure>
</div>
<p>Shown are values <span class="math inline">\(v_{\pi}(s)\)</span> for the different states.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-14" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="partially-observable-environments" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Partially Observable Environments</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-15" class="top box block">
<h2 class="top"></h2>
<p>We will focus on fully observable environments: The agent state contains all necessary information required for making an informed decision.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="partial-observability" class="left box block">
<h2 class="left">Partial Observability</h2>
<p>The agent only indirectly experiences the environment, e.g. no exact position information, but only relying on a camera.</p>
<p>Importantly, he might not be able to distinguish states.</p>
<p>The agent, therefore, must construct its own internal state representation (which could, e.g., include information on history).</p>
</div>
</div><div class="area right">
<div id="section-16" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/dm_maze_pomdp.png" style="height:400px;width:auto;" alt="../data/02/dm_maze_pomdp.png" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="mdp-alternative-definition" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>MDP Alternative Definition</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A Markov Decision Process is a tuple <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, \color{red}{r}\color{black}, \gamma)\)</span>, consisting of</p>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span></li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span></li>
<li> <span class="math inline">\(p(s&#39; | s, a)\)</span> is the probability of transitioning to <span class="math inline">\(s&#39;\)</span>, given a state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span></li>
<li><span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span> the expected reward</li>
<li>the discount factor <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
<div id="section-17" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="reminder-markov-property" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Reminder – Markov Property</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A state <span class="math inline">\(S_t\)</span> is Markov iff <span class="math display">\[p(S_{t+1} | S_t) = p(S_{t+1} | S_1,...,S_t)\]</span></p>
<p>States captures all relevant information from the history (“The future is independent of the past given the present”).</p>
<p>In a Markov Decision Process all states are assumed to have the Markov Property.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-understanding-the-markov-property" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Understanding the Markov Property?</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-18" class="top box block">
<h2 class="top"></h2>
<div class="grid-layout" style="grid-template-columns: 30fr 70fr;">
<div class="media">
<figure class="image" style="height:auto;width:300px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>In a Markov Decision Process all states are assumed to have the Markov Property. <br> Which of the following statements are therefore true for an MDP?</p>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area center">
<div id="section-19" class="center box block">
<h2 class="center"></h2>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1}, A_1, \dots, A_{t-1}, S_t = s)
\]</span></p>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1},  S_t = s, A_t = a)
\]</span></p>
<p><span class="math display">\[
p( S_{t+1} = s&#39;| S_t = s, A_t = a) = p( S_{t+1} = s&#39;| S_1, \dots, S_{t-1},  S_t = s)
\]</span></p>
<p><span class="math display">\[
p( R_{t+1} = r, S_{t+1} = s&#39;| S_t = s) = p( R_{t+1} = r, S_{t+1} = s&#39;| S_1, \dots, S_{t-1}, S_t = s)
\]</span></p>
</div>
<div id="section-20" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="selection-of-action" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Selection of Action</h1>
<div class="layout">
<div class="area">
<div id="policies-def." class="definition box block">
<h2 class="definition">Policies (def.)</h2>
<p>A policy <span class="math inline">\(\pi\)</span> is a distribution over actions given states,</p>
<p><span class="math display">\[
\pi (a | s) = p ( A_t = a | S_t = s)
\]</span></p>
</div>
<div id="section-21" class="box block">
<h2></h2>
<ul>
<li>A policy fully defines the behaviour of an agent.</li>
<li>MDP policies depend only on the current state (but not the history), <br> which means: Policies are stationary (time-independent)</li>
</ul>
</div>
<div id="section-22" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="tabular-approaches-in-rl" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Tabular Approaches in RL</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-23" class="left box block">
<h2 class="left"></h2>
<p>A policy can be a deterministic mapping from states to actions.</p>
<p>Important: policies depend only on the state, which defines all the things relevant for action selection, and not on other things like time.</p>
</div>
</div><div class="area center">
<div id="action-mapping" class="center box block">
<h2 class="center">Action Mapping</h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-e7a616a8.tex.svg" style="height:auto;width:100%;" alt="code-e7a616a8.tex.svg" />
</figure>
</div>
</div>
</div><div class="area right">
<div id="table-representation" class="right box block">
<h2 class="right">Table Representation</h2>
<p>A deterministic policy can also be represented as a table: Each row describes the selected action.</p>
<div class="media">
<figure class="render image rendered" style="height:auto;width:360px;">
<img src="../.decker/code/code-59dbb0c8.tex.svg" style="height:auto;width:100%;" alt="code-59dbb0c8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="state-value-function-in-an-mdp" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>State-Value Function in an MDP</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The <strong>state-value function</strong> of a state <span class="math inline">\(s\)</span> is the expected return if we are in this state at time <span class="math inline">\(t, S_t=s\)</span>:</p>
<p><span class="math display">\[V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]\]</span></p>
<ul>
<li>Used to evaluate the goodness/badness of states,</li>
<li>and therefore can be used to select between actions.</li>
</ul>
</div>
<div id="section-24" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-v_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(V_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:720px;">
<img src="../.decker/code/code-71fac017.tex.svg" style="height:auto;width:100%;" alt="code-71fac017.tex.svg" />
</figure>
</div>
</div>
<div id="recursive-formulation-for-state-value-function" class="box block">
<h2>Recursive Formulation for State-Value Function</h2>
<p><span class="math display">\[
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) \sum_{r} \sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) \Big(r + \gamma v_{\pi}(s&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[ v_{\pi}(s)= \sum_{a \in \mathcal{A}} \pi (a|s) \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-iteratively-calculating-the-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Iteratively Calculating the Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-25" class="left box block">
<h2 class="left"></h2>
<div class="media">
<figure class="image" style="height:auto;width:240px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>Given is the MDP on the right with two possible actions <span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span> for state <span class="math inline">\(X\)</span>.</p>
<p>These lead deterministically to following staes <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> for which only a single action is available.</p>
<p>Calculate the value function for <span class="math inline">\(X\)</span> for two different policies: either prefering <span class="math inline">\(A_1\)</span> or <span class="math inline">\(A_2\)</span>. How does the value function depend on selection of <span class="math inline">\(\gamma\)</span>?</p>
</div>
</div><div class="area right">
<div id="section-26" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-calculation-of-value-function" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Calculation of Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="focus-on-immediate-reward" class="left box block">
<h2 class="left">Focus on Immediate Reward</h2>
<p>For <span class="math inline">\(\gamma = 0\)</span>:</p>
<p>The value <span class="math inline">\(V(X)\)</span> under <span class="math inline">\(\pi_1(X)=A_1\)</span> is <span class="math inline">\(+1\)</span>.</p>
<p>While the value under <span class="math inline">\(\pi_2(X)=A_2\)</span> is <span class="math inline">\(0\)</span>.</p>
</div>
</div><div class="area right">
<div id="section-27" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="task-calculation-of-value-function-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Task – Calculation of Value Function</h1>
<div class="layout row columns">
<div class="area left">
<div id="discounting-future-returns" class="left box block">
<h2 class="left">Discounting Future Returns</h2>
<p>For <span class="math inline">\(\gamma = 0.9\)</span>:</p>
<p><span class="math display">\[
\begin{eqnarray*}
  v_{\pi_1}(X) &amp;=&amp; \fragment{1 + 0.9 * 0 + (0.9)^2 * 1 + \ldots }\\ 
  &amp;=&amp; \fragment{\sum_{k=0}^{\infty} 0.9^{2k} = \frac{1}{1-0.9^2} \approx 5.3 }\\ \\
  v_{\pi_2}(X) &amp;=&amp; \fragment{0 + 0.9 * 2 + (0.9)^2 * 0 + \ldots }\\
  &amp;=&amp; \fragment{\sum_{k=0}^{\infty} (0.9)^{2k+1}*2 = \frac{0.9}{1-0.9^2} * 2 \approx 9.5}
\end{eqnarray*}
\]</span></p>
</div>
</div><div class="area right">
<div id="section-28" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../.decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid</h1>
<div class="layout row columns">
<div class="area left">
<div id="simple-mdp-moving-around" class="left box block">
<h2 class="left">Simple MDP – Moving around</h2>
<p>States: four states, labeled <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(D\)</span> on a grid.</p>
<p>Action space: moving up, down, left, and right. Actions which would move off the grid, keep the agent in place.</p>
<p>Reward: everywhere <span class="math inline">\(0\)</span> except for landing in state <span class="math inline">\(B\)</span>, where the agent gets a reward of <span class="math inline">\(+5\)</span> (also when staying in <span class="math inline">\(B\)</span>).</p>
<p>Policy: uniform random (probab. <span class="math inline">\(0.25\)</span>).</p>
<p><span class="math inline">\(\gamma = 0.7\)</span> for continuing task.</p>
</div>
</div><div class="area right">
<div id="section-29" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-30" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-rl_notes_2019" role="doc-biblioref">Santucci 2019</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid-2" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid (2)</h1>
<div class="layout row columns">
<div class="area left">
<div id="value-function-for-random-policy" class="left box block">
<h2 class="left">Value function for random policy</h2>
<p><span class="math display">\[
  v_\pi(s) = \color{purple} \sum_a \pi(a|s) \color{orange} \sum_r \sum_{s&#39;} \color{black} P\left(s&#39;, r|s,a\right) \left[ \color{orange} r + \gamma v_\pi(s&#39;) \color{black} \right]
\]</span></p>
<div class="fragment">
<p>Simple MDP: Deterministic state update and reward, therefore, for <span class="math inline">\(A\)</span>: <span class="math display">\[
\begin{eqnarray*}
v_\pi(A) &amp;=&amp; \color{purple} \sum_a \pi(a|s) \color{orange} \left(r + 0.7 v_\pi(s&#39;)\right)\\
&amp;=&amp; \fragment{\color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 v_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} \left(0.7 v_\pi(C)\right) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(A)}
\end{eqnarray*}
\]</span></p>
</div>
</div>
</div><div class="area right">
<div id="section-31" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="extended-example-a-simple-minimal-grid-3" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Extended Example: A simple, minimal Grid (3)</h1>
<div class="layout row columns">
<div class="area left">
<div id="value-function-for-random-policy-1" class="left box block">
<h2 class="left">Value function for random policy</h2>
<p><span class="math display">\[
\begin{eqnarray*}
v_\pi(A) &amp;=&amp; \color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 v_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} \left(0.7 v_\pi(C)\right) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(A)\\
  v_\pi(B) &amp;=&amp; \frac{1}{2} \left(5 + 0.7 v_\pi(B)\right) + \frac{1}{4} 0.7              v_\pi(A) + \frac{1}{4} 0.7 v_\pi (D) \\
  v_\pi(C) &amp;=&amp; \frac{1}{4} 0.7 v_\pi(A) + \frac{1}{4} 0.7 v_\pi(D) + \frac{1}{2}              0.7 v_\pi(C) \\
  v_\pi(D) &amp;=&amp; \color{purple} \frac{1}{4} \color{orange} \left(5 + 0.7 V_\pi(B)\right) \color{black} + \color{purple} \frac{1}{4} \color{orange} 0.7 v_\pi(C) \color{black} + \color{purple} \frac{1}{2} \color{orange} 0.7 v_\pi(D).
\end{eqnarray*}
\]</span></p>
<div class="fragment">
<p>Iterative solution (or solving lin. equ.)</p>
<p><span class="math display">\[
  v_\pi(A) = 4.2, \hspace{15pt} v_\pi(B) = 6.1, \hspace{15pt} v_\pi(C) = 2.2, \hspace{15pt} v_\pi(D) = 4.2.
\]</span></p>
</div>
</div>
</div><div class="area right">
<div id="section-32" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../.decker/code/code-7ef907b8.tex.svg" style="height:auto;width:100%;" alt="code-7ef907b8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="a-gridworld-value-function-for-a-random-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>A Gridworld: Value-function for a Random Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/02/sutton_3_2_gridworld.svg" style="height:auto;width:100%;" alt="../data/02/sutton_3_2_gridworld.svg" />
</figure>
</div>
<p>Each location is a state.</p>
<p>Actions: North, West, South, East</p>
<p>Reward: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise</p>
<p>For state <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: all actions lead to <span class="math inline">\(A&#39;\)</span> and a reward of <span class="math inline">\(+10\)</span> (respectively <span class="math inline">\(B&#39;, +5\)</span>).</p>
</div>
<div id="section-33" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="q-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Q-Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The <strong>action-value function</strong> (“Q-value”) of a state-action pair is defined as:</p>
<p><span class="math display">\[Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]\]</span></p>
<p>When following a target policy <span class="math inline">\(\pi\)</span>, we can integrate over the probility distribution of possible actions which again leads to the state-value function:</p>
<p><span class="math display">\[V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)\]</span></p>
</div>
<div id="advantage-function" class="box block">
<h2>Advantage Function</h2>
<p>The difference between action-value and state-value is the <strong>action advantage function</strong></p>
<p><span class="math display">\[A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)\]</span></p>
</div>
<div id="section-34" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-762d4f05.tex.svg" style="height:auto;width:100%;" alt="code-762d4f05.tex.svg" />
</figure>
</div>
</div>
<div id="action-value-function-using-bellman-expectation" class="box block">
<h2>Action-Value Function using Bellman Expectation</h2>
<p><span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_pi-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{\pi}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-ea9fe2e0.tex.svg" style="height:auto;width:100%;" alt="code-ea9fe2e0.tex.svg" />
</figure>
</div>
</div>
<div id="action-value-function-using-bellman-expectation-1" class="box block">
<h2>Action-Value Function using Bellman Expectation</h2>
<p><span class="math display">\[
q_{\pi}(s,a) = \sum_{r}\sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) 
\Big(r + \gamma \sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) 
\sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-v_pi-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(V_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-65083fad.tex.svg" style="height:auto;width:100%;" alt="code-65083fad.tex.svg" />
</figure>
</div>
</div>
<div id="recursive-formulation-for-state-value-function-1" class="box block">
<h2>Recursive Formulation for State-Value Function</h2>
<p><span class="math display">\[
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) q_{\pi}(s,a)
\]</span></p>
<p>Important: A value function depends on a given policy which the agent follows.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="approaches-to-reinforcement-learning" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Approaches to Reinforcement Learning</h1>
</div>
</div>
</section>
<section id="model-of-the-environment" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Model of the Environment</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The reaction of the environment to certain actions can be represented by a <strong>model</strong> which the agent may or may not know.</p>
<p>The model defines the reward function and transition probabilities.</p>
<div class="media">
<figure class="image" style="height:auto;width:720px;">
<img src="../data/03/weng_RL_algorithm_categorization.png" style="height:auto;width:100%;" alt="../data/03/weng_RL_algorithm_categorization.png" />
</figure>
</div>
<ul>
<li>Model-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Use planning on learned or given model.</li>
<li>Model-free: No dependency on the model during learning. Learning with imperfect information.</li>
</ul>
</div>
<div id="section-35" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="rl-cycle-sequential-decision-making-1" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>RL Cycle – Sequential Decision Making</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-36" class="top box block">
<h2 class="top"></h2>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/02/rl_cycle_2.png" style="height:240px;width:auto;" alt="../data/02/rl_cycle_2.png" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="agent-2" class="left box block">
<h2 class="left">Agent</h2>
<ul>
<li>Policy: choose an action <em>depending on current state</em>.</li>
<li>Value-Function: Estimate of achievable return from a state (following <span class="math inline">\(\pi\)</span>).</li>
<li><strong>Model</strong>: A <em>predictor</em> of the environment.</li>
</ul>
</div>
</div><div class="area right">
<div id="environment-2" class="right box block">
<h2 class="right">Environment</h2>
<ul>
<li>Reward: Describing goal</li>
<li>State: observation for agent</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="model" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Model</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>A model allows to predict how the environment will react (as a probability distribution).</p>
<ul>
<li><span class="math inline">\(\mathcal{P}\)</span> predicts the subsequent state: <span class="math display">\[\mathcal{P}^a_{ss&#39;} \approx p(S_{t+1}=s&#39; | S_t = s, A_t = a)\]</span></li>
<li><span class="math inline">\(\mathcal{R}\)</span> predicts the next reward: <span class="math display">\[\mathcal{R}^a_{s} = \mathbb{E}(R_{t+1} | S_t = s, A_t = a)\]</span></li>
</ul>
<p>A model does not give us immediately a good policy.</p>
<p>But it allows us to plan – test possible alternative actions.</p>
</div>
<div id="section-37" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="categorization-of-reinforcement-learning-agents" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Categorization of Reinforcement Learning Agents</h1>
<div class="layout row columns">
<div class="area left">
<div id="section-38" class="left box block">
<h2 class="left"></h2>
<ul>
<li>Value Based
<ul>
<li>No Policy (Implicit)</li>
<li>Value Function</li>
</ul></li>
<li>Policy Based
<ul>
<li>Policy</li>
<li>No Value Function</li>
</ul></li>
<li>Actor Critic
<ul>
<li>Policy</li>
<li>Value Function</li>
</ul></li>
</ul>
</div>
</div><div class="area right">
<div id="section-39" class="right box block">
<h2 class="right"></h2>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/01/silver_RL_categorization.svg" style="height:auto;width:100%;" alt="../data/01/silver_RL_categorization.svg" />
</figure>
</div>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="section-40" class="bottom footer box block">
<h2 class="bottom footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-policies" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Optimal Policies</h1>
</div>
</div>
</section>
<section id="optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>There is a partial ordering over policies which means: <span class="math display">\[
\pi \geq \pi&#39; \text{ if } V_{\pi}(s) \geq V_{\pi&#39;}(s), \forall s
\]</span></p>
</div>
<div id="for-any-markov-decision-process" class="definition box block">
<h2 class="definition">For any Markov Decision process …</h2>
<ul>
<li>There is an optimal policy <span class="math inline">\(\pi_*\)</span> which is better (or equal) than all other policies.</li>
<li>Every optimal policy achieves the optimal value function and the optimal action-value function.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The optimal state-value function <span class="math inline">\(V_*(s)\)</span> is the maximum value function over all policies:</p>
<p><span class="math display">\[
V_*(s) = \max_{\pi} V_{\pi}(s)
\]</span></p>
<p>The optimal action-value function <span class="math inline">\(q_*(s,a)\)</span> is the maximum action-value function over all policies <span class="math display">\[
Q_*(s,a) = \max_{\pi} Q_{\pi}(s,a)
\]</span></p>
<ul>
<li>The optimal value function specifies the best possible performance in the MDP.</li>
<li>An MDP is “solved” when we know the optimal value function.</li>
</ul>
</div>
<div id="section-41" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-optimality-equation-for-v_" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Optimality Equation for <span class="math inline">\(V_{*}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-89bc691f.tex.svg" style="height:auto;width:100%;" alt="code-89bc691f.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
v_{*}(s) = \max_{a \in \mathcal{A}} q_{*}(s,a)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-optimality-equation-for-v_-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Optimality Equation for <span class="math inline">\(V_{*}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-bbdd3e74.tex.svg" style="height:auto;width:100%;" alt="code-bbdd3e74.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
v_{*}(s) = \max_{a \in \mathcal{A}} \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{*}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-f3554bbb.tex.svg" style="height:auto;width:100%;" alt="code-f3554bbb.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
q_{*}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{*}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../.decker/code/code-ca812591.tex.svg" style="height:auto;width:100%;" alt="code-ca812591.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
q_{*}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) 
\max_{a&#39; \in \mathcal{A}} q_{*}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalVexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalVexample.svg" />
</figure>
</div>
</div>
<div id="section-42" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-action-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Action-Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalQexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalQexample.svg" />
</figure>
</div>
</div>
<div id="section-43" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-optimality-equations" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Optimality Equations</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can use these equations directly to compute optimal values for following an optimal policy.</p>
<p><span class="math display">\[\begin{align*} 
V_*(s) &amp;= \max_{a \in \mathcal{A}} Q_*(s,a)\\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a V_*(s&#39;) \\
V_*(s) &amp;= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a V_*(s&#39;) \big) \\
Q_*(s, a) &amp;= R(s, a) + \gamma \sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a \max_{a&#39; \in \mathcal{A}} Q_*(s&#39;, a&#39;)
\end{align*}\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalPolicyexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalPolicyexample.svg" />
</figure>
</div>
</div>
<div id="section-44" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-value-function-and-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Value Function and Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The goal in RL is to act optimally – this is possible through learning an optimal value function or directly an optimal policy.</p>
<p>The optimal value function produces the maximum return:</p>
<p><span class="math display">\[ V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)\]</span></p>
<p>The optimal policy achieves optimal value functions:</p>
<p><span class="math display">\[ \pi_{*} = \arg\max_{\pi} V_{\pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)\]</span></p>
<p>These are directly related as <span class="math display">\[ V_{\pi_{*}}(s)=V_{*}(s), Q_{\pi_{*}}(s, a) = Q_{*}(s, a)\]</span></p>
</div>
<div id="section-45" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-solution-for-the-gridworld-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Solution for the Gridworld Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/02/sutton_3_5_gridworld.svg" style="height:auto;width:100%;" alt="../data/02/sutton_3_5_gridworld.svg" />
</figure>
</div>
<p>Each location is a state. Discount factor is <span class="math inline">\(0.9\)</span>.</p>
<p>Actions: North, West, South, East</p>
<p>Reward: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise</p>
<p>For state <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: all actions lead to <span class="math inline">\(A&#39;\)</span> and a reward of <span class="math inline">\(+10\)</span> (respectively <span class="math inline">\(B&#39;, +5\)</span>).</p>
</div>
<div id="section-46" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dynamic-programming" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming</h1>
</div>
</div>
</section>
<section id="dynamic-programming-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Assumes full knowledge of the underlying Markov Decision Process (model is known) – realizes a form of Planning.</p>
<p>Dynamic Programming is a very general solution method for problems which have two properties:</p>
<ul>
<li>Optimal substructure
<ul>
<li>Principle of optimality applies</li>
<li>Optimal solution can be decomposed into subproblems</li>
</ul></li>
<li>Overlapping subproblems
<ul>
<li>Subproblems recur many times</li>
<li>Solutions can be cached and reused</li>
</ul></li>
</ul>
</div>
<div id="section-47" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dp-policy-evaluation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DP: Policy Evaluation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Policy Evaluation is to compute the state-value <span class="math inline">\(V_{\pi}\)</span> for a given policy <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
V_{t+1}(s) 
= \mathbb{E}_\pi [r + \gamma V_t(s&#39;) | S_t = s]
= \sum_a \pi(a \vert s) \sum_{s&#39;, r} P(s&#39;, r \vert s, a) (r + \gamma V_k(s&#39;))
\]</span></p>
<p>It iteratively applies the Bellman expectation backup and converges.</p>
<div class="media">
<figure class="image" style="height:auto;width:600px;">
<img src="../data/03/silver_policyEvaluation.svg" style="height:auto;width:100%;" alt="../data/03/silver_policyEvaluation.svg" />
</figure>
</div>
</div>
<div id="section-48" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dp-policy-improvement" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DP: Policy Improvement</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Based on a value functions, Policy Improvement generates a better policy <span class="math inline">\(\pi&#39; \geq \pi\)</span> by acting greedily.</p>
<p><span class="math display">\[
Q_\pi(s, a) 
= \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]
= \sum_{s&#39;, r} P(s&#39;, r \vert s, a) (r + \gamma V_\pi(s&#39;))
\]</span></p>
</div>
<div id="section-49" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dp-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DP: Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Iterative procedure to improve the policy when combining policy evaluation and improvement:</p>
<p><span class="math display">\[
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*
\]</span></p>
</div>
<div id="section-50" class="box block fragment">
<h2></h2>
<div class="media">
<figure class="image" style="height:auto;width:1000px;">
<img src="../data/03/silver_policyIteration.svg" style="height:auto;width:100%;" alt="../data/03/silver_policyIteration.svg" />
</figure>
</div>
</div>
<div id="section-51" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>; <a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dp-generalized-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DP: Generalized Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The value function is approximated repeatedly to be closer to the true value of the current policy.</p>
<p>And in the meantime, the policy is improved repeatedly to approach optimality.</p>
<p>This policy iteration process works and always converges to the optimality.</p>
</div>
<div id="section-52" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018a</a>)</span></p>
<!--
# Iterative Policy Evaluation

``` {.tex .render width=800px}
\documentclass{standalone}
\usepackage[boxruled,lined]{algorithm2e}
\usepackage{mathtools}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\begin{document}
\pagestyle{empty}
\begin{algorithm}[H]
\caption{Iterative Policy Evaluation, for estimating $v_\pi$} 
  \KwInput{$\pi$, the policy to be evaluated; algorithm parameter: a small     threshold $\theta > 0$ determining accuracy of estimation}
  Initialize $V(s)$ for all $s \in \mathcal S^+$, arbitrarily except that   $V(\textrm{terminal}) = 0$. \\
\While{$\Delta \geq \theta$} {
  $\Delta \gets 0$ \\
  \For{$s \in \mathcal S$} {
    $v \gets V(s)$\\
    $V(s) \gets \sum_a \pi(a|s) \sum_{s',r} \Pr(s', r| s, a) \left[ r + \gamma       V(s')\right]$ \\
    $\Delta \gets \max\left(\Delta, |v - V(s)|\right)$ \hspace{35pt} \tcp{Track largest update to state-value function.}
  }
}
\end{algorithm}
\end{document}
```

# General Policy Iteration {.columns}

## {.right}

``` {.tex .render width=400px}
\documentclass{standalone}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{latexsym}
\usetikzlibrary{positioning}
\begin{document}
\pagestyle{empty}
  \begin{tikzpicture}
    \node (pi) at (-1, 0) {$\pi$};
    \node (v)  at ( 1, 0) {$V$};
    \path[-stealth, line width=0.5 mm] (pi) edge[bend left = 90] (v);
    \path[-stealth, line width=0.5 mm] (v)  edge[bend left = 90] (pi);
    \node at (0, 1.1) {evaluation};
    \node at (0, 0.55) {$\scriptstyle V \leadsto v_\pi$};
    \node at (0, -0.4) {$\scriptstyle \pi \leadsto \textrm{\tiny greedy}(V)$};
    \node at (0, -1.1) {improvement};
    \node at (0, -1.5) {$\vdots$};
    \node (pistar) at (-1, -2.2) {$\pi_*$};
    \node (pistar) at (1, -2.2) {$v_*$};
    \draw[-stealth, line width=0.5 mm] (-.8, -2.025) -- (.8, -2.025);
    \draw[stealth-, line width=0.5 mm] (-.8, -2.275) -- (.8, -2.275);
  \end{tikzpicture}
\end{document}
```

## {.left}

We use the term \emph{generalized policy iteration} (GPI) to refer to the general idea of letting policy evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all RL learning methods can be described as GPI: i.e. all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested in our diagram. If both the evaluation process and improvement process stabilize, then the value function and policy must be optimal: the value function only stabilizes when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies the Bellman optimality equation holds and thus that the policy and value function are optimal!

# General Policy Iteration {.columns}

## {.right}

``` {.tex .render width=400px}
\documentclass{standalone}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{latexsym}
\usetikzlibrary{positioning}
\begin{document}
\pagestyle{empty}
  \begin{tikzpicture}
    \node (pi) at (-1, 0) {$\pi$};
    \node (v)  at ( 1, 0) {$V$};
    \path[-stealth, line width=0.5 mm] (pi) edge[bend left = 90] (v);
    \path[-stealth, line width=0.5 mm] (v)  edge[bend left = 90] (pi);
    \node at (0, 1.1) {evaluation};
    \node at (0, 0.55) {$\scriptstyle V \leadsto v_\pi$};
    \node at (0, -0.4) {$\scriptstyle \pi \leadsto \textrm{\tiny greedy}(V)$};
    \node at (0, -1.1) {improvement};
    \node at (0, -1.5) {$\vdots$};
    \node (pistar) at (-1, -2.2) {$\pi_*$};
    \node (pistar) at (1, -2.2) {$v_*$};
    \draw[-stealth, line width=0.5 mm] (-.8, -2.025) -- (.8, -2.025);
    \draw[stealth-, line width=0.5 mm] (-.8, -2.275) -- (.8, -2.275);
  \end{tikzpicture}
\end{document}
```

## {.left}

We use the term \emph{generalized policy iteration} (GPI) to refer to the general idea of letting policy evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes. Almost all RL learning methods can be described as GPI: i.e. all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested in our diagram. If both the evaluation process and improvement process stabilize, then the value function and policy must be optimal: the value function only stabilizes when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function. This implies the Bellman optimality equation holds and thus that the policy and value function are optimal!

# General Policy Iteration {.columns}

## {.right}

``` {.tex .render width=400px}
\documentclass{standalone}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{latexsym}
\usetikzlibrary{positioning}
\begin{document}
\pagestyle{empty}
  \begin{tikzpicture}[scale=3/4]
    \draw[line width=0.3 mm] (-4, 2) -- (0, 0) node[above left = 3mm, rotate = -28] {$v = v_\pi$};  % y = -1/2x
    \draw[line width=0.3 mm] (-3.5,-1.16666) -- (0, 0) node[below left, rotate=20] {$\pi =       \textrm{greedy}(v)$}; % y= 1/3*x
    \node at (0.75, 0) {$v_*, \pi_*$};
    \node[draw,circle,fill,scale=1/4,blue!50] (init) at (-4, 0.25) {};
    \node[below = 1mm of init] {$\scriptstyle v,\pi$};
    \node[draw,circle,fill,scale=1/4,blue!50] (one)  at (-3.5, 1.75) {};
    \node[draw,circle,fill,scale=1/4,blue!50] (two)  at (-3, -1) {};
    \node[draw,circle,fill,scale=1/4,blue!50] (three)at (-2.5, 1.25) {};
    \node[draw,circle,fill,scale=1/4,blue!50] (four) at (-2, -2/3) {};
    \node[draw,circle,fill,scale=1/4,blue!50] (five) at (-1.5, 0.75) {};
    \node[draw,circle,fill,scale=1/4,blue!50] (six)  at (-1, -1/3) {};
    \draw[-stealth,blue!50] (init) -- (one);
    \draw[-stealth,blue!50] (one) -- (two);
    \draw[-stealth,blue!50] (two) -- (three);
    \draw[-stealth,blue!50] (three) -- (four);
    \draw[-stealth,blue!50] (four) --(five);
    \draw[-stealth,blue!50] (five) -- (six);
  \end{tikzpicture}
\end{document}
```

## {.left}

We can think of the nteraction between the evaluation and improvement processes in GPI in terms of two constraints or goals -- for example as two lines in two-dimensional space as suggested by the diagram. Although the real geometry is much more complicated, the diagram suggests what happens in the real case. Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals \emph{interact} because the lines are not orthogonal: driving directly toward one goal causes some movement away from the other goal; inevitably, however, the joint process is brought closer to the overall goal of optimality. The arrows in the diagram correspond to the behavior of policy iteration in that each takes the system all the way to achieving one of the two goals completely. In GPI, one could also take smaller, incomplete steps toward each goal. In either case, the two processes together achieve the overall goal of optimality even though neither is attempting to achieve it directly!

#

:::col40

## DP: Policy Iteration

Iterative procedure to improve the policy when combining policy evaluation and improvement.

\

TODO TiKz

![](../data/03/silver_policyIteration.svg){width=600px}

:::

:::col60

![](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html){width="100%" height="700px"}

:::

## {.footer}

[@silver2015;@karpathy_mdp] -->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="drl-robot-soccer" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DRL Robot Soccer</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="stream" style="width:auto;height:auto;">
<iframe style="width:auto;height:720px;aspect-ratio:16/9;" allow="fullscreen" src="https://www.youtube.com/embed/iX6OgG67-ZQ?cc_load_policy=0&amp;controls=1&amp;iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;showinfo=0">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-robot-playing-soccer-hierarchical-rl" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Robot Playing Soccer – Hierarchical RL</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><a href="https://www.youtube.com/watch?v=iX6OgG67-ZQ">See Video of robots playing soccer</a></p>
</div>
<div id="section-53" class="footer box block">
<h2 class="footer"></h2>
<p>Details see <span class="citation">(<a href="#ref-huang2022soccer" role="doc-biblioref">Huang u. a. 2022</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-huang2022soccer" class="csl-entry">
Huang, Xiaoyu, Zhongyu Li, Yanzhen Xiang, Yiming Ni, Yufeng Chi, Yunhao Li, Lizhi Yang, Xue Bin Peng, und Koushil Sreenath. 2022. <span>„Creating a Dynamic Quadrupedal Robotic Goalkeeper with Reinforcement Learning“</span>. arXiv. doi:<a href="https://doi.org/10.48550/ARXIV.2210.04435">10.48550/ARXIV.2210.04435</a>.
</div>
<div id="ref-rl_notes_2019" class="csl-entry">
Santucci, Andreas. 2019. <span>„Course Notes from RL Specialization, University of Alberta“</span>. https://github.com/asantucci/rl_notes.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-weng2018rl" class="csl-entry">
Weng, Lilian. 2018a. <span>„A (Long) Peek into Reinforcement Learning“</span>. <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a>.
</div>
<div id="ref-weng2018bandit" class="csl-entry">
———. 2018b. <span>„The Multi-Armed Bandit Problem and Its Solutions“</span>. <a href="https://The Multi-Armed Bandit Problem and Its Solutions">The Multi-Armed Bandit Problem and Its Solutions</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("66ff34f2d.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
