<!DOCTYPE html>
<html>

    <head>
        <meta charset="utf-8">
        <meta name="generator" content="pandoc">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">         <meta name="author" content="Malte Schilling, Neuroinformatics Group, Bielefeld University">           <title>14 Deep Reinforcement Learning</title>
        <style type="text/css">
        :root {
                /* This is the global variable for the default alignment. Defaults to left.    */
                /* Can be overriden in local YAML and for whole slides or individual elements  */
                --align-global: left;
                /* Column spacing, in same awkward unit as reveal's margin which is fraction of 
                space. Beware, does strange things if set too large. */
                --margin-columns: calc(0em * 100%);
                /* Now all the different font sizes */
                --font-size-base: 28px;
                --font-size-medium: calc(var(--font-size-base) * 1);
                --font-size-xx-small: calc(var(--font-size-base) * 0.4);
                --font-size-x-small: calc(var(--font-size-base) * 0.6);
                --font-size-small: calc(var(--font-size-base) * 0.8);
                --font-size-large: calc(var(--font-size-base) * 1.2);
                --font-size-x-large: calc(var(--font-size-base) * 1.4);
                --font-size-xx-large: calc(var(--font-size-base) * 1.6);
                /* This one defines the basic vertical spacing between elements */
                --spacing-vertical-base: 0.5em;
                --margin-bottom-elements-base: 0.5em;
                --margin-bottom-h1: calc(var(--margin-bottom-elements-base) * 1);
                --margin-bottom-h2: calc(var(--margin-bottom-elements-base) * 0);
                --margin-bottom-h2-block: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-h3: calc(var(--margin-bottom-elements-base) * 0);
                --margin-bottom-h3-block: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-p: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-ol: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-ul: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-dl: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-figure: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-img: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-blockquote: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-table: calc(var(--margin-bottom-elements-base) * 1);
                --margin-bottom-columns: calc(var(--margin-bottom-elements-base) * 0);       
                --margin-bottom-references: calc(var(--margin-bottom-elements-base) * 0.3);
                /* The next custom properties for the look of the decoration */
                --border-decoration-width: 20px;
                --border-decoration-padding: 10px;
                --border-decoration-style: solid;  
                /* whiteboard buttons */
                --whiteboard-icon-size: 2vmin;
                --whiteboard-active-color: rgb(42,157,223);
                --whiteboard-inactive-color: lightgrey;
                --whiteboard-background-color: rgb(250,250,250);      
        } 
        </style>
                
    <link rel="stylesheet" href="../support/css/handout.css">
    

    <link rel="stylesheet" href="../support/examiner/examiner.css">


    <!-- MathJax config -->
    <script>
        window.MathJax = {
            loader: {
                load: ['[tex]/ams']
            },
            svg: {
                minScale: .5,                  // smallest scaling factor to use
                matchFontHeight: false,        // true to match ex-height of surrounding font
                mtextInheritFont: false,       // true to make mtext elements use surrounding font
                merrorInheritFont: true,       // true to make merror text use surrounding font
                mathmlSpacing: false,          // true for MathML spacing rules, false for TeX rules
                skipAttributes: {},            // RFDa and other attributes NOT to copy to the output
                exFactor: .5,                  // default size of ex in em units
                displayAlign: 'center',        // default for indentalign when set to 'auto'
                displayIndent: '0',            // default for indentshift when set to 'auto'
                fontCache: 'none',             // or 'global' or 'none'
                localID: null,                 // ID to use for local font cache (for single equation processing)
                internalSpeechTitles: true,    // insert <title> tags with speech content
                titleID: 0                     // initial id number to use for aria-labeledby titles
            },
            tex: {
                tags: 'ams',
                packages: {
                    '[+]': ['ams']
                },
                macros: {
                    R: "{\\mathrm{{I}\\kern-.15em{R}}}",
                    laplace: "{\\Delta}",
                    grad: "{\\nabla}",
                    T: "^{\\mathsf{T}}",
                    norm: ['\\left\\Vert #1 \\right\\Vert', 1],
                    iprod: ['\\left\\langle #1 \\right\\rangle', 1],
                    vec: ['\\boldsymbol{\\mathbf{#1}}', 1],
                    mat: ['\\boldsymbol{\\mathbf{#1}}', 1],
                    set: ['\\mathcal{#1}', 1],
                    func: ['\\mathrm{#1}', 1],
                    trans: ['{#1}\\mkern-1mu^{\\mathsf{T}}', 1],
                    matrix: ['\\begin{bmatrix} #1 \\end{bmatrix}', 1],
                    vector: ['\\begin{pmatrix} #1 \\end{pmatrix}', 1],
                    of: ['\\mkern{-2mu}\\left( #1 \\right\)', 1],
                    diff: ['\\frac{\\mathrm{d}{#1}}{\\mathrm{d}{#2}}', 2],
                    pdiff: ['\\frac{\\partial {#1}}{\\partial {#2}}', 2],

                    vc: ['\\mathbf{#1}', 1],
                    abs: ['\\lvert#1\\rvert', 1],
                    norm: ['\\lVert#1\\rVert', 1],
                    det: ['\\lvert#1\\rvert', 1],
                    qt: ['\\hat{\\vc {#1}}', 1],
                    mt: ['\\boldsymbol{#1}', 1],
                    pt: ['\\boldsymbol{#1}', 1],
                    textcolor: ['\\color{#1}', 1]
                }
            },
            options: {
                // disable menu
                renderActions: {
                    addMenu: [0, '', '']
                },
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async src="../support/vendor/mathjax/tex-svg.js">
    </script>

    <script>
      var socket = new WebSocket("ws://" + location.host + "/reload");
      socket.onmessage = function (event) {
        if (event.data.startsWith("reload!")) 
          window.location.reload(true);
      };
    </script>

                        <link rel="stylesheet" href="../mschilling_10.css">
            </head>

    <body class="document handout" onload="initHandout();">
                        <header>
          <h1 class="title">14 Deep Reinforcement Learning</h1>
                    <h2 class="subtitle">Advanced Machine Learning</p>
                              <p class="lead author">Malte Schilling, Neuroinformatics Group, Bielefeld University</p>
                            </header>
                
<!-- body must not be indented or code blocks render badly -->
<!-- The following line must be left aligned! -->
<section class="slide level1">

<div class="slide-wrapper">
<hr />
<section id="recap" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Recap</h1>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="markov-decision-process" class="slide level1 columns">
<h1 class="columns">Markov Decision Process</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="section"></h2>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span>: Each location is a state.</li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span>: North, West, South, East</li>
<li>a transition probability function <span class="math inline">\(\mathcal{P}\)</span>: here deterministic</li>
<li>a reward function <span class="math inline">\(\mathcal{R}\)</span>: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise, for state <span class="math inline">\(A: +10\)</span> and <span class="math inline">\(B: +5\)</span></li>
<li>the discount factor <span class="math inline">\(\gamma\)</span> – influences long term return</li>
</ul>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right">
<h2 class="right" id="section-1"></h2>
<img class="decker" data-src="../data/12/sutton_3_2_gridworld_MDP.svg" alt="sutton_3_2_gridworld_MDP.svg" style="height:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom footer">
<h2 class="bottom footer" id="section-2"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="from-return-to-value-function" class="slide level1">
<h1>From Return to Value Function</h1>
<div class="box columns definition">
<h2 class="definition" id="return">Return</h2>
<p>cumulative future reward is a total sum of discounted rewards going forward, starting from time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[ G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \]</span></p>
<p>As future rewards are usually more uncertain, they are weighted less through the <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1]\)</span>.</p>
</div>
<div class="box columns">
<h2 id="value-function">Value Function</h2>
<p>The value function measures the estimated goodness of a state, i.e. how rewarding a state is by a prediction of the cumulative future reward.</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-3"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="state-value-function" class="slide level1">
<h1>State-Value Function</h1>
<p>The <strong>state-value function</strong> of a state <span class="math inline">\(s\)</span> is the expected return if we are in this state at time <span class="math inline">\(t, S_t=s\)</span>:</p>
<p><span class="math display">\[V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]\]</span></p>
<ul>
<li>Used to evaluate the goodness/badness of states,</li>
<li>and therefore to select between actions.</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-4"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="action-value-function-q-function" class="slide level1">
<h1>Action-Value Function (Q-Function)</h1>
<p>The <strong>action-value function</strong> (“Q-value”) of a state-action pair is defined as:</p>
<p><span class="math display">\[Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]\]</span></p>
<p>When following a target policy <span class="math inline">\(\pi\)</span>, we can integrate over the probility distribution of possible actions which again leads to the state-value function:</p>
<p><span class="math display">\[V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)\]</span></p>
<div class="box columns">
<h2 id="advantage-function">Advantage Function</h2>
<p>The difference between action-value and state-value is the <strong>action advantage function</strong></p>
<p><span class="math display">\[A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)\]</span></p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-5"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="optimal-solution-for-the-gridworld-example" class="slide level1">
<h1>Optimal Solution for the Gridworld Example</h1>
<img class="decker" data-src="../data/12/sutton_3_5_gridworld.svg" alt="sutton_3_5_gridworld.svg" style="width:1000px;">
<p>Each location is a state. Discount factor is <span class="math inline">\(0.9\)</span>.</p>
<p>Actions: North, West, South, East</p>
<p>Reward: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise</p>
<p>For state <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: all actions lead to <span class="math inline">\(A&#39;\)</span> and a reward of <span class="math inline">\(+10\)</span> (respectively <span class="math inline">\(B&#39;, +5\)</span>).</p>
<div class="box columns footer">
<h2 class="footer" id="section-6"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="reinforcement-learning-approaches" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Reinforcement Learning Approaches</h1>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="common-approaches" class="slide level1">
<h1>Common Approaches</h1>
<div class="col40">
<ul>
<li>Dynamic Programming
<ul>
<li>Policy Evaluation</li>
<li>Policy Improvement</li>
<li>Policy Iteration</li>
</ul></li>
<li>Monte-Carlo Methods</li>
<li>Temporal-Difference Learning
<ul>
<li>SARSA: On-Policy TD control</li>
<li>Q-Learning: Off-Policy TD control</li>
</ul></li>
<li>Policy Gradient</li>
</ul>
</div>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html" style="width:1000px;height:620px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-7"></h2>
<p><span class="citation" data-cites="karpathy_mdp">[@karpathy_mdp]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="section-8" class="slide level1">
<h1></h1>
<section id="dp-policy-iteration" class="col40">
<h2>DP: Policy Iteration</h2>
<p>Iterative procedure to improve the policy when combining policy evaluation and improvement.</p>
<p><br />
</p>
<img class="decker" data-src="../data/13/silver_policyIteration.svg" alt="silver_policyIteration.svg" style="width:600px;">
</section>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html" style="width:100%;height:700px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-9"></h2>
<p><span class="citation" data-cites="silver2015 karpathy_mdp">[@silver2015;@karpathy_mdp]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="exploring-unknown-state-space" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Exploring Unknown State Space</h1>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="monte-carlo-methods" class="slide level1">
<h1>Monte-Carlo Methods</h1>
<p>… learns from episodes of raw experience without modeling the environmental dynamics.</p>
<p>MC methods computes the observed mean return as an approximation of the expected return.</p>
<p>Computation of the empirical return <span class="math inline">\(G_t\)</span> requires complete episodes <span class="math inline">\(S_1, A_1, R_2, ... , S_T\)</span>:</p>
<p><span class="math display">\[
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}, Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
\]</span></p>
<p>An optimal policy can be learned through an iteration of evaluation and improvement (similar to GPI).</p>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="from-monte-carlo-to-temporal-difference-learning" class="slide level1 columns">
<h1 class="columns">From Monte-Carlo to Temporal-Difference Learning</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="monte-carlo-backup">Monte-Carlo Backup</h2>
<img class="decker" data-src="../data/13/silver_mc_backup.svg" alt="silver_mc_backup.svg" style="width:600px;">
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right fragment">
<h2 class="right" id="td-backup">TD Backup</h2>
<img class="decker" data-src="../data/13/silver_td_backup.svg" alt="silver_td_backup.svg" style="width:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom">
<h2 class="bottom" id="section-10"></h2>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-11"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="temporal-difference-learning" class="slide level1">
<h1>Temporal-Difference Learning</h1>
<ul>
<li>model-free method – no knowledge of MDP required</li>
<li>learns from episodes of experience – but can learn from incomplete episodes (!)</li>
</ul>
<div class="box columns definition">
<h2 class="definition" id="bootstrapping">Bootstrapping</h2>
<p>TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods.</p>
</div>
<div class="box columns">
<h2 id="section-12"></h2>
<p>The key idea in TD learning is to update the value function <span class="math inline">\(V(S_t)\)</span> towards an estimated return <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> (known as “TD target”).</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-13"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="td-value-estimation" class="slide level1">
<h1>TD: Value Estimation</h1>
<p>Update of the value function is regulated by the learning rate <span class="math inline">\(\alpha\)</span>.</p>
<p>In brief: TD means update a guess (of the value function) towards a guess (experiencing a single step and a guess of what follows):</p>
<p><span class="math display">\[\begin{align*}
V(S_t) &amp;\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &amp;\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &amp;\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{align*}\]</span></p>
<p>Similarly for the Q-function: <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
\]</span></p>
<div class="box columns footer">
<h2 class="footer" id="section-14"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="comparison-of-monte-carlo-and-td-control" class="slide level1">
<h1>Comparison of Monte-Carlo and TD Control</h1>
<p>Temporal-difference (TD) learning has several advantages over Monte-Carlo methods (MC):</p>
<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Can use incomplete sequences</li>
</ul>
<div class="box columns fragment">
<h2 id="natural-idea">Natural idea:</h2>
<p>Use TD instead of MC in our iterative control approach:</p>
<ul>
<li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li>
<li>Use <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</li>
<li>But now: update every time-step</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-15"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="comparison-of-monte-carlo-and-td" class="slide level1 columns">
<h1 class="columns">Comparison of Monte-Carlo and TD</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="monte-carlo-backup-1">Monte-Carlo Backup</h2>
<img class="decker" data-src="../data/13/silver_mc_backup.svg" alt="silver_mc_backup.svg" style="width:600px;">
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right">
<h2 class="right" id="td-backup-1">TD Backup</h2>
<img class="decker" data-src="../data/13/silver_td_backup.svg" alt="silver_td_backup.svg" style="width:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom">
<h2 class="bottom" id="section-16"></h2>
<p>Apply TD to <span class="math inline">\(Q(S, A)\)</span>, use <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement updating every time-step.</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-17"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="sarsa-as-on-policy-td-control" class="slide level1">
<h1>SARSA as On-Policy TD control</h1>
<div class="col80">
<p>SARSA realizes such an update procedure on a sequence <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots\)</span></p>
<ol type="1">
<li>At time step <span class="math inline">\(t\)</span>, we start from state <span class="math inline">\(S_t\)</span> and pick action according to <span class="math inline">\(Q\)</span> values, <span class="math inline">\(A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)\)</span>; <span class="math inline">\(\varepsilon\)</span>-greedy is commonly applied.</li>
<li>With action <span class="math inline">\(A_t\)</span>, we observe reward <span class="math inline">\(R_{t+1}\)</span> and get into the next state <span class="math inline">\(S_{t+1}\)</span>.</li>
<li>Then pick the next action in the same way as in step 1.</li>
<li>Update the action-value function: <span class="math inline">\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))\)</span></li>
<li><span class="math inline">\(t = t+1\)</span> and repeat from step 1.</li>
</ol>
</div>
<div class="col20">
<img class="decker" data-src="../data/13/sutton_sarsa.svg" alt="sutton_sarsa.svg" style="height:480px;">
</div>
<div class="box columns footer">
<h2 class="footer" id="section-18"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="on-policy-control-with-sarsa" class="slide level1">
<h1>On-Policy Control with SARSA</h1>
<p>At every timestep:</p>
<ul>
<li>Policy Evaluation following SARSA, <span class="math inline">\(Q \approx q_{\pi}\)</span></li>
<li>Policy improvement: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</li>
</ul>
<img class="decker" data-src="../data/13/silver_sarsa_improvement.svg" alt="silver_sarsa_improvement.svg" style="width:640px;">
<div class="box columns footer">
<h2 class="footer" id="section-19"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="section-20" class="slide level1">
<h1></h1>
<section id="td-learning-example" class="col40">
<h2>TD Learning Example</h2>
<p><span class="math display">\[\begin{align*}

Q(S_t, A_t) \leftarrow &amp; Q(S_t, A_t) + \\ &amp; \alpha (R_{t+1} + \\ &amp; \gamma Q(S_{t+1}, A_{t+1}) \\ &amp;- Q(S_t, A_t))

\end{align*}\]</span></p>
<p>Two stochastic sources:</p>
<ol type="1">
<li>the environment</li>
<li>the agent policy</li>
</ol>
<p>Difference to DP: TD Learning estimates the value functions of an agent, collecting experience online.</p>
</section>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html" style="width:100%;height:700px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-21"></h2>
<p><span class="citation" data-cites="karpathy_mdp">[@karpathy_mdp]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="off-policy-learning" class="slide level1">
<h1>Off-Policy Learning</h1>
<p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(V_{\pi}(s)\)</span> or <span class="math inline">\(Q_{\pi}(s,a)\)</span></p>
<p>… while following different behavior policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[
{S_1,A_1,R_2,...,S_T} \sim \mu
\]</span></p>
<p>Why is this important?</p>
<div class="box columns fragment">
<h2 id="section-22"></h2>
<ul>
<li>Learn from observing humans or other agents.</li>
<li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, ...\)</span></li>
<li>Learn about optimal policy while following exploratory policy.</li>
<li>Learn about multiple policies while following one policy.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-23"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="q-learning" class="slide level1">
<h1>Q-Learning</h1>
<p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s,a)\)</span>:</p>
<ul>
<li>Next action is chosen using behaviour policy <span class="math inline">\(A_{t+1} \sim \mu(·|S_t)\)</span></li>
<li>But we consider alternative successor action <span class="math inline">\(A&#39; \sim \pi(·|S_t)\)</span></li>
<li>And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-24"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="q-learning-off-policy-td-control" class="slide level1">
<h1>Q-Learning – Off-Policy TD control</h1>
<div class="col20">
<img class="decker" data-src="../data/13/sutton_q_backup.svg" alt="sutton_q_backup.svg" style="height:360px;">
</div>
<div class="col70">
<ol type="1">
<li>At time step <span class="math inline">\(t\)</span>, we start from state <span class="math inline">\(S_t\)</span> and pick action according to <span class="math inline">\(Q\)</span> values, <span class="math inline">\(A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)\)</span>; <span class="math inline">\(\varepsilon\)</span>-greedy is commonly applied.</li>
<li>With action <span class="math inline">\(A_t\)</span>, we observe reward <span class="math inline">\(R_{t+1}\)</span> and get into the next state <span class="math inline">\(S_{t+1}\)</span>.</li>
<li>Update the action-value function: <span class="math inline">\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))\)</span></li>
<li><span class="math inline">\(t = t+1\)</span> and repeat from step 1.</li>
</ol>
</div>
<p>Difference to SARSA: Q-learning does not follow the current policy to pick the second action, but rather estimate <span class="math inline">\(Q_∗\)</span> out of the best <span class="math inline">\(Q\)</span> values independently.</p>
<div class="box columns footer">
<h2 class="footer" id="section-25"></h2>
<p><span class="citation" data-cites="weng2018rl sutton2018">[@weng2018rl;@sutton2018]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="convergence-of-q-learning" class="slide level1">
<h1>Convergence of Q-Learning</h1>
<p>Q-learning converges towards an optimal policy. Even if you’re acting suboptimally, this process converges.</p>
<p>Caveats:</p>
<ul>
<li>Needs exhaustive exploration to guarantee convergence for suboptimal exploration.</li>
<li>eventually learning rate must be quite small, but can not be reduced too quickly</li>
</ul>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="relationship-between-dp-and-td" class="slide level1">
<h1>Relationship Between DP and TD</h1>
<img class="decker" data-src="../data/13/silver_dp_td.svg" alt="silver_dp_td.svg" style="width:1000px;">
<div class="box columns footer">
<h2 class="footer" id="section-26"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="relationship-between-dp-and-td-2" class="slide level1">
<h1>Relationship Between DP and TD 2</h1>
<img class="decker" data-src="../data/13/silver_dp_td_equations.svg" alt="silver_dp_td_equations.svg" style="width:1000px;">
<div class="box columns footer">
<h2 class="footer" id="section-27"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="reinforcement-learning-algorithms-overview" class="slide level1">
<h1>Reinforcement Learning Algorithms Overview</h1>
<img class="decker" data-src="../data/13/arulkumaran_drl.svg" alt="arulkumaran_drl.svg" style="width:640px;">
<div class="box columns footer">
<h2 class="footer" id="section-28"></h2>
<p><span class="citation" data-cites="arulkumaran2017brief">[@arulkumaran2017brief]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="deep-reinforcement-learning" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Deep Reinforcement Learning</h1>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="drawbacks-of-tabular-methods" class="slide level1">
<h1>Drawbacks of Tabular methods</h1>
<p>For tabular methods like basic Q-Learning: we keep a table of all <span class="math inline">\(Q\)</span>-values.</p>
<div class="box columns fragment">
<h2 id="section-29"></h2>
<p>In real world application: not possible to learn about every single state:</p>
<ul>
<li>too many states (or even continuous input spaces) to visit in training</li>
<li>table would be too large for so many states</li>
</ul>
<img class="decker" data-src="../data/14/klein_pacman.svg" alt="klein_pacman.svg" style="width:1000px;">
</div>
<div class="box columns footer">
<h2 class="footer" id="section-30"></h2>
<p><span class="citation" data-cites="kleinCS188">[@kleinCS188]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="generalization-over-states" class="slide level1">
<h1>Generalization over States</h1>
<p>In order to deal with continuous or large state spaces, we want to generalize. For this, we use <em>Function Approximation</em>:</p>
<ul>
<li>Learn about a small number of training states from experiences.</li>
<li>Generalize these experiences to new, similar situations.</li>
</ul>
<p>As a basic idea for <em>Deep Reinforcement Learning</em>: use Neural Networks for function approximation.</p>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="possible-problems-for-function-approximation" class="slide level1">
<h1>Possible Problems for Function Approximation</h1>
<p>Goal: apply efficiency and flexibility of TD methods to realistic problems</p>
<div class="box columns">
<h2 id="problem-deadly-triad">Problem: Deadly Triad</h2>
<p>Approach is …</p>
<ul>
<li>off-policy,</li>
<li>employs non-linear function approximation,</li>
<li>and uses bootstrapping.</li>
</ul>
<p>Combined: can become unstable or does not converge!</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-31"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="deep-q-networks" class="slide level1">
<h1>Deep Q-Networks</h1>
<p>… improved and stabilized training of Q-learning when using a Deep Neural Network for function approximation.</p>
<p>Two innovative mechanisms:</p>
<div>
<ul>
<li class="fragment"><em>Experience Replay:</em> use a replay buffer for storing experiences.</li>
<li class="fragment">Periodically Update <em>Target network</em> that are employed for bootstrapping.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-32"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="dqn-architecture-overview" class="slide level1">
<h1>DQN Architecture Overview</h1>
<img class="decker" data-src="../data/14/mnih_dqn_architecture.png" alt="mnih_dqn_architecture.png">
<p>“we developed a novel agent, a deep Q-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network known as deep neural networks.”</p>
<div class="box columns footer">
<h2 class="footer" id="section-33"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="goal-of-dqn-approximation-of-q-function" class="slide level1">
<h1>Goal of DQN: Approximation of Q-Function</h1>
<div>
<ul>
<li class="fragment">Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP).</li>
<li class="fragment">It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter.</li>
<li class="fragment">One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment.</li>
<li class="fragment">Q-learning learns estimates of the optimal Q-values of an MDP, which means that behavior can be dictated by taking actions greedily with respect to the learned Q-values.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-34"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="problems-for-rl-and-deep-neural-networks" class="slide level1">
<h1>Problems for RL and Deep Neural Networks</h1>
<p>Reinforcement learning is known to be unstable when a nonlinear function approximator such as a neural network is used to represent the action-value function.</p>
<p><br />
</p>
<p>This instability has several causes:</p>
<ul>
<li>the correlations present in the sequence of observations,</li>
<li>the fact that small updates to <span class="math inline">\(Q\)</span> may significantly change the policy and therefore change the data distribution,</li>
<li>and the correlations between the action-values and the target values</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-35"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
</div>
<div class="slide-wrapper">
<hr />
<section id="references" class="slide level1 biblio">
<h1 class="biblio">References</h1>
</section>
</div>
</section>
<!-- The previous line must be left aligned! -->

        <script src="../support/vendor/js/jquery.min.js" type="text/javascript"></script>
        <script src="../support/vendor/js/lazyload.min.js" type="text/javascript"></script>
        <script src="../support/js/handout.js"></script>

        <script type="module">
          import {prepareExaminer} from "./../support/examiner/examiner.js";
          prepareExaminer();
        </script>

                        <hr>
        <address>
                    <p class="author"> Malte Schilling, Neuroinformatics Group, Bielefeld University</p>
                  </address>
        <script>
          var lazyLoadInstance = new LazyLoad({
            elements_selector: "img, video, iframe"
          });
        </script>
            </body>
  </html>
