<!DOCTYPE html>
<!-- This is the pandoc 2.7.3 template for reveal.js output modified for decker. -->
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Malte Schilling, Neuroinformatics Group, Bielefeld University">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <title>14 Deep Reinforcement Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="../support/vendor/reveal/css/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/css/reveal.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/thebelab/thebelab.css">
  <style>
        /* This is the global style block for decker's layout css variables (aka css custom properties) 
        for all kinds of parameterization. Variables are set via an according YAML variable 
        which defaults from the default.yaml. 
        Can't be sourced out to external css file since variable substitution is performed
        by pandoc which does not process the css */

        :root {
                /* This is the global variable for the default alignment. Defaults to left.    */
                /* Can be overriden in local YAML and for whole slides or individual elements  */
                --align-global: left;
                /* Column spacing, in same awkward unit as reveal's margin which is fraction of 
                space. Beware, does strange things if set too large. */
                --margin-columns: calc(0em * 100%);
                /* Now all the different font sizes */
                --font-size-base: 28px;
                --font-size-medium: calc(var(--font-size-base) * 1);
                --font-size-xx-small: calc(var(--font-size-base) * 0.4);
                --font-size-x-small: calc(var(--font-size-base) * 0.6);
                --font-size-small: calc(var(--font-size-base) * 0.8);
                --font-size-large: calc(var(--font-size-base) * 1.2);
                --font-size-x-large: calc(var(--font-size-base) * 1.4);
                --font-size-xx-large: calc(var(--font-size-base) * 1.6);
                /* This one defines the basic vertical spacing between elements */
                --spacing-vertical-base: 0.5em;
                --margin-bottom-elements-base: 0.5em;
                --margin-bottom-h1: calc(var(--margin-bottom-elements-base) * 1);
                --margin-bottom-h2: calc(var(--margin-bottom-elements-base) * 0);
                --margin-bottom-h2-block: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-h3: calc(var(--margin-bottom-elements-base) * 0);
                --margin-bottom-h3-block: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-p: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-ol: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-ul: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-dl: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-figure: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-img: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-blockquote: calc(var(--margin-bottom-elements-base) * 0.8);
                --margin-bottom-table: calc(var(--margin-bottom-elements-base) * 1);
                --margin-bottom-columns: calc(var(--margin-bottom-elements-base) * 0);       
                --margin-bottom-references: calc(var(--margin-bottom-elements-base) * 0.3);
                /* The next custom properties for the look of the decoration */
                --border-decoration-width: 20px;
                --border-decoration-padding: 10px;
                --border-decoration-style: solid;  
                /* whiteboard buttons */
                --whiteboard-icon-size: 2vmin;
                --whiteboard-active-color: rgb(42,157,223);
                --whiteboard-inactive-color: lightgrey;
                --whiteboard-background-color: rgb(250,250,250);      
        } 
  </style>
   
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
  <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
      <link rel="stylesheet" href="../support/css/decker.css">
        <link rel="stylesheet" href="../mschilling_10.css"/>
  <script>
    /* Store JSON encoded Pandoc meta data in a global variable for easy
    reference from any script. */
    window.Decker = {meta: {"fragmentInURL":true,"controls":true,"border-decoration-padding":"10px","height":"800","hideCursorTime":"2000.0","transition":"none","chalkboard":"chalkboard-deck.json","hash":true,"history":true,"progress":true,"border-decoration-width":"20px","center":false,"whiteboard-inactive-color":"lightgrey","whiteboard-icon-size":"2vmin","menu":true,"css":"../mschilling.css","overview":true,"width":"1280","margin":"5.0e-2","spacing-vertical-base":"0.5em","bibliography":"AnalyseErwarteterFall/../References.bib","margin-columns":"0em","test123":"test123","help":true,"provisioning":"Copy","reveal":{"checkOverflow":true,"verticalSlides":false},"hideInactiveCursor":true,"backgroundTransition":"none","subtitle":"Advanced Machine Learning","whiteboard-active-color":"rgb(42,157,223)","rsync-destination":{"path":"/some/path","host":"git@gitlab2.informatik.uni-wuerzburg.de"},"zoom":true,"keyboard":true,"touch":true,"author":"Malte Schilling, Neuroinformatics Group, Bielefeld University","mario":false,"title":"14 Deep Reinforcement Learning","template":{"css":"../mschilling_10.css"},"dictionary":{"de":{"quiz":{"qmi-drop-hint":"...und hier in die richtige Kategorie einsortieren.","solve-button":"Lösung","reset-button":"Zurücksetzen","input-placeholder":"Eingeben und 'Enter'","qmi-drag-hint":"Objekte per Drag&amp;Drop ziehen...","solution":"Lösung"},"exam":{"points":"Punkte","Medium":"Mittelschwer","solve-button":"Lösen","Easy":"Leicht","reset-button":"Zurücksetzen","Hard":"Schwer","again-button":"Nochmal","placeholder":"Antwort eingeben","solution":"Lösung"}},"en":{"quiz":{"qmi-drop-hint":"...and put them here into the correct category.","reset-button":"Reset","solution-button":"Show Solution","input-placeholder":"Type and press 'Enter'","qmi-drag-hint":"Drag items from here...","solution":"Show Solution"},"exam":{"points":"Points","Medium":"Medium","solve-button":"Solve","Easy":"Easy","reset-button":"Reset","Hard":"Hard","again-button":"Again","placeholder":"Type in answer","solution":"Solution"}}},"align-global":"left","vertical-slides":true,"font-size-base":"28px","fragments":true,"whiteboard-background-color":"rgb(250,250,250)","border-decoration-style":"solid","thebelab":{"token":"plc","baseUrl":"http://localhost:8192/","repoProvider":"github","enable":true,"language":"python3","binderUrl":"https://mybinder.org","repo":"malteschilling/advml_binder","ref":"master"}}};
  </script>

  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? String.raw`../support/vendor/reveal/css/print/pdf.css` : String.raw`../support/vendor/reveal/css/print/paper.css`;
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@^4//lib/js/html5shiv.js"></script>
  <![endif]-->
  <script type="text/x-thebe-config">
  {
      bootstrap: false,
      requestKernel: false,
      predefinedOutput: false,
      binderOptions: {
          repo: "malteschilling/advml_binder",
          ref: "master",
          binderUrl: "https://mybinder.org",
          repoProvider: "github",
      },
      kernelOptions: {
          name: "python3",
          kernelName: "python3"
      },
      selector: "[data-executable]",
      mathjaxUrl: false,
      codeMirrorConfig: {
          mode: "python3"
      }
  }
  </script>
   <script src="https://unpkg.com/thebelab@latest/lib/index.js"></script> 

</head>
<body data-deckid="">
  <div class="reveal">
    <div class="slides">

    <!-- standard settings -->
               <section id="title-slide" class="title-page">
              <div class="valign-block">
            <div class="valigned">
                <div class="title"> 14 Deep Reinforcement Learning</div>
                                   <div class="subtitle"> Advanced Machine Learning </div>
                                                   <div style="height:100px"> </div>      
                                                                   <div class="author"> Malte Schilling, Neuroinformatics Group, Bielefeld University </div>
                                                 
            </div>
        </div>
         </section>      
   


<section id="recap" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Recap</h1>
</section>
<section id="markov-decision-process" class="slide level1 columns">
<h1 class="columns">Markov Decision Process</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="section"></h2>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span>: Each location is a state.</li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span>: North, West, South, East</li>
<li>a transition probability function <span class="math inline">\(\mathcal{P}\)</span>: here deterministic</li>
<li>a reward function <span class="math inline">\(\mathcal{R}\)</span>: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise, for state <span class="math inline">\(A: +10\)</span> and <span class="math inline">\(B: +5\)</span></li>
<li>the discount factor <span class="math inline">\(\gamma\)</span> – influences long term return</li>
</ul>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right">
<h2 class="right" id="section-1"></h2>
<img class="decker" data-src="../data/12/sutton_3_2_gridworld_MDP.svg" alt="sutton_3_2_gridworld_MDP.svg" style="height:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom footer">
<h2 class="bottom footer" id="section-2"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</div>
</section>
<section id="from-return-to-value-function" class="slide level1">
<h1>From Return to Value Function</h1>
<div class="box columns definition">
<h2 class="definition" id="return">Return</h2>
<p>cumulative future reward is a total sum of discounted rewards going forward, starting from time <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[ G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \]</span></p>
<p>As future rewards are usually more uncertain, they are weighted less through the <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1]\)</span>.</p>
</div>
<div class="box columns">
<h2 id="value-function">Value Function</h2>
<p>The value function measures the estimated goodness of a state, i.e. how rewarding a state is by a prediction of the cumulative future reward.</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-3"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="state-value-function" class="slide level1">
<h1>State-Value Function</h1>
<p>The <strong>state-value function</strong> of a state <span class="math inline">\(s\)</span> is the expected return if we are in this state at time <span class="math inline">\(t, S_t=s\)</span>:</p>
<p><span class="math display">\[V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]\]</span></p>
<ul>
<li>Used to evaluate the goodness/badness of states,</li>
<li>and therefore to select between actions.</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-4"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="action-value-function-q-function" class="slide level1">
<h1>Action-Value Function (Q-Function)</h1>
<p>The <strong>action-value function</strong> (“Q-value”) of a state-action pair is defined as:</p>
<p><span class="math display">\[Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]\]</span></p>
<p>When following a target policy <span class="math inline">\(\pi\)</span>, we can integrate over the probility distribution of possible actions which again leads to the state-value function:</p>
<p><span class="math display">\[V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)\]</span></p>
<div class="box columns">
<h2 id="advantage-function">Advantage Function</h2>
<p>The difference between action-value and state-value is the <strong>action advantage function</strong></p>
<p><span class="math display">\[A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)\]</span></p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-5"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="optimal-solution-for-the-gridworld-example" class="slide level1">
<h1>Optimal Solution for the Gridworld Example</h1>
<img class="decker" data-src="../data/12/sutton_3_5_gridworld.svg" alt="sutton_3_5_gridworld.svg" style="width:1000px;">
<p>Each location is a state. Discount factor is <span class="math inline">\(0.9\)</span>.</p>
<p>Actions: North, West, South, East</p>
<p>Reward: <span class="math inline">\(-1\)</span> when trying to move out of the grid, <span class="math inline">\(0\)</span> otherwise</p>
<p>For state <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: all actions lead to <span class="math inline">\(A&#39;\)</span> and a reward of <span class="math inline">\(+10\)</span> (respectively <span class="math inline">\(B&#39;, +5\)</span>).</p>
<div class="box columns footer">
<h2 class="footer" id="section-6"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</section>
<section id="reinforcement-learning-approaches" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Reinforcement Learning Approaches</h1>
</section>
<section id="common-approaches" class="slide level1">
<h1>Common Approaches</h1>
<div class="col40">
<ul>
<li>Dynamic Programming
<ul>
<li>Policy Evaluation</li>
<li>Policy Improvement</li>
<li>Policy Iteration</li>
</ul></li>
<li>Monte-Carlo Methods</li>
<li>Temporal-Difference Learning
<ul>
<li>SARSA: On-Policy TD control</li>
<li>Q-Learning: Off-Policy TD control</li>
</ul></li>
<li>Policy Gradient</li>
</ul>
</div>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html" style="width:1000px;height:620px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-7"></h2>
<p><span class="citation" data-cites="karpathy_mdp">[@karpathy_mdp]</span></p>
</div>
</section>
<section id="section-8" class="slide level1">
<h1></h1>
<section id="dp-policy-iteration" class="col40">
<h2>DP: Policy Iteration</h2>
<p>Iterative procedure to improve the policy when combining policy evaluation and improvement.</p>
<p><br />
</p>
<img class="decker" data-src="../data/13/silver_policyIteration.svg" alt="silver_policyIteration.svg" style="width:600px;">
</section>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html" style="width:100%;height:700px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-9"></h2>
<p><span class="citation" data-cites="silver2015 karpathy_mdp">[@silver2015;@karpathy_mdp]</span></p>
</div>
</section>
<section id="exploring-unknown-state-space" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Exploring Unknown State Space</h1>
</section>
<section id="monte-carlo-methods" class="slide level1">
<h1>Monte-Carlo Methods</h1>
<p>… learns from episodes of raw experience without modeling the environmental dynamics.</p>
<p>MC methods computes the observed mean return as an approximation of the expected return.</p>
<p>Computation of the empirical return <span class="math inline">\(G_t\)</span> requires complete episodes <span class="math inline">\(S_1, A_1, R_2, ... , S_T\)</span>:</p>
<p><span class="math display">\[
V(s) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s]}, Q(s, a) = \frac{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a] G_t}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}
\]</span></p>
<p>An optimal policy can be learned through an iteration of evaluation and improvement (similar to GPI).</p>
</section>
<section id="from-monte-carlo-to-temporal-difference-learning" class="slide level1 columns">
<h1 class="columns">From Monte-Carlo to Temporal-Difference Learning</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="monte-carlo-backup">Monte-Carlo Backup</h2>
<img class="decker" data-src="../data/13/silver_mc_backup.svg" alt="silver_mc_backup.svg" style="width:600px;">
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right fragment">
<h2 class="right" id="td-backup">TD Backup</h2>
<img class="decker" data-src="../data/13/silver_td_backup.svg" alt="silver_td_backup.svg" style="width:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom">
<h2 class="bottom" id="section-10"></h2>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-11"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</div>
</section>
<section id="temporal-difference-learning" class="slide level1">
<h1>Temporal-Difference Learning</h1>
<ul>
<li>model-free method – no knowledge of MDP required</li>
<li>learns from episodes of experience – but can learn from incomplete episodes (!)</li>
</ul>
<div class="box columns definition">
<h2 class="definition" id="bootstrapping">Bootstrapping</h2>
<p>TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods.</p>
</div>
<div class="box columns">
<h2 id="section-12"></h2>
<p>The key idea in TD learning is to update the value function <span class="math inline">\(V(S_t)\)</span> towards an estimated return <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> (known as “TD target”).</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-13"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="td-value-estimation" class="slide level1">
<h1>TD: Value Estimation</h1>
<p>Update of the value function is regulated by the learning rate <span class="math inline">\(\alpha\)</span>.</p>
<p>In brief: TD means update a guess (of the value function) towards a guess (experiencing a single step and a guess of what follows):</p>
<p><span class="math display">\[\begin{align*}
V(S_t) &amp;\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &amp;\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &amp;\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
\end{align*}\]</span></p>
<p>Similarly for the Q-function: <span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
\]</span></p>
<div class="box columns footer">
<h2 class="footer" id="section-14"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="comparison-of-monte-carlo-and-td-control" class="slide level1">
<h1>Comparison of Monte-Carlo and TD Control</h1>
<p>Temporal-difference (TD) learning has several advantages over Monte-Carlo methods (MC):</p>
<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Can use incomplete sequences</li>
</ul>
<div class="box columns fragment">
<h2 id="natural-idea">Natural idea:</h2>
<p>Use TD instead of MC in our iterative control approach:</p>
<ul>
<li>Apply TD to <span class="math inline">\(Q(S, A)\)</span></li>
<li>Use <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</li>
<li>But now: update every time-step</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-15"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="comparison-of-monte-carlo-and-td" class="slide level1 columns">
<h1 class="columns">Comparison of Monte-Carlo and TD</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box columns left">
<h2 class="left" id="monte-carlo-backup-1">Monte-Carlo Backup</h2>
<img class="decker" data-src="../data/13/silver_mc_backup.svg" alt="silver_mc_backup.svg" style="width:600px;">
</div>
</div>
<div class="grow-1 column column-3">
<div class="box columns right">
<h2 class="right" id="td-backup-1">TD Backup</h2>
<img class="decker" data-src="../data/13/silver_td_backup.svg" alt="silver_td_backup.svg" style="width:600px;">
</div>
</div>
</div>
<div class="single-column-row">
<div class="box columns bottom">
<h2 class="bottom" id="section-16"></h2>
<p>Apply TD to <span class="math inline">\(Q(S, A)\)</span>, use <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement updating every time-step.</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-17"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</div>
</section>
<section id="sarsa-as-on-policy-td-control" class="slide level1">
<h1>SARSA as On-Policy TD control</h1>
<div class="col80">
<p>SARSA realizes such an update procedure on a sequence <span class="math inline">\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots\)</span></p>
<ol type="1">
<li>At time step <span class="math inline">\(t\)</span>, we start from state <span class="math inline">\(S_t\)</span> and pick action according to <span class="math inline">\(Q\)</span> values, <span class="math inline">\(A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)\)</span>; <span class="math inline">\(\varepsilon\)</span>-greedy is commonly applied.</li>
<li>With action <span class="math inline">\(A_t\)</span>, we observe reward <span class="math inline">\(R_{t+1}\)</span> and get into the next state <span class="math inline">\(S_{t+1}\)</span>.</li>
<li>Then pick the next action in the same way as in step 1.</li>
<li>Update the action-value function: <span class="math inline">\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))\)</span></li>
<li><span class="math inline">\(t = t+1\)</span> and repeat from step 1.</li>
</ol>
</div>
<div class="col20">
<img class="decker" data-src="../data/13/sutton_sarsa.svg" alt="sutton_sarsa.svg" style="height:480px;">
</div>
<div class="box columns footer">
<h2 class="footer" id="section-18"></h2>
<p><span class="citation" data-cites="sutton2018">[@sutton2018]</span></p>
</div>
</section>
<section id="on-policy-control-with-sarsa" class="slide level1">
<h1>On-Policy Control with SARSA</h1>
<p>At every timestep:</p>
<ul>
<li>Policy Evaluation following SARSA, <span class="math inline">\(Q \approx q_{\pi}\)</span></li>
<li>Policy improvement: <span class="math inline">\(\varepsilon\)</span>-greedy policy improvement</li>
</ul>
<img class="decker" data-src="../data/13/silver_sarsa_improvement.svg" alt="silver_sarsa_improvement.svg" style="width:640px;">
<div class="box columns footer">
<h2 class="footer" id="section-19"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="section-20" class="slide level1">
<h1></h1>
<section id="td-learning-example" class="col40">
<h2>TD Learning Example</h2>
<p><span class="math display">\[\begin{align*}

Q(S_t, A_t) \leftarrow &amp; Q(S_t, A_t) + \\ &amp; \alpha (R_{t+1} + \\ &amp; \gamma Q(S_{t+1}, A_{t+1}) \\ &amp;- Q(S_t, A_t))

\end{align*}\]</span></p>
<p>Two stochastic sources:</p>
<ol type="1">
<li>the environment</li>
<li>the agent policy</li>
</ol>
<p>Difference to DP: TD Learning estimates the value functions of an agent, collecting experience online.</p>
</section>
<div class="col60">
<iframe class="decker" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html" style="width:100%;height:700px;"></iframe>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-21"></h2>
<p><span class="citation" data-cites="karpathy_mdp">[@karpathy_mdp]</span></p>
</div>
</section>
<section id="off-policy-learning" class="slide level1">
<h1>Off-Policy Learning</h1>
<p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(V_{\pi}(s)\)</span> or <span class="math inline">\(Q_{\pi}(s,a)\)</span></p>
<p>… while following different behavior policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[
{S_1,A_1,R_2,...,S_T} \sim \mu
\]</span></p>
<p>Why is this important?</p>
<div class="box columns fragment">
<h2 id="section-22"></h2>
<ul>
<li>Learn from observing humans or other agents.</li>
<li>Re-use experience generated from old policies <span class="math inline">\(\pi_1, \pi_2, ...\)</span></li>
<li>Learn about optimal policy while following exploratory policy.</li>
<li>Learn about multiple policies while following one policy.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-23"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="q-learning" class="slide level1">
<h1>Q-Learning</h1>
<p>We now consider off-policy learning of action-values <span class="math inline">\(Q(s,a)\)</span>:</p>
<ul>
<li>Next action is chosen using behaviour policy <span class="math inline">\(A_{t+1} \sim \mu(·|S_t)\)</span></li>
<li>But we consider alternative successor action <span class="math inline">\(A&#39; \sim \pi(·|S_t)\)</span></li>
<li>And update <span class="math inline">\(Q(S_t, A_t)\)</span> towards value of alternative action</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-24"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="q-learning-off-policy-td-control" class="slide level1">
<h1>Q-Learning – Off-Policy TD control</h1>
<div class="col20">
<img class="decker" data-src="../data/13/sutton_q_backup.svg" alt="sutton_q_backup.svg" style="height:360px;">
</div>
<div class="col70">
<ol type="1">
<li>At time step <span class="math inline">\(t\)</span>, we start from state <span class="math inline">\(S_t\)</span> and pick action according to <span class="math inline">\(Q\)</span> values, <span class="math inline">\(A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)\)</span>; <span class="math inline">\(\varepsilon\)</span>-greedy is commonly applied.</li>
<li>With action <span class="math inline">\(A_t\)</span>, we observe reward <span class="math inline">\(R_{t+1}\)</span> and get into the next state <span class="math inline">\(S_{t+1}\)</span>.</li>
<li>Update the action-value function: <span class="math inline">\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))\)</span></li>
<li><span class="math inline">\(t = t+1\)</span> and repeat from step 1.</li>
</ol>
</div>
<p>Difference to SARSA: Q-learning does not follow the current policy to pick the second action, but rather estimate <span class="math inline">\(Q_∗\)</span> out of the best <span class="math inline">\(Q\)</span> values independently.</p>
<div class="box columns footer">
<h2 class="footer" id="section-25"></h2>
<p><span class="citation" data-cites="weng2018rl sutton2018">[@weng2018rl;@sutton2018]</span></p>
</div>
</section>
<section id="convergence-of-q-learning" class="slide level1">
<h1>Convergence of Q-Learning</h1>
<p>Q-learning converges towards an optimal policy. Even if you’re acting suboptimally, this process converges.</p>
<p>Caveats:</p>
<ul>
<li>Needs exhaustive exploration to guarantee convergence for suboptimal exploration.</li>
<li>eventually learning rate must be quite small, but can not be reduced too quickly</li>
</ul>
</section>
<section id="relationship-between-dp-and-td" class="slide level1">
<h1>Relationship Between DP and TD</h1>
<img class="decker" data-src="../data/13/silver_dp_td.svg" alt="silver_dp_td.svg" style="width:1000px;">
<div class="box columns footer">
<h2 class="footer" id="section-26"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="relationship-between-dp-and-td-2" class="slide level1">
<h1>Relationship Between DP and TD 2</h1>
<img class="decker" data-src="../data/13/silver_dp_td_equations.svg" alt="silver_dp_td_equations.svg" style="width:1000px;">
<div class="box columns footer">
<h2 class="footer" id="section-27"></h2>
<p><span class="citation" data-cites="silver2015">[@silver2015]</span></p>
</div>
</section>
<section id="reinforcement-learning-algorithms-overview" class="slide level1">
<h1>Reinforcement Learning Algorithms Overview</h1>
<img class="decker" data-src="../data/13/arulkumaran_drl.svg" alt="arulkumaran_drl.svg" style="width:640px;">
<div class="box columns footer">
<h2 class="footer" id="section-28"></h2>
<p><span class="citation" data-cites="arulkumaran2017brief">[@arulkumaran2017brief]</span></p>
</div>
</section>
<section id="deep-reinforcement-learning" class="slide level1 section" data-background-color="#2CA02C">
<h1 class="section" data-background-color="#2CA02C">Deep Reinforcement Learning</h1>
</section>
<section id="drawbacks-of-tabular-methods" class="slide level1">
<h1>Drawbacks of Tabular methods</h1>
<p>For tabular methods like basic Q-Learning: we keep a table of all <span class="math inline">\(Q\)</span>-values.</p>
<div class="box columns fragment">
<h2 id="section-29"></h2>
<p>In real world application: not possible to learn about every single state:</p>
<ul>
<li>too many states (or even continuous input spaces) to visit in training</li>
<li>table would be too large for so many states</li>
</ul>
<img class="decker" data-src="../data/14/klein_pacman.svg" alt="klein_pacman.svg" style="width:1000px;">
</div>
<div class="box columns footer">
<h2 class="footer" id="section-30"></h2>
<p><span class="citation" data-cites="kleinCS188">[@kleinCS188]</span></p>
</div>
</section>
<section id="generalization-over-states" class="slide level1">
<h1>Generalization over States</h1>
<p>In order to deal with continuous or large state spaces, we want to generalize. For this, we use <em>Function Approximation</em>:</p>
<ul>
<li>Learn about a small number of training states from experiences.</li>
<li>Generalize these experiences to new, similar situations.</li>
</ul>
<p>As a basic idea for <em>Deep Reinforcement Learning</em>: use Neural Networks for function approximation.</p>
</section>
<section id="possible-problems-for-function-approximation" class="slide level1">
<h1>Possible Problems for Function Approximation</h1>
<p>Goal: apply efficiency and flexibility of TD methods to realistic problems</p>
<div class="box columns">
<h2 id="problem-deadly-triad">Problem: Deadly Triad</h2>
<p>Approach is …</p>
<ul>
<li>off-policy,</li>
<li>employs non-linear function approximation,</li>
<li>and uses bootstrapping.</li>
</ul>
<p>Combined: can become unstable or does not converge!</p>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-31"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="deep-q-networks" class="slide level1">
<h1>Deep Q-Networks</h1>
<p>… improved and stabilized training of Q-learning when using a Deep Neural Network for function approximation.</p>
<p>Two innovative mechanisms:</p>
<div>
<ul>
<li class="fragment"><em>Experience Replay:</em> use a replay buffer for storing experiences.</li>
<li class="fragment">Periodically Update <em>Target network</em> that are employed for bootstrapping.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-32"></h2>
<p><span class="citation" data-cites="weng2018rl">[@weng2018rl]</span></p>
</div>
</section>
<section id="dqn-architecture-overview" class="slide level1">
<h1>DQN Architecture Overview</h1>
<img class="decker" data-src="../data/14/mnih_dqn_architecture.png" alt="mnih_dqn_architecture.png">
<p>“we developed a novel agent, a deep Q-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network known as deep neural networks.”</p>
<div class="box columns footer">
<h2 class="footer" id="section-33"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
<section id="goal-of-dqn-approximation-of-q-function" class="slide level1">
<h1>Goal of DQN: Approximation of Q-Function</h1>
<div>
<ul>
<li class="fragment">Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP).</li>
<li class="fragment">It works by learning an action-value function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter.</li>
<li class="fragment">One of the strengths of Q-learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment.</li>
<li class="fragment">Q-learning learns estimates of the optimal Q-values of an MDP, which means that behavior can be dictated by taking actions greedily with respect to the learned Q-values.</li>
</ul>
</div>
<div class="box columns footer">
<h2 class="footer" id="section-34"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
<section id="problems-for-rl-and-deep-neural-networks" class="slide level1">
<h1>Problems for RL and Deep Neural Networks</h1>
<p>Reinforcement learning is known to be unstable when a nonlinear function approximator such as a neural network is used to represent the action-value function.</p>
<p><br />
</p>
<p>This instability has several causes:</p>
<ul>
<li>the correlations present in the sequence of observations,</li>
<li>the fact that small updates to <span class="math inline">\(Q\)</span> may significantly change the policy and therefore change the data distribution,</li>
<li>and the correlations between the action-values and the target values</li>
</ul>
<div class="box columns footer">
<h2 class="footer" id="section-35"></h2>
<p><span class="citation" data-cites="mnih-dqn-2015">[@mnih-dqn-2015]</span></p>
</div>
</section>
<section id="references" class="slide level1 biblio">
<h1 class="biblio">References</h1>
</section>
    </div>
  </div>

  <script src="../support/vendor/reveal/js/reveal.js"></script>
  <script src="../support/vendor/js/jquery.min.js"></script>
  <script src="../support/vendor/videojs/video.min.js"></script>
  <script src="../support/js/decker.js"></script>
  <script src="../support/js/sage.js"></script>
  
  <script>
      // this has to be done before reveal highlight plugin is initialized
      prepareCodeHighlighting();
      prepareSAGE();
  </script>


  <link rel="stylesheet" href="../support/examiner/examiner.css">
  <script type="module">
    import {prepareExaminer} from "./../support/examiner/examiner.js";
    prepareExaminer();
  </script>





  <script>
      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        pdfMaxPagesPerSlide: 10,
        pdfSeparateFragments: false,
        // Display controls in the bottom right corner
        controls: Decker.meta.controls,
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        // Enables touch navigation on devices with touch input
        touch: true,
        // Loop the presentation
        // Change the presentation direction to be RTL
	// Randomizes the order of slides each time the presentation loads
        // Turns fragments on and off globally
        fragments: true,
        fragmentInURL: true,
        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Flags if speaker notes should be visible to all viewers
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        // Stop auto-sliding after user input
          // Hide cursor if inactive
          hideInactiveCursor: true,
          // Time before the cursor is hidden (in ms)
          hideCursorTime: 2000.0,
        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'none', // none/fade/slide/convex/concave/zoom
        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,
        height: 800,
        // Factor of the display size that should remain empty around the content
        margin: 5.0e-2,
        // whether to mark y-overflowing slides by a red border
        checkOverflow: false,


        // whether to use vertical slides (in decker.js)
         verticalSlides: true,


        // setup reveal-menu
        menu: {
          side: 'left',
          width: 'wide',
          numbers: false,
          titleSelector: 'h1',
          useTextContentForMissingTitles: false,
          hideMissingTitles: false,
          markers: true,
          custom: false,  
          themes: false,
          transitions: false,
          openButton: true,
          openSlideNumber: true,
          keyboard: true,
          sticky: false,
          autoOpen: true,
          delayInit: false,
          openOnInit: false,
          loadIcons: false
	    },

        // mathjax
        math: {
          mathjax: String.raw`../support/vendor/mathjax/`,
          macros: {
              R:         "{{\\mathrm{{I}\\kern-.15em{R}}}}",
              laplace:   "{\\Delta}",
              grad:      "{\\nabla}",
              T:         "^{\\mathsf{T}}",
              abs:       ['\\left\\lvert #1 \\right\\rvert', 1],
              det:       ['\\left\\lvert #1 \\right\\rvert', 1],
              norm:      ['\\left\\Vert #1 \\right\\Vert', 1],
              iprod:     ['\\left\\langle #1 \\right\\rangle', 1],
              vec:       ['{\\mathbf{\\boldsymbol{#1}}}', 1],
              mat:       ['{\\mathbf{\\boldsymbol{#1}}}', 1],
              set:       ['\\mathcal{#1}', 1],
              func:      ['\\mathrm{#1}', 1],
              trans:     ['{#1}\\mkern-1mu^{\\mathsf{T}}', 1],
              matrix:    ['\\begin{bmatrix} #1 \\end{bmatrix}', 1],
              vector:    ['\\begin{pmatrix} #1 \\end{pmatrix}', 1],
              of:        ['\\left( #1 \\right\)', 1],
              diff:      ['\\frac{\\mathrm{d}{#1}}{\\mathrm{d}{#2}}', 2],
              pdiff:     ['\\frac{\\partial {#1}}{\\partial {#2}}', 2],
              vc:        ['{\\mathbf{\\boldsymbol{#1}}}', 1],
              qt:        ['\\hat{\\vc {#1}}', 1],
              mt:        ['{\\mathbf{\\boldsymbol{#1}}}', 1],
              pt:        ['\\boldsymbol{#1}', 1],
              textcolor: ['\\color{#1}', 1]
          }
        },

          // setup charts
          chart: {
              defaults: {
                  global: {
                      tooltips: {
                          mode: 'point',
                          intersect: true,
                      },
                      legend: { position: "bottom" },
                      plugins: { colorschemes: { scheme: "tableau.Classic10" } }
                  },
              },
              bar:   { borderWidth: "1" },
              line:  { borderWidth: "2", tension: "0" },
              radar: { borderWidth: "2" }
          },


          // Henrik's ThebeLab plugin
          thebelab: true,

       

        
          // plugins
          dependencies: [
              { src: String.raw`../support/plugins/charts/Chart.js`},
              { src: String.raw`../support/plugins/charts/plugin-errorbars.js`},
              { src: String.raw`../support/plugins/charts/plugin-colorschemes.js`},
              { src: String.raw`../support/plugins/charts/plugin-csszoom.js`},
              { src: String.raw`../support/plugins/charts/csv2chart.js`},
              { src: String.raw`../support/plugins/math/math.js` },
              { src: String.raw`../support/plugins/whiteboard/whiteboard.js`},
              { src: String.raw`../support/plugins/zoom/zoom.js` },
              { src: String.raw`../support/plugins/print/print.js` },
              { src: String.raw`../support/plugins/search/search.js`, async: true },
              { src: String.raw`../support/plugins/quiz-wue/quiz-wue.js` },
 
    
              { src: String.raw`../support/plugins/menu/menu.js`, async: true },
              { src: String.raw`../support/plugins/thebelab/thebelab.js`, async: true },
              { src: String.raw`../support/plugins/explain/explain.js` },
              { src: String.raw`../support/vendor/reveal/plugin/notes/notes.js`, async: true }
          ]
      });
  </script>

    </body>
</html>
