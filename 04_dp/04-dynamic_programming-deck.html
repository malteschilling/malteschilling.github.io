<!DOCTYPE html>
<html lang="de">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Prof. Dr. Malte Schilling">
  <title>Deep Reinforcement Learning: 4 - Dynamic Programming</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <!-- Default values for CSS variables can live here. They can be overridden by
  meta data values. -->
  <link rel="stylesheet" href="../support/css/variables.css">

  <!-- Transfer meta data values from keys `palette.colors` and `css-variables`
  into a style sheet. Default values can come from `variables.css`. -->
  <style class="css-declarations">
    @media (prefers-color-scheme: light) {
      :root {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }

      :root.dark {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }
    }
    @media (prefers-color-scheme: dark) {
      :root {
                  --shade7: #ffffff;
                  --shade6: #e0e0e0;
                  --shade5: #c5c8c6;
                  --shade4: #b4b7b4;
                  --shade3: #969896;
                  --shade2: #373b41;
                  --shade1: #282a2e;
                  --shade0: #1d1f21;
                  --base0F-fg: #ceb9b5;
                  --base0F-ffg: #f0ebea;
                  --base0F-bg: #825449;
                  --base0F-bbg: #513733;
                  --base0F: #a3685a;
                  --base0E-fg: #d5c8da;
                  --base0E-ffg: #f2eff3;
                  --base0E-bg: #8e7796;
                  --base0E-bbg: #594b5e;
                  --base0E: #b294bb;
                  --base0D-fg: #c1cedb;
                  --base0D-ffg: #edf0f4;
                  --base0D-bg: #678298;
                  --base0D-bbg: #42515f;
                  --base0D: #81a2be;
                  --base0C-fg: #c4dbd8;
                  --base0C-ffg: #edf4f3;
                  --base0C-bg: #6e9893;
                  --base0C-bbg: #465f5c;
                  --base0C: #8abeb7;
                  --base0B-fg: #bfd0ac;
                  --base0B-ffg: #ecf1e8;
                  --base0B-bg: #648627;
                  --base0B-bbg: #405423;
                  --base0B: #7da72a;
                  --base0A-fg: #f6dfbc;
                  --base0A-ffg: #fcf5ec;
                  --base0A-bg: #c09f5e;
                  --base0A-bbg: #77633d;
                  --base0A: #f0c674;
                  --base09-fg: #ecc8b6;
                  --base09-ffg: #f9eeea;
                  --base09-bg: #b2764d;
                  --base09-bbg: #6e4a35;
                  --base09: #de935f;
                  --base08-fg: #e2b8b8;
                  --base08-ffg: #f6eaea;
                  --base08-bg: #a35253;
                  --base08-bbg: #653637;
                  --base08: #cc6666;
                  --base07: #ffffff;
                  --base06: #e0e0e0;
                  --base05: #c5c8c6;
                  --base04: #b4b7b4;
                  --base03: #969896;
                  --base02: #373b41;
                  --base01: #282a2e;
                  --base00: #1d1f21;
                  --accent7-fg: #ceb9b5;
                  --accent7-ffg: #f0ebea;
                  --accent7-bg: #825449;
                  --accent7-bbg: #513733;
                  --accent7: #a3685a;
                  --accent6-fg: #d5c8da;
                  --accent6-ffg: #f2eff3;
                  --accent6-bg: #8e7796;
                  --accent6-bbg: #594b5e;
                  --accent6: #b294bb;
                  --accent5-fg: #c1cedb;
                  --accent5-ffg: #edf0f4;
                  --accent5-bg: #678298;
                  --accent5-bbg: #42515f;
                  --accent5: #81a2be;
                  --accent4-fg: #c4dbd8;
                  --accent4-ffg: #edf4f3;
                  --accent4-bg: #6e9893;
                  --accent4-bbg: #465f5c;
                  --accent4: #8abeb7;
                  --accent3-fg: #bfd0ac;
                  --accent3-ffg: #ecf1e8;
                  --accent3-bg: #648627;
                  --accent3-bbg: #405423;
                  --accent3: #7da72a;
                  --accent2-fg: #f6dfbc;
                  --accent2-ffg: #fcf5ec;
                  --accent2-bg: #c09f5e;
                  --accent2-bbg: #77633d;
                  --accent2: #f0c674;
                  --accent1-fg: #ecc8b6;
                  --accent1-ffg: #f9eeea;
                  --accent1-bg: #b2764d;
                  --accent1-bbg: #6e4a35;
                  --accent1: #de935f;
                  --accent0-fg: #e2b8b8;
                  --accent0-ffg: #f6eaea;
                  --accent0-bg: #a35253;
                  --accent0-bbg: #653637;
                  --accent0: #cc6666;
              }

      :root.light {
                  --shade7: #1d1f21;
                  --shade6: #282a2e;
                  --shade5: #4d4d4c;
                  --shade4: #969896;
                  --shade3: #8e908c;
                  --shade2: #d6d6d6;
                  --shade1: #e0e0e0;
                  --shade0: #ffffff;
                  --base0F-fg: #825449;
                  --base0F-ffg: #513733;
                  --base0F-bg: #ceb9b5;
                  --base0F-bbg: #f0ebea;
                  --base0F: #a3685a;
                  --base0E-fg: #6e4887;
                  --base0E-ffg: #453155;
                  --base0E-bg: #c4b5d1;
                  --base0E-bbg: #edeaf1;
                  --base0E: #8959a8;
                  --base0D-fg: #257eb3;
                  --base0D-ffg: #204f6f;
                  --base0D-bg: #acccec;
                  --base0D-bbg: #e8f0f9;
                  --base0D: #2a9ddf;
                  --base0C-fg: #347b80;
                  --base0C-ffg: #264d51;
                  --base0C-bg: #afcacd;
                  --base0C-bbg: #e8eff0;
                  --base0C: #3e999f;
                  --base0B-fg: #6a931c;
                  --base0B-ffg: #435c20;
                  --base0B-bg: #c2d8ab;
                  --base0B-bbg: #edf3e7;
                  --base0B: #84b819;
                  --base0A-fg: #bb9212;
                  --base0A-ffg: #745b1d;
                  --base0A-bg: #f3d8aa;
                  --base0A-bbg: #fbf3e7;
                  --base0A: #eab700;
                  --base09-fg: #c46c20;
                  --base09-ffg: #794521;
                  --base09-bg: #f9c3ab;
                  --base09-bbg: #fdede7;
                  --base09: #f5871f;
                  --base08-fg: #a02526;
                  --base08-ffg: #632123;
                  --base08-bg: #e0acac;
                  --base08-bbg: #f5e8e8;
                  --base08: #c82829;
                  --base07: #1d1f21;
                  --base06: #282a2e;
                  --base05: #4d4d4c;
                  --base04: #969896;
                  --base03: #8e908c;
                  --base02: #d6d6d6;
                  --base01: #e0e0e0;
                  --base00: #ffffff;
                  --accent7-fg: #825449;
                  --accent7-ffg: #513733;
                  --accent7-bg: #ceb9b5;
                  --accent7-bbg: #f0ebea;
                  --accent7: #a3685a;
                  --accent6-fg: #6e4887;
                  --accent6-ffg: #453155;
                  --accent6-bg: #c4b5d1;
                  --accent6-bbg: #edeaf1;
                  --accent6: #8959a8;
                  --accent5-fg: #257eb3;
                  --accent5-ffg: #204f6f;
                  --accent5-bg: #acccec;
                  --accent5-bbg: #e8f0f9;
                  --accent5: #2a9ddf;
                  --accent4-fg: #347b80;
                  --accent4-ffg: #264d51;
                  --accent4-bg: #afcacd;
                  --accent4-bbg: #e8eff0;
                  --accent4: #3e999f;
                  --accent3-fg: #6a931c;
                  --accent3-ffg: #435c20;
                  --accent3-bg: #c2d8ab;
                  --accent3-bbg: #edf3e7;
                  --accent3: #84b819;
                  --accent2-fg: #bb9212;
                  --accent2-ffg: #745b1d;
                  --accent2-bg: #f3d8aa;
                  --accent2-bbg: #fbf3e7;
                  --accent2: #eab700;
                  --accent1-fg: #c46c20;
                  --accent1-ffg: #794521;
                  --accent1-bg: #f9c3ab;
                  --accent1-bbg: #fdede7;
                  --accent1: #f5871f;
                  --accent0-fg: #a02526;
                  --accent0-ffg: #632123;
                  --accent0-bg: #e0acac;
                  --accent0-bbg: #f5e8e8;
                  --accent0: #c82829;
              }
    }
    :root {
          }
  </style>

  <link rel="stylesheet" href="../support/vendor/reveal/dist/reset.css">
  <link rel="stylesheet" href="../support/vendor/reveal/dist/reveal.css">
  <link rel="stylesheet" href="../support/components/components.css">
  <link rel="stylesheet" href="../support/plugins/decker/ui-anchors.css">
  <link rel="stylesheet" href="../support/plugins/whiteboard/whiteboard.css">
  <link rel="stylesheet" href="../support/plugins/menu/menu.css">
  <link rel="stylesheet" href="../support/plugins/feedback/feedback.css">
  <link rel="stylesheet" href="../support/plugins/explain/explain.css">
    <link rel="stylesheet" href="../support/plugins/live-captioning/live-captioning.css">
    <link rel="stylesheet" href="../support/vendor/videojs/video-js.min.css">
  <link rel="stylesheet" href="../support/vendor/css/xcode.css">
  <link rel="stylesheet" href="../support/flyingFocus/flying-focus.css">
  <link rel="stylesheet" href="../support/plugins/quiz-wue/quiz-wue.css">
  <link rel="stylesheet" href="../support/css/deck.css">
  <link rel="stylesheet" href="../support/css/msms-deck.css">

</head>

<body >
  <div class="reveal">
    <div class="slides">

      <section id="title-slide">

         <div class="background-on-accent">
                     <h1>Deep Reinforcement Learning</h1>
                              <h2>4 - Dynamic Programming</h2>
                  </div>



                  <div class="author"> Prof. Dr. Malte Schilling </div>

                  <div class="affiliation"> Autonomous Intelligent
Systems Group </div>


         <img class="logo affiliation-logo light-only" src="./../support/assets/ms-logo-light.svg">
         <img class="logo affiliation-logo dark-only" src="./../support/assets/ms-logo-dark.svg">

               </section>


<section id="recap-markov-decision-process" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Markov Decision Process</h1>
<div class="layout">
<div class="area">
<div id="mdp-definition-sutton2018" class="definition box block">
<h2 class="definition">MDP Definition <span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></h2>
<p>A Markov Decision Process is a tuple <span class="math inline">\((\mathcal{S}, \mathcal{A}, p, \gamma)\)</span>, consisting of</p>
<ul>
<li>a set of states <span class="math inline">\(\mathcal{S}\)</span></li>
<li>a set of actions <span class="math inline">\(\mathcal{A}\)</span></li>
<li>a joint probability <span class="math inline">\(p(r, s&#39; | s, a)\)</span> describing the dynamics of the environment as
<ul>
<li>transition probabilities for switching states <span class="math display">\[p(s&#39; | s,a)= \sum_r p(r, s&#39; | s, a)\]</span></li>
<li>and expected reward <span class="math inline">\(\mathbb{E}(R | s,a)= \sum_r r \sum_{s&#39;} p(r, s&#39; | s, a)\)</span></li>
</ul></li>
<li>the discount factor <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-approaches-to-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Approaches to Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The reaction of the environment to certain actions can be represented by a <strong>model</strong> which the agent may or may not know.</p>
<p>The model defines the reward function and transition probabilities.</p>
<div class="media">
<figure class="image" style="height:auto;width:640px;">
<img src="../data/03/weng_RL_algorithm_categorization.png" style="height:auto;width:100%;" alt="../data/03/weng_RL_algorithm_categorization.png" />
</figure>
</div>
<ul>
<li>Model-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Use planning on learned or given model.</li>
<li>Model-free: No dependency on the model during learning. Learning with imperfect information.</li>
</ul>
</div>
<div id="section" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="distinction-of-problems-in-reinforcement-learning" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Distinction of problems in Reinforcement Learning</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 70fr 30fr;">
<div class="left">
<h2 id="prediction">Prediction</h2>
<p>Estimating value functions for a given policy: this is called policy evaluation.</p>
<ul>
<li>Given a policy, what is my expected return?</li>
<li>Using a strategy, what is my expected return?</li>
</ul>
<h2 id="control">Control</h2>
<p>On the other hand, exploiting a value function (ideally an optimal one) is used for optimisation of a policy.</p>
<ul>
<li>What is the optimal way of behaving?</li>
<li>What is the optimal control policy to maximise a reward or minimise time, fuel consumption, etc.?</li>
</ul>
</div>
<div class="center">
<p><div style="display:block; clear:both; height:100px;"></div></p>
<div class="media">
<figure class="image" style="height:auto;width:420px;vertical-align=middle;">
<img src="../data/04/rl_problems.png" style="height:auto;width:100%;" alt="../data/04/rl_problems.png" />
</figure>
</div>
<h2 class="footer" id="section-1"></h2>
<p><span class="citation">(<a href="#ref-2010Szepesvari" role="doc-biblioref">Szepesvári 2010</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-bellman-expectation-for-q_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Bellman Expectation for <span class="math inline">\(Q_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:640px;">
<img src="../decker/code/code-ea9fe2e0.tex.svg" style="height:auto;width:100%;" alt="code-ea9fe2e0.tex.svg" />
</figure>
</div>
</div>
<div id="action-value-function-using-bellman-expectation" class="box block">
<h2>Action-Value Function using Bellman Expectation</h2>
<p><span class="math display">\[
q_{\pi}(s,a) = \sum_{r}\sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a)
\Big(r + \gamma \sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a)
\sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-bellman-expectation-for-v_pi" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Bellman Expectation for <span class="math inline">\(V_{\pi}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:640px;">
<img src="../decker/code/code-71fac017.tex.svg" style="height:auto;width:100%;" alt="code-71fac017.tex.svg" />
</figure>
</div>
</div>
<div id="recursive-formulation-for-state-value-function" class="box block">
<h2>Recursive Formulation for State-Value Function</h2>
<p><span class="math display">\[
v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi (a|s) \sum_{r} \sum_{s&#39; \in \mathcal{S}} p (r, s&#39;|s,a) \Big(r + \gamma v_{\pi}(s&#39;) \Big)
\]</span></p>
<p>With reward independent of <span class="math inline">\(s&#39;\)</span>: <span class="math display">\[ v_{\pi}(s)= \sum_{a \in \mathcal{A}} \pi (a|s) \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="overview-lecture" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Overview Lecture</h1>
<div class="layout">
<div class="area">
<div id="until-now" class="box block">
<h2>Until now:</h2>
<ul>
<li>Assumed a policy as given</li>
<li>Computing of a Value Function</li>
</ul>
</div>
<div id="now-find" class="box block fragment">
<h2>Now find</h2>
<ul>
<li>Optimal Policies and Value Function</li>
<li>through Dynamic Programming when MDP is fully known (transition and reward policies)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-policies" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Optimal Policies</h1>
</div>
</div>
</section>
<section id="optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>There is a partial ordering over policies which means: <span class="math display">\[
\pi \geq \pi&#39; \text{ if } v_{\pi}(s) \geq v_{\pi&#39;}(s), \forall s
\]</span></p>
</div>
<div id="for-any-markov-decision-process" class="definition box block">
<h2 class="definition">For any Markov Decision process …</h2>
<ul>
<li>There is an optimal policy <span class="math inline">\(\pi_*\)</span> which is better (or equal) than all other policies.</li>
<li>Every optimal policy achieves the optimal value function and the optimal action-value function.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-example-calculation-of-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap Example – Calculation of Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<h2 id="discounting-future-returns">Discounting Future Returns</h2>
<p>For <span class="math inline">\(\gamma = 0.9\)</span>:</p>
<p><span class="math display">\[
\begin{eqnarray*}
  v_{\pi_1}(X) &amp;=&amp; {\sum_{k=0}^{\infty} 0.9^{2k} = \frac{1}{1-0.9^2} \approx 5.3 }\\ \\
  v_{\pi_2}(X)
  &amp;=&amp; {\sum_{k=0}^{\infty} (0.9)^{2k+1}*2 = \frac{0.9}{1-0.9^2} * 2 \approx 9.5}
\end{eqnarray*}
\]</span></p>
<p>with <span class="math inline">\(\pi_1\)</span> always selecting <span class="math inline">\(A_1\)</span> and <span class="math inline">\(\pi_2\)</span> always selecting <span class="math inline">\(A_2\)</span>.</p>
<h2 class="fragment" id="section-2"></h2>
<p><span class="math display">\[
\pi_2 \geq \pi_1\]</span></p>
</div>
<div id="section-3" class="right col40">
<h2 class="right"></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-ecd89391.tex.svg" style="height:480px;width:auto;" alt="code-ecd89391.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="optimal-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The optimal state-value function <span class="math inline">\(v_*(s)\)</span> is the maximum value function over all policies:</p>
<p><span class="math display">\[
v_*(s) = \max_{\pi} v_{\pi}(s)
\]</span></p>
<p>The optimal action-value function <span class="math inline">\(q_*(s,a)\)</span> is the maximum action-value function over all policies <span class="math display">\[
q_*(s,a) = \max_{\pi} q_{\pi}(s,a)
\]</span></p>
<ul>
<li>The optimal value function specifies the best possible performance in the MDP.</li>
<li>An MDP is “solved” when we know the optimal value function.</li>
</ul>
</div>
<div id="section-4" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<div class="media">
<figure class="image" style="height:auto;width:200px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>For an undiscounted MDP (<span class="math inline">\(\gamma = 1\)</span>),</p>
<ul>
<li>What is the optimal value for each state?</li>
<li>How do you compute the optimal value?</li>
</ul>
</div>
<div class="col70">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/silver_student_example.png" style="height:540px;width:auto;" alt="../data/04/silver_student_example.png" />
</figure>
</div>
</div>
</div>
<div id="section-5" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-value-function-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalVexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalVexample.svg" />
</figure>
</div>
</div>
<div id="section-6" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-action-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Action-Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalQexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalQexample.svg" />
</figure>
</div>
</div>
<div id="section-7" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="finding-an-optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Finding an Optimal Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can find an optimal policy by directly acting greedy on the optimal action-state value function <span class="math inline">\(q_*(s,a)\)</span>:</p>
<p><span class="math display">\[
\pi_*(s,a) =
  \begin{cases}
      1, &amp; \text{if}\ a=\arg\max\limits_{a \in \mathcal{A}} q_* (s,a) \\
      0, &amp; \text{otherwise}
    \end{cases}
\]</span></p>
</div>
<div id="note" class="box block">
<h2>Note:</h2>
<ul>
<li>There is always a deterministic optimal policy for any MDP: <span class="math inline">\(\pi_* \geq \pi, \forall \pi\)</span></li>
<li>There can be multiple optimal policies.</li>
<li>If multiple actions maximize <span class="math inline">\(q_*\)</span> in a state, we can simply pick any of these (including stochastically)</li>
</ul>
</div>
<div id="section-8" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Optimal Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/03/silver_optimalPolicyexample.svg" style="height:540px;width:auto;" alt="../data/03/silver_optimalPolicyexample.svg" />
</figure>
</div>
</div>
<div id="section-9" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="recap-summary-bellman-expectation-equations" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Recap – Summary Bellman Expectation Equations</h1>
<div class="layout">
<div class="area">
<div id="bellman-expectation-equation" class="definition box block">
<h2 class="definition">Bellman Expectation Equation</h2>
<p>For a given MDP <span class="math inline">\(\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, p, r, \gamma \rangle\)</span> for any policy <span class="math inline">\(\pi\)</span>, the value functions obey these expectation equations:</p>
<p><span class="math display">\[ v_{\pi}(s)= \sum_{a \in \mathcal{A}} \pi (a|s) \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;) \Big)
\]</span></p>
<p><span class="math display">\[
q_{\pi}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a)
\sum_{a&#39; \in \mathcal{A}} \pi(a&#39; | s&#39;) q_{\pi}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-optimality-equation-for-v_" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Optimality Equation for <span class="math inline">\(V_{*}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-89bc691f.tex.svg" style="height:auto;width:100%;" alt="code-89bc691f.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
v_{*}(s) = \max_{a \in \mathcal{A}} q_{*}(s,a)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-optimality-equation-for-v_-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Optimality Equation for <span class="math inline">\(V_{*}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-bbdd3e74.tex.svg" style="height:auto;width:100%;" alt="code-bbdd3e74.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
v_{*}(s) = \max_{a \in \mathcal{A}} \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;) \Big)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{*}\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-f3554bbb.tex.svg" style="height:auto;width:100%;" alt="code-f3554bbb.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
q_{*}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="bellman-expectation-for-q_-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Bellman Expectation for <span class="math inline">\(Q_{*}\)</span> (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-ca812591.tex.svg" style="height:auto;width:100%;" alt="code-ca812591.tex.svg" />
</figure>
</div>
<p><span class="math display">\[
q_{*}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a)
\max_{a&#39; \in \mathcal{A}} q_{*}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="solving-the-bellman-optimality-equation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Solving the Bellman Optimality Equation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li><p>The Bellman optimality equation is non-linear</p></li>
<li><p>Multiple iterative solution methods:</p>
<ul>
<li>Using models – dynamic programming
<ul>
<li>Value iteration</li>
<li>Policy iteration</li>
</ul></li>
<li>Using samples
<ul>
<li>Monte Carlo Approaches</li>
<li>Q-learning</li>
<li>SARSA</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="section-10" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-deepmind2021" role="doc-biblioref">Hasselt und Borsa 2021</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="summary-optimal-value-function-and-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Summary – Optimal Value Function and Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>The goal in RL is to act optimally – this is possible through learning an optimal value function or directly an optimal policy.</p>
<p>The optimal value function produces the maximum return:</p>
<p><span class="math display">\[ v_{*}(s) = \max_{\pi} v_{\pi}(s),
q_{*}(s, a) = \max_{\pi} q_{\pi}(s, a)\]</span></p>
<p>The optimal policy achieves optimal value functions:</p>
<p><span class="math display">\[ \pi_{*} = \arg\max_{\pi} v_{\pi}(s),
\pi_{*} = \arg\max_{\pi} q_{\pi}(s, a)\]</span></p>
<p>These are directly related as <span class="math display">\[ v_{\pi_{*}}(s)=v_{*}(s), q_{\pi_{*}}(s, a) = q_{*}(s, a)\]</span></p>
</div>
<div id="section-11" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
<!--# Summary -- Bellman Optimality Equations

We can use these equations directly to compute optimal values for following an optimal policy.

\begin{align*}
v_*(s) &= \max_{a \in \mathcal{A}} q_*(s,a)\\
q_*(s, a) &= R_s^a + \gamma \sum_{s' \in \mathcal{S}} p(s' | s,a) v_*(s') \\
v_*(s) &= \max_{a \in \mathcal{A}} \big( R_s^a + \gamma \sum_{s' \in \mathcal{S}} p(s' | s,a) v_*(s') \big) \\
q_*(s, a) &= R_s^a + \gamma \sum_{s' \in \mathcal{S}} p(s' |s,a) \max_{a' \in \mathcal{A}} q_*(s', a')
\end{align*}-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dynamic-programming" class="section slide level1" data-background-color="#6A931C">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming</h1>
</div>
</div>
</section>
<section id="dynamic-programming-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>In general, dynamic programming relies on two parts:</p>
<ul>
<li>Optimal substructure: The problem can be broken down into subproblems that provide partial solutions that can be used to solve the overall problem.</li>
<li>Overlapping sub-problems: Sub-problems occur many times so that they can be cached for later reuse.</li>
</ul>
</div>
<div class="col40">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/wikicommons_fibonacci_tree.svg" aria-label="Example for Fibonacci Numbers shown as a recursive (inefficient) tree" title="Example for Fibonacci Numbers shown as a recursive (inefficient) tree" style="height:320px;width:auto;" alt="../data/04/wikicommons_fibonacci_tree.svg" />
<figcaption>
Example for Fibonacci Numbers shown as a recursive (inefficient) tree
</figcaption>
</figure>
</div>
<p><span class="math display">\[
F_n = F_{n-1} + F_{n-2}, F_0 = 0, F_1 = 1
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="history-of-the-name-dynamic-programming" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>History of the Name Dynamic Programming</h1>
<div class="layout">
<div class="area">
<div class="box block">
<h3 id="from-multistage-decision-making">From Multistage Decision Making …</h3>
<blockquote>
<p>The 1950s were not good years for mathematical research … [the] Secretary of Defense … had a pathological fear … of the word, research… His face would suffuse, he would turn red, and he would get violent if people used the term, research, in his presence. You can imagine how he felt, then, about the term, mathematical… I felt I had to … shield … Air Force from the fact that I was … doing mathematics.</p>
</blockquote>
<h3 id="to-dynamic-programming">… to Dynamic Programming</h3>
<blockquote>
<p>What title, what name, could I choose? … I was interested in planning, in decision making, in thinking … I decided therefore to use the word, `programming.’ I wanted to get across the idea that this was dynamic, … [which] has an absolutely precise meaning, … It also has a very interesting property as an adjective, and that is it’s impossible to use the word, dynamic, in a pejorative sense. … It was something not even a Congressman could object to. So I used it as an umbrella for my activities.</p>
</blockquote>
</div>
<div id="section-12" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-bellman1984eye" role="doc-biblioref">Bellman 1984</a>, p. 159)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="section-13" class="slide level1">
<div class="decker">
<div class="alignment">
<h1></h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><div style="display:block; clear:both; height:180px;"></div></p>
</div>
<div id="section-14" class="center box block">
<h2 class="center"></h2>
<p><em>The essence of reinforcement learning is memorized (context-sensitive) search.</em></p>
<p><div style="display:block; clear:both; height:150px;"></div></p>
</div>
<div id="section-15" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dynamic-programming-for-mdps" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Dynamic Programming for MDPs</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>In general, dynamic programming relies on two parts:</p>
<ul>
<li>Optimal substructure: The problem can be broken down into subproblems.</li>
<li>Overlapping sub-problems: Sub-problems occur many times.</li>
</ul>
<p>Dynamic programming algorithms allow to deal with planning problems: This means, the <strong>complete model and environment is known (the MDP)</strong>.</p>
</div>
<div id="markov-decision-processes-satisfy-both-properties" class="box block fragment">
<h2>Markov Decision Processes satisfy both properties</h2>
<ul>
<li>The Bellman Equation gives recursive decomposition: we have the optimal behaviour of the next step and then the estimated optimal value of the remaining steps.</li>
<li>The Value function stores solutions for reuses: It caches the optimal achievable reward from a state.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="principle-of-optimality-for-a-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Principle of Optimality for a Policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>As we are searching an optimal policy, this optimal policy can be divided into two parts:</p>
<ul>
<li>An optimal immediate action <span class="math inline">\(a\)</span>,</li>
<li>and following the optimal policy afterwards from new state <span class="math inline">\(s&#39;\)</span>.</li>
</ul>
</div>
<div id="principle-of-optimality" class="theorem box block">
<h2 class="theorem">Principle of Optimality</h2>
<p>A policy <span class="math inline">\(\pi(a | s)\)</span> achieves the optimal value from a state <span class="math inline">\(s\)</span> as <span class="math inline">\(v_\pi (s) = v_* (s)\)</span>, iff:</p>
<ul>
<li><span class="math inline">\(\forall s&#39;\)</span> reachable from <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;, v_\pi (s&#39;) = v_* (s&#39;)\)</span></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-and-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation and Control</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-4c4cf418.tex.svg" style="height:480px;width:auto;" alt="code-4c4cf418.tex.svg" />
</figure>
</div>
<p>DP uses the various Bellman equations along with <strong>full</strong> knowledge of the joint probability to work out value functions and optimal policies.</p>
<p>Classical DP does not involve interaction with the environment at all.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="summary-bellman-optimality-equations" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Summary Bellman Optimality Equations</h1>
<div class="layout">
<div class="area">
<div id="bellman-optimality-equation" class="definition box block">
<h2 class="definition">Bellman Optimality Equation</h2>
<p>For a given MDP <span class="math inline">\(\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, p, r, \gamma \rangle\)</span> the optimal value functions obey these expectation equations:</p>
<p><span class="math display">\[ v_* (s)= \max_{a \in \mathcal{A}} \Big( R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{*}(s&#39;) \Big)
\]</span></p>
<p><span class="math display">\[
q_{*}(s,a) = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a)
\max_{a&#39; \in \mathcal{A}} q_{*}(s&#39;, a&#39;)
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="dp-policy-evaluation" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DP: Policy Evaluation</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>Policy Evaluation is to compute the state-value <span class="math inline">\(v_{\pi}\)</span> for a given policy <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
\begin{eqnarray*}
v_{k+1}(s)
&amp;=&amp; \mathbb{E}_\pi [r + \gamma v_k(s&#39;) | S= s] \\
&amp;=&amp; \sum_a \pi(a \vert s) \sum_{s&#39;, r} p(s&#39;, r \vert s, a) (r + \gamma v_k(s&#39;))
\end{eqnarray*}
\]</span></p>
<p>It iteratively applies the Bellman expectation backup and converges.</p>
</div>
<div class="col40">
<div class="media">
<figure class="image" style="height:auto;width:480px;">
<img src="../data/03/silver_policyEvaluation.svg" style="height:auto;width:100%;" alt="../data/03/silver_policyEvaluation.svg" />
</figure>
</div>
</div>
</div>
<div id="section-16" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-weng2018rl" role="doc-biblioref">Weng 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-iterative-convergence-of-value-function" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation: Iterative Convergence of Value Function</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Given is a policy <span class="math inline">\(\pi\)</span> for an MDP and we want to find the corresponding value function.</p>
<p>Approach: Iteratively apply Bellman expectation: <span class="math inline">\(v_1 \rightarrow v_2 \rightarrow \dots \rightarrow v_{\pi}\)</span></p>
</div>
<div id="section-17" class="box block fragment">
<h2></h2>
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-4f26c330.tex.svg" style="height:360px;width:auto;" alt="code-4f26c330.tex.svg" />
</figure>
</div>
<!--# Optimal Solution for the Gridworld Example


![](../data/02/sutton_3_5_gridworld.svg){width=1000px}

Each location is a state. Discount factor is $0.9$.

Actions: North, West, South, East

Reward: $-1$ when trying to move out of the grid, $0$ otherwise

For state $A$ and $B$: all actions lead to $A'$ and a reward of $+10$ (respectively $B', +5$).

## {.footer}

[@sutton2018] -->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="grid-world-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Grid world example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col30">
<p>Given a grid word (MDP)</p>
<ul>
<li><span class="math inline">\(\mathcal{S}: 16\)</span> states as places, two terminal states</li>
<li><span class="math inline">\(\mathcal{A}: 4\)</span> actions into main directions</li>
<li>R: <span class="math inline">\(-1\)</span> for each step</li>
<li><span class="math inline">\(\gamma = 1\)</span> (undiscounted)</li>
</ul>
</div>
<div class="col70">
<div class="media">
<figure class="render image rendered" style="height:auto;width:800px;">
<img src="../decker/code/code-b153d627.tex.svg" style="height:auto;width:100%;" alt="code-b153d627.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section-18" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-0" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = 0\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-00a9e80e.tex.svg" style="height:auto;width:100%;" alt="code-00a9e80e.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-1" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = 1\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-00653d55.tex.svg" style="height:auto;width:100%;" alt="code-00653d55.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = 2\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-d4a562d5.tex.svg" style="height:auto;width:100%;" alt="code-d4a562d5.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-3" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = 3\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-c931bfd8.tex.svg" style="height:auto;width:100%;" alt="code-c931bfd8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-10" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = 10\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-da845315.tex.svg" style="height:auto;width:100%;" alt="code-da845315.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-evaluation-k-infty" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Evaluation <span class="math inline">\(k = \infty\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1000px;">
<img src="../decker/code/code-a857476f.tex.svg" style="height:auto;width:100%;" alt="code-a857476f.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-improvement" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Improvement</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can use <span class="math inline">\(v_\pi\)</span> to change <span class="math inline">\(\pi\)</span>. We are interested if for some state <span class="math inline">\(s\)</span> we can pick a better action <span class="math inline">\(a\)</span> then proposed by <span class="math inline">\(\pi\)</span>.</p>
<p><span class="math inline">\(v_\pi(s)\)</span> tells us how valuable it is to follow <span class="math inline">\(\pi\)</span> from this current state. Selecting a different action <span class="math inline">\(a\)</span> from <span class="math inline">\(s\)</span> and only afterwards following <span class="math inline">\(\pi\)</span> is given as</p>
<p><span class="math display">\[
  q_\pi(s,a) := \mathbb E \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t =     s, A_t = a \right] = \sum_{s&#39;,r} p(s&#39;,r | s,a) \left[ r + \gamma v_\pi(s&#39;)\right].
\]</span></p>
<p>If we find a better action, we can improve on our current policy <span class="math inline">\(\pi\)</span> when encountering state <span class="math inline">\(s\)</span> and define this as an improved policy <span class="math inline">\(\pi&#39; \geq \pi\)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-policy-improvement" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of Policy Improvement</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>Consider we start with a deterministic policy <span class="math inline">\(a = \pi(s)\)</span>.</p>
</div>
<div id="section-19" class="box block fragment">
<h2></h2>
<p>We can improve this policy by acting greedy on the action-value function</p>
<p><span class="math display">\[\pi&#39; (s) = \arg\max_{a \in \mathcal{A}} q_{\pi}(s,a)\]</span></p>
</div>
<div id="section-20" class="box block fragment">
<h2></h2>
<p>which improves the value from any state <span class="math inline">\(s\)</span> over one step</p>
<p><span class="math display">\[q_{\pi}(s, \pi&#39;(s)) = \max_{a \in \mathcal{A}} q_{\pi}(s,a) \geq q_{\pi}(s,\pi(s))= v_{\pi}(s)\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-policy-improvement-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of Policy Improvement (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><span class="math display">\[q_{\pi}(s, \pi&#39;(s)) = \max_{a \in \mathcal{A}} q_{\pi}(s,a) \geq q_{\pi}(s,\pi(s))= v_{\pi}(s)\]</span></p>
<p>improves the value function as well, <span class="math inline">\(v_{\pi&#39;} \geq v_{\pi}\)</span>: <span class="math display">\[
\begin{eqnarray*}
v_{\pi}(s) &amp;\leq&amp; q_{\pi}(s, \pi&#39;(s)) = \mathbb E_{\pi&#39;} \left[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t= s\right]\\
&amp;\leq&amp; \fragment{\mathbb E_{\pi&#39;} \left[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi&#39;(S_{t+1})) | S_t= s\right] }\\
&amp;\leq&amp; \fragment{\mathbb E_{\pi&#39;} \left[R_{t+1} + \gamma R_{t+2} + \gamma^2q_{\pi}(S_{t+2}, \pi&#39;(S_{t+2})) | S_t= s\right] }\\
&amp;\leq&amp; \fragment{\mathbb E_{\pi&#39;} \left[R_{t+1} + \gamma R_{t+2} + \dots | S_t= s\right]}\fragment{ = v_{\pi&#39;}(s)}\\
\end{eqnarray*}
\]</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-improvement-towards-optimal-policy" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Improvement towards optimal policy</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>If a new greedy policy <span class="math inline">\(\pi&#39;\)</span> is as good as, but not better than, the old policy <span class="math inline">\(\pi\)</span>. Then <span class="math inline">\(v_\pi = v_{\pi&#39;}\)</span> (and if we choose <span class="math inline">\(\pi&#39;\)</span> acting greedy on <span class="math inline">\(v_\pi\)</span>): <span class="math inline">\(v_{\pi&#39;} = v_{*}\)</span> .</p>
<p>So, if the improvment of our policy stops:</p>
<p><span class="math display">\[q_{\pi}(s, \pi&#39;(s))\fragment{= \max_{a \in \mathcal{A}} q_{\pi}(s,a) = q_{\pi}(s,\pi(s))} \fragment{= v_{\pi}(s),}\]</span></p>
</div>
<div id="section-21" class="box block fragment">
<h2></h2>
<p>which directly satisfies the Bellman equation <span class="math display">\[v_\pi(s) = \max_{a \in \mathcal{A}}q_\pi(s,a).\]</span></p>
<!--For all $s \in \mathcal{S}$ we already take an optimal action:
$$
\begin{eqnarray*}
  v_{\pi'}(s) &=& \arg\max_a \mathbb E \left[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t= s | A_t = a \right] \\
  &=& \arg\max_a \sum_{s',r} p(s',r|s,a)\left[r + \gamma v_{\pi'}(s)\right].
\end{eqnarray*}
$$

This is the Bellman optimality equation -- and, therefore, $v_{\pi'}$ must be $v_*$.-->
<p><strong>Policy improvement therefore must give a strictly better policy except when the original policy is already optimal.</strong></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-improvement-theorem" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Improvement Theorem</h1>
<div class="layout">
<div class="area">
<div id="policy-improvement-theorem-1" class="definition box block">
<h2 class="definition">Policy Improvement Theorem</h2>
<p>For any deterministic policies <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\pi&#39;\)</span> with for all <span class="math inline">\(s \in \mathcal S\)</span>, <span class="math display">\[\begin{equation*}
  \label{eq: policyimprovementthm}
  q_\pi(s, \pi&#39;(s)) \geq v_\pi(s)
\end{equation*}\]</span> it holds that policy <span class="math inline">\(\pi&#39;\)</span> is as good as or better than <span class="math inline">\(\pi\)</span>; i.e. it must obtain greater than or equal expected return for all states <span class="math inline">\(s \in \mathcal S\)</span>: <span class="math inline">\(v_{\pi&#39;}(s) \geq v_\pi(s)\)</span>.</p>
</div>
<div id="finding-an-optimal-policy-1" class="box block">
<h2>Finding an optimal policy</h2>
<p>Policy improvement converges towards optimal policies.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-iteration-control" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Iteration (Control)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:1200px;">
<img src="../decker/code/code-2c8f8dd4.tex.svg" style="height:auto;width:100%;" alt="code-2c8f8dd4.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation:</strong><span class="math inline">\(\overset{\textrm{E}}{\longrightarrow}\)</span></p>
<p><strong>Policy improvement</strong> <span class="math inline">\(\overset{\textrm{I}}{\longrightarrow}\)</span></p>
<p>For deterministic policies: each policy is guaranteed to be strictly better until we reach the optimal policy.</p>
<p>For finite MDP: <span class="math inline">\(\exists\)</span> only a finite number of deterministic policies; therefore this converges to an optimal policy and an optimal value function in a finite number of iterations.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="stochastic-optimal-policies" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Stochastic optimal policies</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>For a policy <span class="math inline">\(\pi\)</span> action selection can be defined in a probabilistic way through <span class="math inline">\(\pi(a|s)\)</span> for taking each action <span class="math inline">\(a\)</span> in each state <span class="math inline">\(s\)</span>.</p>
<p>When there are ties in the policy improvement steps – there are several actions with maximum value – then we can select any of these maximal actions or can distribute the probability among these in any way (as long as all submaximal actions are given zero probability.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-iteration-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Iteration Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-ad3f3b85.tex.svg" style="height:540px;width:auto;" alt="code-ad3f3b85.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<div class="media">
<figure class="render image rendered" style="height:auto;width:700px;">
<img src="../decker/code/code-440540e9.tex.svg" style="height:auto;width:100%;" alt="code-440540e9.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation</strong> = Estimate <span class="math inline">\(v_{\pi}\)</span></p>
<p><strong>Policy improvement</strong> = Generate new <span class="math inline">\(\pi&#39; \geq \pi\)</span></p>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-c885d170.tex.svg" style="height:auto;width:100%;" alt="code-c885d170.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="policy-improvement-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Policy Improvement Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col40">
<div class="media">
<figure class="image" style="height:auto;width:240px;">
<img src="../data/Discussion.png" style="height:auto;width:100%;" alt="../data/Discussion.png" />
</figure>
</div>
<p>You can step <strong>policy evaluation</strong> and <strong>policy improvement</strong> in this interactive grid environment.</p>
<ul>
<li>What is worst case complexity for finding an optimal policy (<span class="math inline">\(n\)</span> states, <span class="math inline">\(m\)</span> actions)?</li>
<li>Observe convergence towards the optimal policy using a form of policy improvement!</li>
<li>How do you proceed?</li>
</ul>
<h2 class="footer" id="section-22"></h2>
<p><span class="citation">(<a href="#ref-karpathy_mdp" role="doc-biblioref">Karpathy 2015</a>)</span></p>
</div>
<div class="col60">
<div class="media">
<figure class="iframe" style="width:100%;height:auto;">
<iframe style="width:100%;height:700px;" allow="fullscreen" data-src="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="efficiency-of-dynamic-programming" class="columns slide level1">
<div class="decker">
<div class="alignment">
<h1>Efficiency of Dynamic Programming</h1>
<div class="layout row columns">
<div class="area ">
<div id="section-23" class="top box block">
<h2 class="top"></h2>
<p>MDP with <span class="math inline">\(n\)</span> the number of states and <span class="math inline">\(k\)</span> for action.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area left">
<div id="worst-case-scenarios" class="left box block">
<h2 class="left">Worst Case Scenarios</h2>
<p>Considering Fibonacci Numbers: <span class="math inline">\(2^n\)</span> steps when not caching solutions (without DP).</p>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/wikicommons_fibonacci_tree.svg" style="height:180px;width:auto;" alt="../data/04/wikicommons_fibonacci_tree.svg" />
</figure>
</div>
<p>For <strong>MDP</strong>: Number of possible deterministic policies is in <span class="math inline">\(k^n\)</span>.</p>
</div>
</div><div class="area right">
<div id="dynamic-programming-2" class="right box block">
<h2 class="right">Dynamic Programming</h2>
<p>Exploiting structure and reusing solutions leads to <span class="math inline">\(O(n)\)</span> for Fibonacci numbers.</p>
<p>For MDP: worst case work is <strong>only</strong> polynomial in the number of states and actions.</p>
<p>MDPs of growing state space are considered difficult and problematic – but this is inherent and mostly not stemming from using DP.</p>
</div>
</div>
</div>
<div class="layout row columns">
<div class="area ">
<div id="required-space" class="bottom box block">
<h2 class="bottom">Required Space</h2>
<p>For high dimensional state spaces: asynchronous methods are preferred</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="general-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>General Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>General idea of running</p>
<ul>
<li>policy evaluation and</li>
<li>policy improvement</li>
</ul>
<p>in interaction. Importantly, this could be as well done asynchronously.</p>
<p>Most RL learning methods can be described as GPI: they consist of a policy and value function.</p>
<p>When evaluation and improvement process each converge, then value function and policy are optimal – the policy is greedy wrt. the stable value function. This implies: the Bellman optimality equation holds.</p>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-c885d170.tex.svg" style="height:auto;width:100%;" alt="code-c885d170.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-iteration-example" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Iteration Example</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:auto;">
<img src="../decker/code/code-f4440bcf.tex.svg" style="height:540px;width:auto;" alt="code-f4440bcf.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="grid-layout" style="grid-template-columns: 60fr 40fr;">
<ul>
<li>State Space: For two locations, each maximum of <span class="math inline">\(20\)</span> cars</li>
<li>Actions: Move up to 5 cars overnight</li>
<li>Reward: $10 for each rented car</li>
<li>Transitions of Environment: Returns and Requests are probabilistic
<ul>
<li>following Poisson distribution, <span class="math inline">\(n\)</span> returns (same for requests) with probability <span class="math inline">\(\frac{\lambda^n}{n!}e^{-\lambda}\)</span></li>
<li>location A: average requests is 3, returns also 3</li>
<li>location B: average request is 4, returns 2</li>
</ul></li>
</ul>
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/Europcar_jhb.JPG" style="height:540px;width:auto;" alt="../data/04/Europcar_jhb.JPG" />
</figure>
</div>
</div>
</div>
<div id="section-24" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-sutton2018" role="doc-biblioref">Sutton und Barto 2018</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-0" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(0\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:1280px;">
<img src="../data/04/sb_Figure_4.2_0_car_rental.png" style="height:auto;width:100%;" alt="../data/04/sb_Figure_4.2_0_car_rental.png" />
</figure>
</div>
</div>
<div id="section-25" class="footer box block">
<h2 class="footer"></h2>
<p>Visualization from <span class="citation">(<a href="#ref-pignatelli_sb_python_2020" role="doc-biblioref"><strong>pignatelli_sb_python_2020?</strong></a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-1-and-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_1.png" aria-label="Policy after first iteration" title="Policy after first iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_1.png" />
<figcaption>
Policy after first iteration
</figcaption>
</figure>
</div>
<div class="media">
<figure class="fragment image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_2.png" aria-label="Policy after second iteration" title="Policy after second iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_2.png" />
<figcaption>
Policy after second iteration
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="jacks-car-rental-value-iteration-iteration-3-and-4" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Jack’s Car Rental – Value Iteration, iteration <span class="math inline">\(3\)</span> and <span class="math inline">\(4\)</span></h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_1.png" aria-label="Policy after third iteration" title="Policy after third iteration" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_1.png" />
<figcaption>
Policy after third iteration
</figcaption>
</figure>
</div>
<div class="media">
<figure class="fragment image" style="height:auto;width:auto;">
<img src="../data/04/sb_Figure_4.2_2.png" aria-label="fourth" title="fourth" style="height:250px;width:auto;" alt="../data/04/sb_Figure_4.2_2.png" />
<figcaption>
fourth
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="value-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Value Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="render image rendered" style="height:auto;width:640px;">
<img src="../decker/code/code-d8fe6e36.tex.svg" style="height:auto;width:100%;" alt="code-d8fe6e36.tex.svg" />
</figure>
</div>
<p><strong>Policy evaluation</strong> = Estimate <span class="math inline">\(v_{\pi}\)</span></p>
<p><strong>Policy improvement</strong> = Generate new <span class="math inline">\(\pi&#39; \geq \pi\)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>The Bellman equation for <span class="math inline">\(s \in \mathcal{S}\)</span> is given as</p>
<p><span class="math display">\[ v_{\pi}(s)= R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v_{\pi}(s&#39;)
\]</span></p>
<p>We can span a vector space <span class="math inline">\(\mathcal{V}\)</span> over all possible value functions of dimensionality <span class="math inline">\(|\mathcal{S}|\)</span> in which each point represents a specific value function.</p>
<p>The Bellman backup alters value functions in this space and we can show that it actually brings value functions between iterations closer together.</p>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-70e237e8.tex.svg" style="height:auto;width:100%;" alt="code-70e237e8.tex.svg" />
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup-2" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup (2)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p>We can measure the distance between two state-value functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> using the <span class="math inline">\(\infty\)</span>-norm (the largest difference between state values):</p>
<p><span class="math display">\[
\norm{u - v}_\infty \max_{s \in \mathcal{S}} |u(s) - v(s)|
\]</span></p>
</div>
<div id="bellman-operator" class="definition box block">
<h2 class="definition">Bellman operator</h2>
<p>The Bellman operator underlying <span class="math inline">\(\pi\)</span> is defined as a mapping (between value functions) <span class="math inline">\(T^\pi : \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{S}\)</span>:</p>
<p><span class="math display">\[
(T^\pi v)(x)  = R^a_s + \gamma \sum_{s&#39; \in \mathcal{S}} p (s&#39;|s,a) v(s&#39;)
\]</span></p>
</div>
<div id="section-26" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="considering-convergence-of-bellman-backup-3" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Considering Convergence of Bellman Backup (3)</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="col60">
<p>If <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span> then <span class="math inline">\(T^\pi\)</span> is a maximum-norm contraction with a unique fixed point as a solution to <span class="math inline">\(T^\pi v = v\)</span>.</p>
<p>The Bellman operator is a <span class="math inline">\(\gamma\)</span>-contraction – it brings value functions closer together by at least factor <span class="math inline">\(\gamma\)</span>.</p>
<h2 id="section-27"></h2>
</div>
<div class="col40">
<div class="media">
<figure class="render image rendered" style="height:auto;width:400px;">
<img src="../decker/code/code-70e237e8.tex.svg" style="height:auto;width:100%;" alt="code-70e237e8.tex.svg" />
</figure>
</div>
</div>
</div>
<div id="section-28" class="footer box block">
<h2 class="footer"></h2>
<p>For more details on convergence and derivation using contraction mapping theorem, see <span class="citation">(<a href="#ref-2010Szepesvari" role="doc-biblioref">Szepesvári 2010</a>)</span>.</p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="convergence-of-iter.-policy-evaluation-and-policy-iteration" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Convergence of Iter. Policy Evaluation and Policy Iteration</h1>
<div class="layout">
<div class="area">
<div class="box block">
<ul>
<li>The Bellman expectation operator <span class="math inline">\(T^\pi\)</span> has a unique fixed point</li>
<li>which is the value function <span class="math inline">\(v_\pi\)</span> to which it converges (through updates using the Bellman expectation equation).</li>
</ul>
<p>As a consequence * Iterative policy evaluation converges on <span class="math inline">\(v_\pi\)</span> * and over time policy iteration converges on <span class="math inline">\(v_∗\)</span></p>
</div>
<div id="section-29" class="footer box block">
<h2 class="footer"></h2>
<p><span class="citation">(<a href="#ref-silver2015" role="doc-biblioref">Silver 2015</a>)</span></p>
<!--
# Considering Convergence of Bellman Backup (4)

We can think of the Bellman expectation backup as this operator $T^\pi$,

$$
T^\pi (v) = R^\pi + \gamma P^\pi v
$$

This operator is a $\gamma$-contraction: Subsequent value functions are closer by at least $\gamma$:

$$
\begin{eqnarray*}
  \norm{T^\pi(u) - T^\pi(v)}_\infty &=& \norm{ (R_s^a + \gamma p(s' | a,s) u) - (R^a_s + \gamma p(s' | a,s) v)}_\infty \\
  &=& \fragment{\norm{\gamma P^\pi (u-v)}_\infty }\\
  & \leq & \fragment{\norm{\gamma P^\pi \norm{u-v}_\infty}_\infty }\\
  & \leq & \fragment{\gamma \norm{u-v}_\infty}
\end{eqnarray*}
$$

## {.footer}

[@silver2015]-->
</div>
</div>
</div>
</div>
</div>
</section>
<section id="drl-robot-soccer" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>DRL Robot Soccer</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div class="media">
<figure class="stream" style="width:auto;height:auto;">
<iframe style="width:auto;height:720px;aspect-ratio:16/9;" allow="fullscreen" data-src="https://www.youtube.com/embed/iX6OgG67-ZQ?cc_load_policy=0&amp;controls=1&amp;iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;showinfo=0">

</iframe>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="example-robot-playing-soccer-hierarchical-rl" class="slide level1">
<div class="decker">
<div class="alignment">
<h1>Example: Robot Playing Soccer – Hierarchical RL</h1>
<div class="layout">
<div class="area">
<div class="box block">
<p><a href="https://www.youtube.com/watch?v=iX6OgG67-ZQ">See Video of robots playing soccer</a></p>
</div>
<div id="section-30" class="footer box block">
<h2 class="footer"></h2>
<p>Details see <span class="citation">(<a href="#ref-huang2022soccer" role="doc-biblioref">Huang u. a. 2022</a>)</span></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="references" class="unnumbered biblio slide level1">
<div class="decker">
<div class="alignment">
<h1>References</h1>
<div class="layout">
<div class="area">
<div class="box block">
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bellman1984eye" class="csl-entry">
Bellman, Richard. 1984. <em>Eye of the Hurricane</em>. World Scientific.
</div>
<div id="ref-deepmind2021" class="csl-entry">
Hasselt, Hado van, und Diana Borsa. 2021. <span>„Reinforcement Learning Lecture Series 2021“</span>. https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021.
</div>
<div id="ref-huang2022soccer" class="csl-entry">
Huang, Xiaoyu, Zhongyu Li, Yanzhen Xiang, Yiming Ni, Yufeng Chi, Yunhao Li, Lizhi Yang, Xue Bin Peng, und Koushil Sreenath. 2022. <span>„Creating a Dynamic Quadrupedal Robotic Goalkeeper with Reinforcement Learning“</span>. arXiv. doi:<a href="https://doi.org/10.48550/ARXIV.2210.04435">10.48550/ARXIV.2210.04435</a>.
</div>
<div id="ref-karpathy_mdp" class="csl-entry">
Karpathy, Andrej. 2015. <span>„REINFORCEjs“</span>. <a href="https://github.com/karpathy/reinforcejs">https://github.com/karpathy/reinforcejs</a>.
</div>
<div id="ref-silver2015" class="csl-entry">
Silver, David. 2015. <span>„UCL Course on RL UCL Course on RL UCL Course on Reinforcement Learning“</span>. http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html.
</div>
<div id="ref-sutton2018" class="csl-entry">
Sutton, Richard S., und Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press.
</div>
<div id="ref-2010Szepesvari" class="csl-entry">
Szepesvári, Csaba. 2010. <em>Algorithms for Reinforcement Learning</em>. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan &amp; Claypool Publishers. <a href="http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009">http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009</a>.
</div>
<div id="ref-weng2018rl" class="csl-entry">
Weng, Lilian. 2018. <span>„A (Long) Peek into Reinforcement Learning“</span>. <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a>.
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<code class="force-highlight-styles markdown"
style="display:none;"></code>

    </div>
  </div>

  <script type="module">
    /* Store JSON encoded Pandoc meta data in a global variable. */
    import initializeDecker from "./../support/js/decker.js";
    initializeDecker("309e4441b.json");
  </script>

  <script src="../support/vendor/videojs/video.min.js"></script>
  <script type="module" src="../support/components/custom-dialog.js"></script>

  <script type="module">
    // import Reveal and all plugins
    import Reveal from './../support/vendor/reveal/dist/reveal.esm.js';
    import deckerPlugin from './../support/plugins/decker/decker.js';
    import uiAnchorsPlugin from './../support/plugins/decker/ui-anchors.js'
    import mathPlugin from './../support/plugins/math/math.js';
    import whiteboardPlugin from './../support/plugins/whiteboard/whiteboard.js';
    import sagePlugin from './../support/plugins/sage/sage.js';
    import searchPlugin from './../support/plugins/search/search.js';
    import zoomPlugin from './../support/plugins/zoom/zoom.js';
    import printPlugin from './../support/plugins/print/print.js';
    import jinglesPlugin from './../support/plugins/jingles/jingles.js';
    import quizPlugin from './../support/plugins/quiz/quiz.js';
    import quizWuePlugin from './../support/plugins/quiz-wue/quiz-wue.js';
    import explainPlugin from './../support/plugins/explain/explain.js';
    import chartsPlugin from './../support/plugins/charts/charts.js';
    import menuPlugin from './../support/plugins/menu/menu.js';
    import feedbackPlugin from './../support/plugins/feedback/feedback.js';
    import highlightPlugin from './../support/vendor/reveal/plugin/highlight/highlight.esm.js';
    import notesPlugin from './../support/vendor/reveal/plugin/notes/notes.esm.js';
        import captionPlugin from './../support/plugins/live-captioning/live-captioning.js';
        import a11yPlugin from './../support/plugins/a11y/a11y.js';

    let revealConfig = {
      // reveal configuration (see https://revealjs.com/config/)
      ...Decker.meta.reveal,

      // plugin configuration
      math: { mathjax: String.raw`../support/vendor/mathjax/`, ...Decker.meta.math },
      chart: Decker.meta.chart,
      menu: Decker.meta.menu,
      explain: Decker.meta.explain,
      feedback: Decker.meta.feedback || Decker.meta["decker-engine"],
      jingles: Decker.meta.jingles,

      // list of plugins
      plugins: [
        deckerPlugin,
        uiAnchorsPlugin,
        sagePlugin,
        mathPlugin,
        chartsPlugin,
        whiteboardPlugin,
        searchPlugin,
        zoomPlugin,
        printPlugin,
        jinglesPlugin,
        quizPlugin,
        quizWuePlugin,
        explainPlugin,
        menuPlugin,
        feedbackPlugin,
        highlightPlugin,
        notesPlugin,
                captionPlugin,
                a11yPlugin,
      ]
    };

    Reveal.initialize(revealConfig);
  </script>

</body>
<script src="../support/js/inert-polyfill.min.js"></script>
<!-- script src="../support/js/inert.min.js"></script -->
<!-- Use the other implementation if things break under Firefox -->
</html>
